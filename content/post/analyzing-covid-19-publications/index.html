---
title: Analyzing COVID-19 Publications
author: Qiushi Yan
date: '2020-04-06'
slug: [analyzing-covid19-publications]
summary: 'A simple text analysis on the abstract of up-to-date COVID-19 publications.'
lastmod: '2020-05-08T12:54:24+08:00'
featured: no
bibliography: ../bib/covid19.bib
link-citations: yes

categories:
  - Data Analysis
  - R

image:
  caption: 'Image by [cottonbro](https://www.pexels.com/@cottonbro)'
  focal_point: ''
  preview_only: no

---


<div id="TOC">
<ul>
<li><a href="#data-preprocessing">Data preprocessing</a></li>
<li><a href="#common-words-and-keywords-extraction">Common words and keywords extraction</a></li>
<li><a href="#fit-a-lda-topic-model">Fit a LDA topic model</a></li>
<li><a href="#a-network-of-paired-words">A network of paired words</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>In this post, I will be performing a simple text analysis on the abstract of publications on the coronavirus disease (COVID-19), courtesy of <a href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov">WHO</a>. We begin by steps of data preprocessing.</p>
<div id="data-preprocessing" class="section level1">
<h1>Data preprocessing</h1>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre class="r"><code>raw &lt;- read_csv(&quot;D:/RProjects/data/covid-research.csv&quot;) %&gt;% 
  janitor::clean_names() 

glimpse(raw)
#&gt; Rows: 4,190
#&gt; Columns: 16
#&gt; $ title            &lt;chr&gt; &quot;SARS-CoV-2 is not detectable in the vaginal fluid...
#&gt; $ authors          &lt;chr&gt; &quot;Qiu, Lin; Liu, Xia; Xiao, Meng; Xie, Jing; Cao, W...
#&gt; $ abstract         &lt;chr&gt; &quot;Background Severe acute respiratory syndrome coro...
#&gt; $ published_year   &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20...
#&gt; $ published_month  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&gt; $ journal          &lt;chr&gt; &quot;Clinical Infectious Diseases&quot;, &quot;International Jou...
#&gt; $ volume           &lt;chr&gt; NA, &quot;17&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
#&gt; $ issue            &lt;chr&gt; NA, &quot;7&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
#&gt; $ pages            &lt;chr&gt; NA, &quot;2430-2430&quot;, &quot;112275-112275&quot;, NA, &quot;1-4&quot;, NA, N...
#&gt; $ accession_number &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;32229574&quot;, NA, NA, &quot;3...
#&gt; $ doi              &lt;chr&gt; &quot;10.1093/cid/ciaa375&quot;, &quot;10.3390/IJERPH17072430&quot;, &quot;...
#&gt; $ ref              &lt;dbl&gt; 26513, 26499, 26744, 26447, 27114, 26388, 26696, 2...
#&gt; $ covidence_number &lt;chr&gt; &quot;#27487&quot;, &quot;#27413&quot;, &quot;#27869&quot;, &quot;#27815&quot;, &quot;#27905&quot;, ...
#&gt; $ study            &lt;chr&gt; &quot;Qiu 2020&quot;, &quot;Pulido 2020&quot;, &quot;Pillaiyar 2020&quot;, &quot;Piau...
#&gt; $ notes            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&gt; $ tags             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<p>For simplicity I ignore many of the vairables (mostly for identification) and rows with missing values on <code>abstract</code>. I was a little disappointied to find out that <code>published_month</code> are all missing, otherwise we may see a trend of some sort on research topics there. One remaining problem is that some of the papers are not written in English.
The <code>detect_language()</code> from the <a href="https://github.com/ropensci/cld3"><code>cdl3</code></a> can detect language forms at a fairly high success rate. It’s a R wrapper for Google’s Compact Language Detector 3, which is a neural network model for language identification.</p>
<pre class="r"><code>library(cld3)
raw %&gt;% 
  count(language = detect_language(abstract), sort = TRUE)
#&gt; # A tibble: 13 x 2
#&gt;    language     n
#&gt;    &lt;chr&gt;    &lt;int&gt;
#&gt;  1 en        2482
#&gt;  2 ig        1618
#&gt;  3 es          26
#&gt;  4 pt          18
#&gt;  5 de          11
#&gt;  6 ru          10
#&gt;  7 fr           7
#&gt;  8 zh           7
#&gt;  9 &lt;NA&gt;         5
#&gt; 10 fi           2
#&gt; 11 ja           2
#&gt; 12 cs           1
#&gt; 13 it           1</code></pre>
<p>In this post I keep only the rows that were classified as “en”. Also, as illuatrated in <a href="https://www.tidytextmining.com">Text Mining with R</a>, text analysis commonly requires preprocessing steps like tokenizing, eliminating stop words and word stemming. I added custom keywords and did some maunual transformation to make up for misclassifications by <code>detect_language</code>, but there will still be non-English words, though. My transformations (say, both “covid” and “19” now become “covid19”) will certainly induce errors, but there is no better workaround I could think of now.</p>
<pre class="r"><code>library(tidytext)
library(SnowballC)

words &lt;- raw %&gt;% 
  filter(detect_language(abstract) == &quot;en&quot;) %&gt;% 
  unnest_tokens(word, abstract) %&gt;% 
  mutate(word = wordStem(word)) %&gt;%
  mutate(word = case_when(
    word == &quot;19&quot; ~ &quot;covid19&quot;,
    word == &quot;covid&quot; ~ &quot;covid19&quot;,
    word == &quot;coronaviru&quot; ~ &quot;coronavirus&quot;,
    word == &quot;viru&quot; ~ &quot;virus&quot;,
    word == &quot;epidem&quot; ~ &quot;epidemic&quot;,
    word == &quot;studi&quot; ~ &quot;study&quot;,
    word == &quot;respiratori&quot; ~ &quot;respiratory&quot;,
    word == &quot;emetin&quot; ~ &quot;emetine&quot;,
    word == &quot;acut&quot; ~ &quot;acute&quot;,
    word == &quot;sever&quot; ~ &quot;severe&quot;,
    word == &quot;manag&quot; ~ &quot;manage&quot;,
    word == &quot;hospit&quot; ~ &quot;hospital&quot;,
    word == &quot;diseas&quot; ~ &quot;disease&quot;,
    word == &quot;deceas&quot; ~ &quot;disease&quot;,
    word == &quot;caus&quot; ~ &quot;cause&quot;,
    word == &quot;emerg&quot; ~ &quot;emerge&quot;,
    word == &quot;includ&quot; ~ &quot;include&quot;, 
    word == &quot;dai&quot; ~ &quot;wet nurse&quot;,
    word == &quot;ncovid&quot; ~ &quot;ncov&quot;,
    word == &quot;countri&quot; ~ &quot;country&quot;,
    word == &quot;provid&quot; ~ &quot;provide&quot;,
    word == &quot;peopl&quot; ~ &quot;people&quot;,
    TRUE ~ word
  )) %&gt;% 
  anti_join(stop_words %&gt;% 
            add_row(word = c( &quot;2&quot;, &quot;1&quot;,  &quot;dub&quot;, &quot;thi&quot;, &quot;ha&quot;, &quot;wa&quot;, &quot;检查&quot;, &quot;cd&quot;, &quot;gt&quot;,
                              &quot;lt&quot;, &quot;tnt&quot;, &quot;thei&quot;), 
                    lexicon = &quot;custom&quot;)) %&gt;% 
  filter(!(str_detect(word, &quot;^\\d+$&quot;) | str_detect(word, &quot;^\\d+\\w$&quot;)))


words
#&gt; # A tibble: 245,052 x 16
#&gt;    title authors published_year published_month journal volume issue pages
#&gt;    &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;lgl&gt;           &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;
#&gt;  1 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  2 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  3 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  4 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  5 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  6 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  7 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  8 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt;  9 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt; 10 SARS~ Qiu, L~           2020 NA              Clinic~ &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; 
#&gt; # ... with 245,042 more rows, and 8 more variables: accession_number &lt;chr&gt;,
#&gt; #   doi &lt;chr&gt;, ref &lt;dbl&gt;, covidence_number &lt;chr&gt;, study &lt;chr&gt;, notes &lt;chr&gt;,
#&gt; #   tags &lt;chr&gt;, word &lt;chr&gt;</code></pre>
</div>
<div id="common-words-and-keywords-extraction" class="section level1">
<h1>Common words and keywords extraction</h1>
<p>An immediate question is, what are the most common words among all these publications?</p>
<pre class="r"><code>words %&gt;% 
  count(word, sort = TRUE) %&gt;%
  top_n(50) %&gt;%
  ggplot(aes(y = fct_reorder(word, n),
             x = n)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0.01, 0)) + 
  labs(y = NULL,
       x = &quot;# of words&quot;,
       title = &quot;Top 50 common words in COVID-19 publications&quot;) +
  theme(text = element_text(size = 18),
        plot.title.position = &quot;plot&quot;,
        plot.title = element_text(size = 35, face = &quot;bold&quot;),
        axis.ticks.y = element_blank())</code></pre>
<p><img src="/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-7-1.png" width="1152" /></p>
<p>I’m also interested in paper-specific properties, namely their keywords, what topics distinguish them from others? In comparison to the commonly used algorithm tf-idf, I prefer using weighted log odds proposed by <span class="citation">Monroe, Colaresi, and Quinn (<a href="#ref-monroe_colaresi_quinn" role="doc-biblioref">2008</a>)</span>, which a standardized metric from a complete statistical model. It is also implemented in the R package <a href="https://github.com/juliasilge/tidylo"><code>tidylo</code></a><span class="citation">(Schnoebelen and Silge <a href="#ref-tidylo" role="doc-biblioref">2020</a>)</span>. The reason is that tf-idf cannot extract the varying use trend of common words, if a word appears in every research paper, then its inverse document frequency will be zero. For weighted log odds this is not the case, even if all researched mentioned this word it can still differentiate those who used it a lot more often from those who used less. This could be essential when we are trying to find an emphasis on which researchers place as our understanding of the virus advances. Sadly I have no access to the exact date of the publication, so I will just display words with topest score and their corresponding publications.</p>
<pre class="r"><code>library(tidylo)

words %&gt;%
  count(title, word) %&gt;% 
  bind_log_odds(set = title, feature = word, n = n) %&gt;%
  top_n(20) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">title</th>
<th align="left">word</th>
<th align="right">n</th>
<th align="right">log_odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A high ATP concentration enhances the cooperative translocation of the SARS coronavirus helicase nsP13 in the unwinding of duplex RNA</td>
<td align="left">duplex</td>
<td align="right">16</td>
<td align="right">4.525439</td>
</tr>
<tr class="even">
<td align="left">A survey on awareness of digestive system injury caused by corona virus disease 2019 in gastroenterologists</td>
<td align="left">gastroenterologist</td>
<td align="right">14</td>
<td align="right">4.230132</td>
</tr>
<tr class="odd">
<td align="left">Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China</td>
<td align="left">hljby</td>
<td align="right">12</td>
<td align="right">3.917622</td>
</tr>
<tr class="even">
<td align="left">Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China</td>
<td align="left">pedv</td>
<td align="right">21</td>
<td align="right">3.760966</td>
</tr>
<tr class="odd">
<td align="left">Clinical characteristics of 30 medical workers infected with new coronavirus pneumonia</td>
<td align="left">petient</td>
<td align="right">10</td>
<td align="right">3.575936</td>
</tr>
<tr class="even">
<td align="left">Effect of TLR agonist on infections bronchitis virus replication and cytokine expression in embryonated chicken eggs</td>
<td align="left">tlr</td>
<td align="right">14</td>
<td align="right">3.608637</td>
</tr>
<tr class="odd">
<td align="left">Emetine, Ipecac, Ipecac Alkaloids and Analogues as Potential Antiviral Agents for Coronaviruses</td>
<td align="left">emetine</td>
<td align="right">10</td>
<td align="right">3.577769</td>
</tr>
<tr class="even">
<td align="left">Experimental Treatment with Favipiravir for COVID-19: An Open-Label Control Study</td>
<td align="left">fpv</td>
<td align="right">10</td>
<td align="right">3.576505</td>
</tr>
<tr class="odd">
<td align="left">Expert consensus on Pulmonary Function Testing during the epidemic of Corona Virus Disease 2019</td>
<td align="left">检查</td>
<td align="right">11</td>
<td align="right">3.749430</td>
</tr>
<tr class="even">
<td align="left">Facemask shortage and the novel coronavirus disease (COVID-19) outbreak: Reflections on public health measures</td>
<td align="left">facemask</td>
<td align="right">28</td>
<td align="right">5.637175</td>
</tr>
<tr class="odd">
<td align="left">Frequency and Distribution of Chest Radiographic Findings in COVID-19 Positive Patients</td>
<td align="left">cxr</td>
<td align="right">12</td>
<td align="right">3.607049</td>
</tr>
<tr class="even">
<td align="left">Genetic, antigenic and pathogenic characterization of avian coronaviruses isolated from pheasants (Phasianus colchicus) in China</td>
<td align="left">phcov</td>
<td align="right">10</td>
<td align="right">3.577311</td>
</tr>
<tr class="odd">
<td align="left">Influence of trust on two different risk perceptions as an affective and cognitive dimension during Middle East respiratory syndrome coronavirus (MERS-CoV) outbreak in South Korea: serial cross-sectional surveys</td>
<td align="left">percept</td>
<td align="right">16</td>
<td align="right">4.250063</td>
</tr>
<tr class="even">
<td align="left">Microneedle array delivered recombinant coronavirus vaccines: Immunogenicity and rapid translational development</td>
<td align="left">mna</td>
<td align="right">13</td>
<td align="right">3.871558</td>
</tr>
<tr class="odd">
<td align="left">Pregnant women with new coronavirus infection: a clinical characteristics and placental pathological analysis of three cases</td>
<td align="left">placenta</td>
<td align="right">14</td>
<td align="right">3.762505</td>
</tr>
<tr class="even">
<td align="left">Responding to the COVID-19 pandemic in complex humanitarian crises</td>
<td align="left">humanitarian</td>
<td align="right">14</td>
<td align="right">3.681217</td>
</tr>
<tr class="odd">
<td align="left">Serial interval in determining the estimation of reproduction number of the novel coronavirus disease (COVID-19) during the early outbreak | Journal of Travel Medicine | Oxford Academic</td>
<td align="left">si</td>
<td align="right">19</td>
<td align="right">4.108847</td>
</tr>
<tr class="even">
<td align="left">Short-term effects of ambient PM1 and PM2.5 air pollution on hospital admission for respiratory diseases: Case-crossover evidence from Shenzhen, China</td>
<td align="left">pm1</td>
<td align="right">12</td>
<td align="right">3.918315</td>
</tr>
<tr class="odd">
<td align="left">The experience of high-flow nasal cannula in hospitalized patients with 2019 novel coronavirus-infected pneumonia in two hospitals of Chongqing, China</td>
<td align="left">hfnc</td>
<td align="right">12</td>
<td align="right">3.706804</td>
</tr>
<tr class="even">
<td align="left">Thoughts and suggestions on modern construction of disease prevention and control system</td>
<td align="left">modern</td>
<td align="right">20</td>
<td align="right">4.138237</td>
</tr>
</tbody>
</table>
<p>Many words listed here are acronyms and terms in biology, chemistry and medicine. For example, “hljby” is a type of <strong>p</strong>orcine <strong>e</strong>pidemic <strong>d</strong>iarrhoea <strong>v</strong>irus which “pedv” stands for, “fpv” means <strong>F</strong>avi<strong>p</strong>ira<strong>v</strong>ir (a type of drug), and “tlr” represents <strong>T</strong>oll-<strong>l</strong>ike <strong>r</strong>eceptors.</p>
</div>
<div id="fit-a-lda-topic-model" class="section level1">
<h1>Fit a LDA topic model</h1>
<p>Let’s then fit a 5-topic LDA topic model, before that we should convert the data frame to a docuemnt term matrix using <code>cast_dtm</code>. There are various implementations of this kind of model, here I use <code>stm::stm</code>. The choice of <span class="math inline">\(K\)</span> (number of topics) here is somewhat arbitrary here, but <a href="https://juliasilge.com/">Julia Silge</a> had a great <a href="https://juliasilge.com/blog/evaluating-stm/">post</a> about it.</p>
<pre class="r"><code>dfm &lt;- cast_dfm(words %&gt;% count(title, word),
                term = word,
                document = title,
                value = n)</code></pre>
<p><code>dfm</code> is a document-term matrix with 2415 documents and 13383 features.</p>
<pre class="r"><code>library(stm)
topic_model &lt;- stm(dfm, K = 5, init.type = &quot;LDA&quot;, verbose = FALSE)</code></pre>
<p>Topic-term probability distributions are accessed by <code>tidy()</code>, this gives a glance of the underlying meaning of these topics:</p>
<pre class="r"><code># topic-term distribution
tidy(topic_model) %&gt;% 
  group_by(topic) %&gt;% 
  top_n(10) %&gt;% 
  ungroup() %&gt;%
  mutate(topic = factor(topic) %&gt;% str_c(&quot;topic&quot;, .)) %&gt;% 
  ggplot(aes(y = reorder_within(term, beta, topic),
         x = beta,
         fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  scale_y_reordered() + 
  facet_wrap(~ topic, scales = &quot;free_y&quot;, nrow = 3) + 
  labs(y = NULL,
       x = &quot;Docuemtn-term probabilities&quot;,
       title = &quot;A 6-topic LDA model&quot;) + 
  theme(text = element_text(size = 18),
        plot.title = element_text(size = 30, face = &quot;bold&quot;),
        strip.text = element_text(size = 25, hjust = 0.05),
        axis.ticks.y = element_blank())</code></pre>
<p><img src="/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-12-1.png" width="1152" /></p>
<p>It’s hard to interpret these topics without domain knwoledge. But it seems to me that topic3 is related to clinical findings, topic4 to china and wuhan, the epicenter of covid19.</p>
</div>
<div id="a-network-of-paired-words" class="section level1">
<h1>A network of paired words</h1>
<p>Another question of interest is the relationship between words: what group of words tend to appear together? I look at the <a href="https://en.wikipedia.org/wiki/Phi_coefficient">phi coefficient</a>, which is essentailly <span class="math inline">\(\chi^2\)</span> statistc in a contingency table applied to categorical variables.</p>
<p>As each abstract is a natual unit of measure, a pair of words that both appear in the same abstract are seen as “appearing together”. We could compute <span class="math inline">\(\phi\)</span> based on pairwise counts:</p>
<pre class="r"><code>library(widyr)

word_cors &lt;- words %&gt;% 
  add_count(word) %&gt;% 
  filter(n &gt; 20) %&gt;%
  select(-n) %&gt;%
  pairwise_cor(item = word, feature = title, sort = TRUE)

word_cors
#&gt; # A tibble: 2,385,480 x 3
#&gt;    item1        item2        correlation
#&gt;    &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;
#&gt;  1 arabia       saudi              1    
#&gt;  2 saudi        arabia             1    
#&gt;  3 pave         crazi              0.970
#&gt;  4 crazi        pave               0.970
#&gt;  5 kong         hong               0.935
#&gt;  6 hong         kong               0.935
#&gt;  7 dehydrogenas lactat             0.931
#&gt;  8 lactat       dehydrogenas       0.931
#&gt;  9 reserv       copyright          0.928
#&gt; 10 copyright    reserv             0.928
#&gt; # ... with 2,385,470 more rows</code></pre>
<p>A network visualization of word correlation is a good idea:</p>
<pre class="r"><code>library(ggraph)
library(tidygraph)

word_cors %&gt;% 
  filter(correlation &gt; 0.4) %&gt;% 
  as_tbl_graph() %&gt;% 
  ggraph(layout = &quot;fr&quot;) + 
  geom_edge_link(aes(alpha = correlation), show.legend = FALSE) + 
  geom_node_point(color = &quot;lightblue&quot;, size = 6.5) + 
  geom_node_text(aes(label = name), repel = TRUE, size = 5.5)</code></pre>
<p><img src="/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-14-1.png" width="1152" /></p>
<p>As you can see, there are still some non-English words that stemming and adding stopwrods cannot handle… Nonetheless, we are be able to identify some of the clusters revovling around infant infection (infant, pregnant, newborn, mother), pathology (angiotensin, protein, receptor), symptoms (lung, thicken, lesion), etc.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-monroe_colaresi_quinn">
<p>Monroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008. “Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.” <em>Political Analysis</em> 16 (4): 372–403. <a href="https://doi.org/10.1093/pan/mpn018">https://doi.org/10.1093/pan/mpn018</a>.</p>
</div>
<div id="ref-tidylo">
<p>Schnoebelen, Tyler, and Julia Silge. 2020. <em>Tidylo: Tidy Log Odds Ratio Weighted by Uninformative Prior</em>. <a href="http://github.com/juliasilge/tidylo">http://github.com/juliasilge/tidylo</a>.</p>
</div>
</div>
</div>
