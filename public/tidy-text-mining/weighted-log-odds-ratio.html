<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.2 Weighted log odds ratio | Text Mining with R — Notes</title>
  <meta name="description" content="3.2 Weighted log odds ratio | Text Mining with R — Notes" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="3.2 Weighted log odds ratio | Text Mining with R — Notes" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.2 Weighted log odds ratio | Text Mining with R — Notes" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tf-idf.html"/>
<link rel="next" href="a-corpus-of-physics-texts.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes for Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Text Mining with R</b></span></li>
<li class="chapter" data-level="1" data-path="tidy-text-format.html"><a href="tidy-text-format.html"><i class="fa fa-check"></i><b>1</b> Tidy text format</a><ul>
<li class="chapter" data-level="1.1" data-path="the-unnest-tokens-function.html"><a href="the-unnest-tokens-function.html"><i class="fa fa-check"></i><b>1.1</b> The <code>unnest_tokens()</code> function</a></li>
<li class="chapter" data-level="1.2" data-path="the-gutenbergr-package.html"><a href="the-gutenbergr-package.html"><i class="fa fa-check"></i><b>1.2</b> The <code>gutenbergr</code> package</a></li>
<li class="chapter" data-level="1.3" data-path="compare-word-frequency.html"><a href="compare-word-frequency.html"><i class="fa fa-check"></i><b>1.3</b> Compare word frequency</a></li>
<li class="chapter" data-level="1.4" data-path="other-tokenization-methods.html"><a href="other-tokenization-methods.html"><i class="fa fa-check"></i><b>1.4</b> Other tokenization methods</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sentiment-analysis-with-tidy-data.html"><a href="sentiment-analysis-with-tidy-data.html"><i class="fa fa-check"></i><b>2</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="2.1" data-path="the-sentiments-dataset.html"><a href="the-sentiments-dataset.html"><i class="fa fa-check"></i><b>2.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="2.2" data-path="sentiment-analysis-with-inner-join.html"><a href="sentiment-analysis-with-inner-join.html"><i class="fa fa-check"></i><b>2.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="2.3" data-path="comparing-3-different-dictionaries.html"><a href="comparing-3-different-dictionaries.html"><i class="fa fa-check"></i><b>2.3</b> Comparing 3 different dictionaries</a></li>
<li class="chapter" data-level="2.4" data-path="most-common-positive-and-negative-words.html"><a href="most-common-positive-and-negative-words.html"><i class="fa fa-check"></i><b>2.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="2.5" data-path="wordclouds.html"><a href="wordclouds.html"><i class="fa fa-check"></i><b>2.5</b> Wordclouds</a></li>
<li class="chapter" data-level="2.6" data-path="units-other-than-words.html"><a href="units-other-than-words.html"><i class="fa fa-check"></i><b>2.6</b> Units other than words</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyzing-word-and-document-frequency.html"><a href="analyzing-word-and-document-frequency.html"><i class="fa fa-check"></i><b>3</b> Analyzing word and document frequency</a><ul>
<li class="chapter" data-level="3.1" data-path="tf-idf.html"><a href="tf-idf.html"><i class="fa fa-check"></i><b>3.1</b> tf-idf</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tf-idf.html"><a href="tf-idf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>3.1.1</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="3.1.2" data-path="tf-idf.html"><a href="tf-idf.html#zipfs-law"><i class="fa fa-check"></i><b>3.1.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="3.1.3" data-path="tf-idf.html"><a href="tf-idf.html#word-rank-slope-chart"><i class="fa fa-check"></i><b>3.1.3</b> Word rank slope chart</a></li>
<li class="chapter" data-level="3.1.4" data-path="tf-idf.html"><a href="tf-idf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>3.1.4</b> The <code>bind_tf_idf()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html"><i class="fa fa-check"></i><b>3.2</b> Weighted log odds ratio</a><ul>
<li class="chapter" data-level="3.2.1" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#log-odds-ratio"><i class="fa fa-check"></i><b>3.2.1</b> Log odds ratio</a></li>
<li class="chapter" data-level="3.2.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#model-based-approach-weighted-log-odds-ratio"><i class="fa fa-check"></i><b>3.2.2</b> Model-based approach: Weighted log odds ratio</a></li>
<li class="chapter" data-level="3.2.3" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#discussions"><i class="fa fa-check"></i><b>3.2.3</b> Discussions</a></li>
<li class="chapter" data-level="3.2.4" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#bind_log_odds"><i class="fa fa-check"></i><b>3.2.4</b> <code>bind_log_odds()</code></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-corpus-of-physics-texts.html"><a href="a-corpus-of-physics-texts.html"><i class="fa fa-check"></i><b>3.3</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="relationships-between-words-n-grams-and-correlations.html"><a href="relationships-between-words-n-grams-and-correlations.html"><i class="fa fa-check"></i><b>4</b> Relationships between words: n-grams and correlations</a><ul>
<li class="chapter" data-level="4.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html"><i class="fa fa-check"></i><b>4.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="4.1.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#filtering-n-grams"><i class="fa fa-check"></i><b>4.1.1</b> Filtering n-grams</a></li>
<li class="chapter" data-level="4.1.2" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#analyzing-bigrams"><i class="fa fa-check"></i><b>4.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="4.1.3" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-a-network-of-bigrams-with-ggraph"><i class="fa fa-check"></i><b>4.1.4</b> Visualizing a network of bigrams with <code>ggraph</code></a></li>
<li class="chapter" data-level="4.1.5" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-friends"><i class="fa fa-check"></i><b>4.1.5</b> Visualizing “friends”</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html"><i class="fa fa-check"></i><b>4.2</b> Counting and correlating pairs of words with <code>widyr</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>4.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="4.2.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#pairwise-correlation"><i class="fa fa-check"></i><b>4.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="converting-to-and-from-non-tidy-formats.html"><a href="converting-to-and-from-non-tidy-formats.html"><i class="fa fa-check"></i><b>5</b> Converting to and from non-tidy formats</a><ul>
<li class="chapter" data-level="5.1" data-path="tidying-a-document-term-matrix.html"><a href="tidying-a-document-term-matrix.html"><i class="fa fa-check"></i><b>5.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="5.2" data-path="casting-tidy-text-data-into-a-matrix.html"><a href="casting-tidy-text-data-into-a-matrix.html"><i class="fa fa-check"></i><b>5.2</b> Casting tidy text data into a matrix</a></li>
<li class="chapter" data-level="5.3" data-path="tidying-corpus-objects-with-metadata.html"><a href="tidying-corpus-objects-with-metadata.html"><i class="fa fa-check"></i><b>5.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>6</b> Topic modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>6.1</b> Latent Dirichlet Allocation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#example-associated-press"><i class="fa fa-check"></i><b>6.1.1</b> Example: Associated Press</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html"><i class="fa fa-check"></i><b>6.2</b> Example: the great library heist</a><ul>
<li class="chapter" data-level="6.2.1" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#lda-on-chapters"><i class="fa fa-check"></i><b>6.2.1</b> LDA on chapters</a></li>
<li class="chapter" data-level="6.2.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#per-document-classification"><i class="fa fa-check"></i><b>6.2.2</b> Per-document classification</a></li>
<li class="chapter" data-level="6.2.3" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>6.2.3</b> By word assignments: <code>augment()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining with R — Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="weighted-log-odds-ratio" class="section level2">
<h2><span class="header-section-number">3.2</span> Weighted log odds ratio</h2>
<p>This section is heavily based on <a href="https://www.cambridge.org/core/journals/political-analysis/article/fightin-words-lexical-feature-selection-and-evaluation-for-identifying-the-content-of-political-conflict/81B3703230D21620B81EB6E2266C7A66">Monroe, Colaresi, and Quinn</a><span class="citation">(<a href="references.html#ref-monroe_colaresi_quinn" role="doc-biblioref">2008</a>)</span> and a medium post, titled <a href="https://medium.com/@TSchnoebelen/i-dare-say-you-will-never-use-tf-idf-again-4918408b2310">I dare say you will never use tf-idf again</a>.</p>
<div id="log-odds-ratio" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Log odds ratio</h3>
<p>First, the <strong>log odds</strong> of word <span class="math inline">\(w\)</span> in document <span class="math inline">\(i\)</span> is defined as</p>
<p><span class="math display">\[
\log{O_{w}^i} = \log{\frac{f_w^i}{1 - f_w^i}} 
\]</span></p>
<p>Logging the odds ratio provides a measure that is symmetric when comparing the usage of word <span class="math inline">\(w\)</span> across different documents. <strong>Log odds ratio</strong> of word <span class="math inline">\(w\)</span> between document <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is</p>
<p><span class="math display">\[
\log{\frac{O_{w}^i}{O_w^j}} = \log{\frac{f_w^i}{1 - f_w^i} / \frac{f_w^j}{1 - f_w^j}} = \log{\frac{f_w^i}{1 - f_w^i}} - log{\frac{f_w^j}{1 - f_w^j}} 
\]</span>
A problem is that, when some word <span class="math inline">\(w\)</span> is only presented in document <span class="math inline">\(i\)</span>, among other documents in the corpus, odds ratio will have zero denominator and the metric goes to infinity. One solution is to “add a little bit to the zeroes”, in which smoothed word frequency is defined as <span class="math inline">\(\tilde{f_w^i} = f_w^i + \varepsilon\)</span>.</p>
<p>Note that regardless of the zero treatment, words with hightest log odds ratio are often obscure ones. The problem is again the failure to account for sampling variability. <strong>With logodds ratios, the sampling variation goes down with increased frequency</strong>. So that different words are not comparable.</p>
<p>A common response to this is to set some frequency “threshold” for features to ‘‘qualify’’ for consideration. For example, we only compute log odds ratio on words that appears at least 10 times.</p>
</div>
<div id="model-based-approach-weighted-log-odds-ratio" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Model-based approach: Weighted log odds ratio</h3>
<p><a href="https://www.cambridge.org/core/journals/political-analysis/article/fightin-words-lexical-feature-selection-and-evaluation-for-identifying-the-content-of-political-conflict/81B3703230D21620B81EB6E2266C7A66">Monroe, Colaresi, and Quinn</a> then proposed a model-basded approach where the choice of word <span class="math inline">\(w\)</span> are modelled as a function oof document <span class="math inline">\(P(w |j)\)</span>. In general, the strategy is to first model word usage in the full collection of documents and to then investigate how subgroup-specific word usage diverges from that in the full collection of documents.</p>
<p>First, the moel assumes that the W-vector <span class="math inline">\(\boldsymbol{y}\)</span> follows a multinomial distribution</p>
<p><span class="math display">\[
\boldsymbol{y} \sim \text{Multinomial}(n, \boldsymbol{\pi})
\]</span>
where <span class="math inline">\(n = \sum_{w=1}^Wy_w\)</span> and <span class="math inline">\(\boldsymbol{\pi}\)</span> is W-vector of possibilities. And the the baseline log odds ratio between word <span class="math inline">\(w\)</span> and the first word is</p>
<p><span class="math display">\[
\beta_w = \log{\pi_w} - \log{\pi_1}  \;\;\; w=1, 2, ...,  W
\]</span></p>
<p>BTW, this is essentially a baseline logit model including only an intercept, which is an extention of logistic regression for dealing with response with W categories.</p>
<p>Therefore, the likelihood function can be expressed in terms of <span class="math inline">\(\beta_w\)</span></p>
<p><span class="math display">\[
L(\boldsymbol{\beta} | \boldsymbol{y}) = \prod_{w=1}^{W}{(\frac{\exp(\beta_w)}{\sum_{w=1}^{W}\exp(\beta_w)})^{y_w}}
\]</span></p>
<p>Within any document, <span class="math inline">\(i\)</span>, the model to this point goes through with addition of subscripts</p>
<p><span class="math display">\[
\boldsymbol{y^i} \sim \text{Multinomial}(n^i, \boldsymbol{\pi^i})
\]</span></p>
<p>The lack of covariates results in an immediately available analytical solution for the MLE of
<span class="math inline">\({\beta^i_w}\)</span> . We calculate</p>
<p><span class="math display">\[
\boldsymbol{\hat{\pi}}^\text{MLE} = \boldsymbol{y} / n
\]</span></p>
<p>where <span class="math inline">\(n = \sum_{w = 1}^{W}y_w\)</span>and <span class="math inline">\(\boldsymbol{\hat{\beta}}^\text{MLE}\)</span> follows after transforming.</p>
<p>The paper proceeds with a Bayesian model, specifying the prior using the conjugate for the
multinomial distribution, the Dirichlet:</p>
<p><span class="math display">\[
\boldsymbol{\pi} \sim \text{Dirichlet}(\boldsymbol{\alpha})
\]</span>
where <span class="math inline">\(\boldsymbol{\alpha}\)</span> is a W-vector of parameters with each element <span class="math inline">\(\alpha_w &gt; 0\)</span>. There is a nice interpretation of <span class="math inline">\(\alpha_w\)</span>, that is, use of any particular Dirichlet prior defined by <span class="math inline">\(\boldsymbol{\alpha}\)</span> affects the posterior exactly as if we had observed in the data an additional <span class="math inline">\(\alpha_w – 1\)</span> instances of word <span class="math inline">\(w\)</span>. It follows this is a uninformative prior if all <span class="math inline">\(\alpha_w\)</span>s are identitcal.</p>
<p>Due to the conjugacy, the full Bayesian estimate using the Dirichlet prior is also analytically available in analogous form:</p>
<p><span class="math display">\[
\boldsymbol{\hat{\pi}} = \frac{(\boldsymbol{y} + \boldsymbol{\alpha})}{n + \alpha_0}
\]</span>
where <span class="math inline">\(\alpha_0 = \sum_{w=1}^{W}\alpha_w\)</span></p>
<p>Our job, therefore, is to compare if the usage of word <span class="math inline">\(w\)</span> in document <span class="math inline">\(i\)</span>, <span class="math inline">\(\pi_w^i\)</span>, differs from <span class="math inline">\(\pi_w\)</span> overall or in some other document <span class="math inline">\(\pi^j_w\)</span>. One of the advantages of the model-based approach is that we can measure the uncertainty in odds.</p>
<p>Denote the odds (now with probabilistic meaning) of word w, relative to all others, as <span class="math inline">\(\Omega_w = \pi_w / (1- \pi_w)\)</span>. We are interested in how the usage of a word by document <span class="math inline">\(i\)</span> differs
from usage of the word in all documents, which we can capture with the log-odds-ratio, which we will now define as <span class="math inline">\(\delta_w^i = \log{\Omega_w^i / \Omega_w}\)</span>. To compare between documents <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, log odds ratio of word <span class="math inline">\(w\)</span> is defined as <span class="math inline">\(\delta_w^{i-j} = \log{\Omega_w^i / \Omega_w^j}\)</span>. To scale these two estimators, their point estimate and estimated variance are found</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\delta}_{w}^{i-j} &amp;= \log(\frac{y_w^i + \alpha_w}{n^i + \alpha_0 - (y_w^i + \alpha_w)}) - \log(\frac{y_w^j + \alpha_w}{n^j + \alpha_0 - (y_w^j + \alpha_w)}) \\ 
\sigma^2(\hat{\delta}_{w}^{i-j}) &amp;\approx \frac{1}{y_w^i + a_w} + \frac{1}{y_w^j + a_w}
\end{aligned}
\]</span>
The final standardized statistic for a word <span class="math inline">\(w\)</span> is then the z–score of its log–odds–ratio:</p>
<p><span class="math display">\[
\frac{\hat{\delta}_{w}^{i-j}}{\sigma^2(\hat{\delta}_{w}^{i-j})}
\]</span></p>
<p>The Monroe, et al method then uses counts from a background corpus to provide a prior count for words, rather than the uninformative Dirichlet prior, essentially shrinking the counts toward to the prior frequency in a large background corpus to prevent overfitting. The general notion is to put a strong conservative prior (regularization) on the model, requiring the data to speak very loudly if such a difference is to be declared. I am not going to dive into that. But it is important to know that <span class="math inline">\(\alpha_0 = 1\)</span> means no shrinkage, and <span class="math inline">\(\alpha_0 \rightarrow 0\)</span> or <span class="math inline">\(\alpha_0 \rightarrow \infty\)</span> mean strongest shrinkage.</p>
</div>
<div id="discussions" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Discussions</h3>
<p>The Monroe, modifies the commonly used log–odds ratio (introduced in Section <a href="weighted-log-odds-ratio.html#log-odds-ratio">3.2.1</a>) in two ways <span class="citation">(Jurafsky et al. <a href="references.html#ref-10.5210/fm.v19i4.4944" role="doc-biblioref">2014</a>)</span> :</p>
<p>It uses the z–scores of the log–odds–ratio, which controls for the amount of variance in a word’s frequency. So the score on different words are comparable. And the metric also faciliates interpretation, positive log odds means stronger tendency to use the word, and negative ones indicate otherwise. Also, <span class="math inline">\(|\hat{\delta}^i_w| &gt; 1.96\)</span> should mean some sort of significance.</p>
<p>Secondly, it uses counts from a background corpus to provide a prior count for words, essentially shrinking the counts toward to the prior frequency in a large background corpus. These features enable differences even in very frequent words to be detected; previous linguistic methods have all had problems with frequent words. Because function words like pronouns and auxiliary verbs are both extremely frequent and have been shown to be important cues to social and narrative meaning, this is a major limitation of these methods. For example, as an English reader/speaker, you won’t be surprised that all the authors you are trying to compare use “of the” and “said the” in their respective documents, and we want to know who used much more than others. tf-idf will not be able to detect this, because the idf of words appearing in every document will always be zero.</p>
</div>
<div id="bind_log_odds" class="section level3">
<h3><span class="header-section-number">3.2.4</span> <code>bind_log_odds()</code></h3>
<p>There is an <a href="https://github.com/juliasilge/tidylo/issues/3">issue</a> suggesting that <code>tidylo</code> is still experimental. However, it did provide a ussful function <code>bind_log_odds()</code> that implemented the weighted log odds we dicussed above, and it seems that it currently uses the marginal distributions (i.e., distribution of words across all documents combined) to construct an informative Dirichlet prior.</p>
<p>The data structure <code>bind_log_odds()</code> requires is the same with <code>bind_tf_idf()</code></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1"><span class="kw">library</span>(tidylo)</a>
<a class="sourceLine" id="cb57-2" title="2"></a>
<a class="sourceLine" id="cb57-3" title="3">book_words <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb57-4" title="4"><span class="st">  </span><span class="kw">bind_log_odds</span>(<span class="dt">set =</span> book, <span class="dt">feature =</span> word, <span class="dt">n =</span> n) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb57-5" title="5"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(log_odds))</a>
<a class="sourceLine" id="cb57-6" title="6"><span class="co">#&gt; # A tibble: 40,379 x 7</span></a>
<a class="sourceLine" id="cb57-7" title="7"><span class="co">#&gt;   book                word          n      tf   idf   tf_idf log_odds</span></a>
<a class="sourceLine" id="cb57-8" title="8"><span class="co">#&gt;   &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;</span></a>
<a class="sourceLine" id="cb57-9" title="9"><span class="co">#&gt; 1 Emma                emma        786 0.00488 1.10  0.00536      24.5</span></a>
<a class="sourceLine" id="cb57-10" title="10"><span class="co">#&gt; 2 Mansfield Park      fanny       816 0.00509 0.693 0.00352      24.1</span></a>
<a class="sourceLine" id="cb57-11" title="11"><span class="co">#&gt; 3 Sense &amp; Sensibility elinor      623 0.00519 1.79  0.00931      23.4</span></a>
<a class="sourceLine" id="cb57-12" title="12"><span class="co">#&gt; 4 Pride &amp; Prejudice   elizabeth   597 0.00489 0.693 0.00339      21.0</span></a>
<a class="sourceLine" id="cb57-13" title="13"><span class="co">#&gt; 5 Sense &amp; Sensibility marianne    492 0.00410 1.79  0.00735      20.8</span></a>
<a class="sourceLine" id="cb57-14" title="14"><span class="co">#&gt; 6 Persuasion          anne        447 0.00534 0.182 0.000974     20.6</span></a>
<a class="sourceLine" id="cb57-15" title="15"><span class="co">#&gt; # ... with 4.037e+04 more rows</span></a></code></pre></div>
<p>Now let’s compare the result between tf-idf and weighted log odds in a subset of <code>book_words</code></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" title="1">words_tf_idf &lt;-<span class="st"> </span>book_words <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-2" title="2"><span class="st">  </span><span class="kw">filter</span>(book <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Emma&quot;</span>, <span class="st">&quot;Pride &amp; Prejudice&quot;</span>, <span class="st">&quot;Persuasion&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-3" title="3"><span class="st">  </span><span class="kw">bind_tf_idf</span>(<span class="dt">term =</span> word, <span class="dt">document =</span> book, <span class="dt">n =</span> n) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-4" title="4"><span class="st">  </span><span class="kw">group_by</span>(book) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-5" title="5"><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-6" title="6"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-7" title="7"><span class="st">  </span><span class="kw">facet_bar</span>(<span class="dt">y =</span> word, <span class="dt">x =</span> tf_idf, <span class="dt">by =</span> book, <span class="dt">ncol =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-8" title="8"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Top 10 words picked tf-idf&quot;</span>)</a>
<a class="sourceLine" id="cb58-9" title="9"></a>
<a class="sourceLine" id="cb58-10" title="10">words_wlo &lt;-<span class="st"> </span>book_words <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-11" title="11"><span class="st">  </span><span class="kw">filter</span>(book <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Emma&quot;</span>, <span class="st">&quot;Pride &amp; Prejudice&quot;</span>, <span class="st">&quot;Persuasion&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-12" title="12"><span class="st">  </span><span class="kw">bind_log_odds</span>(<span class="dt">feature =</span> word, <span class="dt">set =</span> book, <span class="dt">n =</span> n) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-13" title="13"><span class="st">  </span><span class="kw">group_by</span>(book) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-14" title="14"><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb58-15" title="15"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-16" title="16"><span class="st">  </span><span class="kw">facet_bar</span>(<span class="dt">y =</span> word, <span class="dt">x =</span> log_odds, <span class="dt">by =</span> book, <span class="dt">ncol =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb58-17" title="17"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Top 10 words picked by weighted log odds&quot;</span>)</a>
<a class="sourceLine" id="cb58-18" title="18"></a>
<a class="sourceLine" id="cb58-19" title="19">words_tf_idf <span class="op">/</span><span class="st"> </span>words_wlo</a></code></pre></div>
<p><img src="analyzing-word-and-document-frequency_files/figure-html/unnamed-chunk-16-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tf-idf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-corpus-of-physics-texts.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/tidy-text-mining/edit/master/book/analyzing-word-and-document-frequency.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
