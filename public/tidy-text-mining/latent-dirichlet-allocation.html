<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.1 Latent Dirichlet Allocation | Text Mining with R — Notes</title>
  <meta name="description" content="6.1 Latent Dirichlet Allocation | Text Mining with R — Notes" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="6.1 Latent Dirichlet Allocation | Text Mining with R — Notes" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.1 Latent Dirichlet Allocation | Text Mining with R — Notes" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-04-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="topic-modeling.html"/>
<link rel="next" href="example-the-great-library-heist.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes for Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Text Mining with R</b></span></li>
<li class="chapter" data-level="1" data-path="tidy-text-format.html"><a href="tidy-text-format.html"><i class="fa fa-check"></i><b>1</b> Tidy text format</a><ul>
<li class="chapter" data-level="1.1" data-path="the-unnest-tokens-function.html"><a href="the-unnest-tokens-function.html"><i class="fa fa-check"></i><b>1.1</b> The <code>unnest_tokens()</code> function</a></li>
<li class="chapter" data-level="1.2" data-path="the-gutenbergr-package.html"><a href="the-gutenbergr-package.html"><i class="fa fa-check"></i><b>1.2</b> The <code>gutenbergr</code> package</a></li>
<li class="chapter" data-level="1.3" data-path="compare-word-frequency.html"><a href="compare-word-frequency.html"><i class="fa fa-check"></i><b>1.3</b> Compare word frequency</a></li>
<li class="chapter" data-level="1.4" data-path="other-tokenization-methods.html"><a href="other-tokenization-methods.html"><i class="fa fa-check"></i><b>1.4</b> Other tokenization methods</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sentiment-analysis-with-tidy-data.html"><a href="sentiment-analysis-with-tidy-data.html"><i class="fa fa-check"></i><b>2</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="2.1" data-path="the-sentiments-dataset.html"><a href="the-sentiments-dataset.html"><i class="fa fa-check"></i><b>2.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="2.2" data-path="sentiment-analysis-with-inner-join.html"><a href="sentiment-analysis-with-inner-join.html"><i class="fa fa-check"></i><b>2.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="2.3" data-path="comparing-3-different-dictionaries.html"><a href="comparing-3-different-dictionaries.html"><i class="fa fa-check"></i><b>2.3</b> Comparing 3 different dictionaries</a></li>
<li class="chapter" data-level="2.4" data-path="most-common-positive-and-negative-words.html"><a href="most-common-positive-and-negative-words.html"><i class="fa fa-check"></i><b>2.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="2.5" data-path="wordclouds.html"><a href="wordclouds.html"><i class="fa fa-check"></i><b>2.5</b> Wordclouds</a></li>
<li class="chapter" data-level="2.6" data-path="units-other-than-words.html"><a href="units-other-than-words.html"><i class="fa fa-check"></i><b>2.6</b> Units other than words</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyzing-word-and-document-frequency.html"><a href="analyzing-word-and-document-frequency.html"><i class="fa fa-check"></i><b>3</b> Analyzing word and document frequency</a><ul>
<li class="chapter" data-level="3.1" data-path="tf-idf.html"><a href="tf-idf.html"><i class="fa fa-check"></i><b>3.1</b> tf-idf</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tf-idf.html"><a href="tf-idf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>3.1.1</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="3.1.2" data-path="tf-idf.html"><a href="tf-idf.html#zipfs-law"><i class="fa fa-check"></i><b>3.1.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="3.1.3" data-path="tf-idf.html"><a href="tf-idf.html#word-rank-slope-chart"><i class="fa fa-check"></i><b>3.1.3</b> Word rank slope chart</a></li>
<li class="chapter" data-level="3.1.4" data-path="tf-idf.html"><a href="tf-idf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>3.1.4</b> The <code>bind_tf_idf()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html"><i class="fa fa-check"></i><b>3.2</b> Weighted log odds ratio</a><ul>
<li class="chapter" data-level="3.2.1" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#log-odds-ratio"><i class="fa fa-check"></i><b>3.2.1</b> Log odds ratio</a></li>
<li class="chapter" data-level="3.2.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#model-based-approach-weighted-log-odds-ratio"><i class="fa fa-check"></i><b>3.2.2</b> Model-based approach: Weighted log odds ratio</a></li>
<li class="chapter" data-level="3.2.3" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#discussions"><i class="fa fa-check"></i><b>3.2.3</b> Discussions</a></li>
<li class="chapter" data-level="3.2.4" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#bind_log_odds"><i class="fa fa-check"></i><b>3.2.4</b> <code>bind_log_odds()</code></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-corpus-of-physics-texts.html"><a href="a-corpus-of-physics-texts.html"><i class="fa fa-check"></i><b>3.3</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="relationships-between-words-n-grams-and-correlations.html"><a href="relationships-between-words-n-grams-and-correlations.html"><i class="fa fa-check"></i><b>4</b> Relationships between words: n-grams and correlations</a><ul>
<li class="chapter" data-level="4.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html"><i class="fa fa-check"></i><b>4.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="4.1.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#filtering-n-grams"><i class="fa fa-check"></i><b>4.1.1</b> Filtering n-grams</a></li>
<li class="chapter" data-level="4.1.2" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#analyzing-bigrams"><i class="fa fa-check"></i><b>4.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="4.1.3" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-a-network-of-bigrams-with-ggraph"><i class="fa fa-check"></i><b>4.1.4</b> Visualizing a network of bigrams with <code>ggraph</code></a></li>
<li class="chapter" data-level="4.1.5" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-friends"><i class="fa fa-check"></i><b>4.1.5</b> Visualizing “friends”</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html"><i class="fa fa-check"></i><b>4.2</b> Counting and correlating pairs of words with <code>widyr</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>4.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="4.2.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#pairwise-correlation"><i class="fa fa-check"></i><b>4.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="converting-to-and-from-non-tidy-formats.html"><a href="converting-to-and-from-non-tidy-formats.html"><i class="fa fa-check"></i><b>5</b> Converting to and from non-tidy formats</a><ul>
<li class="chapter" data-level="5.1" data-path="tidying-a-document-term-matrix.html"><a href="tidying-a-document-term-matrix.html"><i class="fa fa-check"></i><b>5.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="5.2" data-path="casting-tidy-text-data-into-a-matrix.html"><a href="casting-tidy-text-data-into-a-matrix.html"><i class="fa fa-check"></i><b>5.2</b> Casting tidy text data into a matrix</a></li>
<li class="chapter" data-level="5.3" data-path="tidying-corpus-objects-with-metadata.html"><a href="tidying-corpus-objects-with-metadata.html"><i class="fa fa-check"></i><b>5.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>6</b> Topic modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>6.1</b> Latent Dirichlet Allocation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#example-associated-press"><i class="fa fa-check"></i><b>6.1.1</b> Example: Associated Press</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html"><i class="fa fa-check"></i><b>6.2</b> Example: the great library heist</a><ul>
<li class="chapter" data-level="6.2.1" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#lda-on-chapters"><i class="fa fa-check"></i><b>6.2.1</b> LDA on chapters</a></li>
<li class="chapter" data-level="6.2.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#per-document-classification"><i class="fa fa-check"></i><b>6.2.2</b> Per-document classification</a></li>
<li class="chapter" data-level="6.2.3" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>6.2.3</b> By word assignments: <code>augment()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="reviews-on-regular-expressions.html"><a href="reviews-on-regular-expressions.html"><i class="fa fa-check"></i><b>A</b> Reviews on regular expressions</a><ul>
<li class="chapter" data-level="A.1" data-path="posix-character-classes.html"><a href="posix-character-classes.html"><i class="fa fa-check"></i><b>A.1</b> POSIX Character Classes</a></li>
<li class="chapter" data-level="A.2" data-path="greedy-and-lazy-quantifiers.html"><a href="greedy-and-lazy-quantifiers.html"><i class="fa fa-check"></i><b>A.2</b> Greedy and lazy quantifiers</a></li>
<li class="chapter" data-level="A.3" data-path="looking-ahead-and-back.html"><a href="looking-ahead-and-back.html"><i class="fa fa-check"></i><b>A.3</b> Looking ahead and back</a></li>
<li class="chapter" data-level="A.4" data-path="backreferences.html"><a href="backreferences.html"><i class="fa fa-check"></i><b>A.4</b> Backreferences</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="text-processing-examples-in-r.html"><a href="text-processing-examples-in-r.html"><i class="fa fa-check"></i><b>B</b> Text processing examples in R</a><ul>
<li class="chapter" data-level="B.1" data-path="replacing-and-removing.html"><a href="replacing-and-removing.html"><i class="fa fa-check"></i><b>B.1</b> Replacing and removing</a></li>
<li class="chapter" data-level="B.2" data-path="combining-and-splitting.html"><a href="combining-and-splitting.html"><i class="fa fa-check"></i><b>B.2</b> Combining and splitting</a></li>
<li class="chapter" data-level="B.3" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html"><i class="fa fa-check"></i><b>B.3</b> Extracting text from pdf and other files</a><ul>
<li class="chapter" data-level="B.3.1" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html#office-documents"><i class="fa fa-check"></i><b>B.3.1</b> Office documents</a></li>
<li class="chapter" data-level="B.3.2" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html#images"><i class="fa fa-check"></i><b>B.3.2</b> Images</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining with R — Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="latent-dirichlet-allocation" class="section level2">
<h2><span class="header-section-number">6.1</span> Latent Dirichlet Allocation</h2>
<blockquote>
<p>Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.</p>
</blockquote>
<p>The LDA model is guided by two principles:</p>
<ul>
<li><p><strong>Each document is a mixture of topics</strong>. In a 3 topic model we could assert that a document is 70% about topic A, 30 about topic B, and 0% about topic C.</p></li>
<li><p><strong>Every topic is a mixture of words</strong>. A topic is considered a probabilistic distribution over multiple words.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="images/LDA.jpg" alt="Source: http://nlpx.net/wp/wp-content/uploads/2016/01/LDA_image2.jpg" width="120%" />
<p class="caption">
Figure 6.1: Source: <a href="http://nlpx.net/wp/wp-content/uploads/2016/01/LDA_image2.jpg" class="uri">http://nlpx.net/wp/wp-content/uploads/2016/01/LDA_image2.jpg</a>
</p>
</div>
<p>In particular, LDA is a imagined generative process, illustrated in the plate notation below:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/LDA_GPM.png" alt="Source: Lee et al. (2018)" width="120%" />
<p class="caption">
Figure 6.2: Source: <span class="citation">Lee et al. (<a href="references.html#ref-lee" role="doc-biblioref">2018</a>)</span>
</p>
</div>

<ul>
<li><span class="math inline">\(M\)</span> denotes the number of documents<br />
</li>
<li><span class="math inline">\(N\)</span> is the number of words in a given document (document <span class="math inline">\(i\)</span> has <span class="math inline">\(N_i\)</span> words)<br />
</li>
<li><span class="math inline">\(\vec{\theta_m}\)</span> is the expected topic proportion of document <span class="math inline">\(m\)</span>, which is generated by a Dirichlet distribution parameterized by <span class="math inline">\(\vec{\alpha}\)</span> (e.g., in a two topic model <span class="math inline">\(\theta_m = [0.3, 0.7]\)</span> means document <span class="math inline">\(m\)</span> is expected to have 30% topic 1 and 70% topic 2)<br />
</li>
<li><span class="math inline">\(\vec{\phi_k}\)</span> is the word distribution of topic <span class="math inline">\(k\)</span>, which is generated by a Dirichlet distribution parameterized by <span class="math inline">\(\vec{\beta}\)</span><br />
</li>
<li><span class="math inline">\(z_{m, n}\)</span> is the topic for the <span class="math inline">\(n\)</span>th word in document <span class="math inline">\(m\)</span>, one word are assigned to one topic.<br />
</li>
<li><span class="math inline">\(w_{m, n}\)</span> is the word in the <span class="math inline">\(n\)</span>th position word of document <span class="math inline">\(m\)</span></li>
</ul>
<p>The only observed variable in this graphical probabilistic model is <span class="math inline">\(w_{m, n}\)</span>, so it is “latent”.</p>
<p>To actually infer the topics in a corpus, we imagine the generative process as follows. LDA assumes the following generative process for a corpus <span class="math inline">\(D\)</span> consisting of <span class="math inline">\(M\)</span> M documents each of length <span class="math inline">\(N_i\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Generate <span class="math inline">\(\vec{\theta_i} \sim \text{Dir}(\vec{\alpha})\)</span>, where <span class="math inline">\(i \in \{1, 2, ..., M\}\)</span>. <span class="math inline">\(\text{Dir}(\vec{\alpha})\)</span> is a Dirichlet distribution with symmetric parameter <span class="math inline">\(\vec{\alpha}\)</span> where <span class="math inline">\(\vec{\alpha}\)</span> is often sparse.</p></li>
<li><p>Generate <span class="math inline">\(\vec{\phi_k} \sim \text{Dir}(\vec{\beta})\)</span>, where <span class="math inline">\(k \in \{1, 2, ..., K\}\)</span> and <span class="math inline">\(\vec{\beta}\)</span> is typically sparse</p></li>
<li><p>For the <span class="math inline">\(n\)</span>th position in document <span class="math inline">\(m\)</span>, where <span class="math inline">\(n \in \{1, 2, ..., N_m\}\)</span> and <span class="math inline">\(m \in \{1, 2, ..., M\}\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Choose a topic <span class="math inline">\(z_{m, n}\)</span> for that position which is generated from <span class="math inline">\(z_{m, n} \sim \text{Multinomial}(\vec{\theta_i})\)</span><br />
</li>
<li>Fill in that position with word <span class="math inline">\(w_{m, n}\)</span> which is generated from the word distribution of the topic picked in the previous step <span class="math inline">\(w_{i,j} \sim \text{Multinomial}(\phi_{z_{m, n}})\)</span></li>
</ol></li>
</ol>
<div id="example-associated-press" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Example: Associated Press</h3>
<p>We come to the <code>AssociatedPress</code> document term matrix (the required data strcture for the modeling function) and fit a two topic LDA model with <code>topicmodels::LDA()</code></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" title="1"><span class="kw">library</span>(topicmodels)</a>
<a class="sourceLine" id="cb102-2" title="2"><span class="kw">data</span>(<span class="st">&quot;AssociatedPress&quot;</span>)</a>
<a class="sourceLine" id="cb102-3" title="3"></a>
<a class="sourceLine" id="cb102-4" title="4">ap_lda &lt;-<span class="st"> </span><span class="kw">LDA</span>(AssociatedPress, <span class="dt">k =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb102-5" title="5">ap_lda</a>
<a class="sourceLine" id="cb102-6" title="6"><span class="co">#&gt; A LDA_VEM topic model with 2 topics.</span></a></code></pre></div>
<p>For tidying model objects, <code>tidy(model_object, matrix = "beta")</code> (the default) access the topic-word probability vector (we denotes with <span class="math inline">\(\vec{\phi_k}\)</span>)</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1"><span class="kw">tidy</span>(ap_lda)</a>
<a class="sourceLine" id="cb103-2" title="2"><span class="co">#&gt; # A tibble: 20,946 x 3</span></a>
<a class="sourceLine" id="cb103-3" title="3"><span class="co">#&gt;   topic term            beta</span></a>
<a class="sourceLine" id="cb103-4" title="4"><span class="co">#&gt;   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;</span></a>
<a class="sourceLine" id="cb103-5" title="5"><span class="co">#&gt; 1     1 aaron     0.00000513</span></a>
<a class="sourceLine" id="cb103-6" title="6"><span class="co">#&gt; 2     2 aaron     0.0000403 </span></a>
<a class="sourceLine" id="cb103-7" title="7"><span class="co">#&gt; 3     1 abandon   0.0000499 </span></a>
<a class="sourceLine" id="cb103-8" title="8"><span class="co">#&gt; 4     2 abandon   0.0000193 </span></a>
<a class="sourceLine" id="cb103-9" title="9"><span class="co">#&gt; 5     1 abandoned 0.0000116 </span></a>
<a class="sourceLine" id="cb103-10" title="10"><span class="co">#&gt; 6     2 abandoned 0.000170  </span></a>
<a class="sourceLine" id="cb103-11" title="11"><span class="co">#&gt; # ... with 20,940 more rows</span></a></code></pre></div>
<p>Which words have a relateve higher probabiltity to appear in each topic?</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" title="1"><span class="kw">tidy</span>(ap_lda) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb104-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb104-3" title="3"><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb104-4" title="4"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb104-5" title="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">str_c</span>(<span class="st">&quot;topic&quot;</span>, topic)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb104-6" title="6"><span class="st">  </span><span class="kw">facet_bar</span>(<span class="dt">y =</span> term, <span class="dt">x =</span> beta, <span class="dt">by =</span> topic) <span class="op">+</span></a>
<a class="sourceLine" id="cb104-7" title="7"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Words with highest probability in each topic&quot;</span>)</a></code></pre></div>
<p><img src="topic-modeling_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As an alternative, we could consider the terms that had the <strong>greatest difference</strong> in <span class="math inline">\(\vec{\phi_k}\)</span> between topic 1 and topic 2. This can be estimated based on the log ratio of the two: <span class="math inline">\(\log_2(\frac{\phi_{1n}}{\phi_{2n}})\)</span>, <span class="math inline">\(\phi_{1n} / \phi_{2n}\)</span> being the probability ratio of the sam e word <span class="math inline">\(n\)</span> in two topics (a log ratio is useful because it makes the difference symmetrical)</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1">phi_ratio &lt;-<span class="st"> </span><span class="kw">tidy</span>(ap_lda) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">str_c</span>(<span class="st">&quot;topic&quot;</span>, topic)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb105-3" title="3"><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> topic, <span class="dt">values_from =</span> beta) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb105-4" title="4"><span class="st">  </span><span class="kw">filter</span>(topic1 <span class="op">&gt;</span><span class="st"> </span><span class="fl">.001</span> <span class="op">|</span><span class="st"> </span>topic2 <span class="op">&gt;</span><span class="st"> </span><span class="fl">.001</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-5" title="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_ratio =</span> <span class="kw">log2</span>(topic2 <span class="op">/</span><span class="st"> </span>topic1))</a></code></pre></div>
<p>This can answer a question like: which word is most representative of a topic?</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" title="1">phi_ratio <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb106-2" title="2"><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">20</span>, <span class="kw">abs</span>(log_ratio)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb106-3" title="3"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">fct_reorder</span>(term, log_ratio),</a>
<a class="sourceLine" id="cb106-4" title="4">             <span class="dt">x =</span> log_ratio)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb106-5" title="5"><span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb106-6" title="6"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb106-7" title="7">       <span class="dt">x =</span> <span class="st">&quot;log ratio of phi between topic 2 and topic 1 (base 2)&quot;</span>)</a></code></pre></div>
<p><img src="topic-modeling_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>To extrac the word proportion vector <span class="math inline">\(\vec{\theta_m}\)</span> for document <span class="math inline">\(m\)</span>, use <code>matrix = "gamma"</code> in <code>tidy()</code></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" title="1"><span class="kw">tidy</span>(ap_lda, <span class="dt">matrix =</span> <span class="st">&quot;gamma&quot;</span>)</a>
<a class="sourceLine" id="cb107-2" title="2"><span class="co">#&gt; # A tibble: 4,492 x 3</span></a>
<a class="sourceLine" id="cb107-3" title="3"><span class="co">#&gt;   document topic gamma</span></a>
<a class="sourceLine" id="cb107-4" title="4"><span class="co">#&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;</span></a>
<a class="sourceLine" id="cb107-5" title="5"><span class="co">#&gt; 1        1     1 0.552</span></a>
<a class="sourceLine" id="cb107-6" title="6"><span class="co">#&gt; 2        2     1 0.623</span></a>
<a class="sourceLine" id="cb107-7" title="7"><span class="co">#&gt; 3        3     1 0.491</span></a>
<a class="sourceLine" id="cb107-8" title="8"><span class="co">#&gt; 4        4     1 0.464</span></a>
<a class="sourceLine" id="cb107-9" title="9"><span class="co">#&gt; 5        5     1 0.517</span></a>
<a class="sourceLine" id="cb107-10" title="10"><span class="co">#&gt; 6        6     1 0.482</span></a>
<a class="sourceLine" id="cb107-11" title="11"><span class="co">#&gt; # ... with 4,486 more rows</span></a></code></pre></div>
<p>With this data frame, we want to knwo which document is most charateristic of each topic?</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb108-2" title="2"><span class="kw">library</span>(wordcloud)</a>
<a class="sourceLine" id="cb108-3" title="3"></a>
<a class="sourceLine" id="cb108-4" title="4"><span class="kw">tidy</span>(ap_lda, <span class="dt">matrix =</span> <span class="st">&quot;gamma&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb108-5" title="5"><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-6" title="6"><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">15</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb108-7" title="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">document =</span> <span class="kw">as.character</span>(document)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb108-8" title="8"><span class="st">  </span><span class="kw">acast</span>(document <span class="op">~</span><span class="st"> </span>topic, <span class="dt">value.var =</span> <span class="st">&quot;gamma&quot;</span>, <span class="dt">fill =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb108-9" title="9"><span class="st">  </span><span class="kw">comparison.cloud</span>(<span class="dt">colors =</span> <span class="kw">c</span>(<span class="st">&quot;gray20&quot;</span>, <span class="st">&quot;gray80&quot;</span>), <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">8</span>))</a></code></pre></div>
<p><img src="topic-modeling_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This plot would definitely be more insightful if we have document titles rather than an ID.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topic-modeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="example-the-great-library-heist.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/tidy-text-mining/edit/master/book/topic-modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
