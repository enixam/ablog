[{"authors":["admin"],"categories":null,"content":"Hello!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hello!","tags":null,"title":"Qiushi Yan","type":"authors"},{"authors":[],"categories":["R"],"content":"\r# setup\rlibrary(tidyverse)\rlibrary(jsonlite)\r# get the data\rlondon_stage \u0026lt;- fromJSON(\u0026quot;D:/RProjects/data/LondonStageFull.json\u0026quot;) %\u0026gt;% as_tibble()\rlondon_stage\r## # A tibble: 52,617 x 13\r## EventId EventDate TheatreCode Season Volume Hathi CommentC TheatreId Phase2\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0 16591029 city 1659-~ 1 \u0026quot;\u0026quot; The \u0026lt;i\u0026gt;~ 63 *p165~\r## 2 1 16591100 mt 1659-~ 1 \u0026quot;\u0026quot; On 23 N~ 206 *p165~\r## 3 2 16591218 none 1659-~ 1 \u0026quot;\u0026quot; Represe~ 1 *p165~\r## 4 3 16600200 mt 1659-~ 1 \u0026quot;\u0026quot; 6 Feb. ~ 206 *p166~\r## 5 4 16600204 cockpit 1659-~ 1 \u0026quot;\u0026quot; $Thomas~ 73 *p166~\r## 6 5 16600328 dh 1659-~ 1 \u0026quot;\u0026quot; At \u0026lt;i\u0026gt;D~ 90 *p166~\r## 7 6 16600406 none 1659-~ 1 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 1 *p166~\r## 8 7 16600412 vh 1659-~ 1 \u0026quot;\u0026quot; Edition~ 319 *p166~\r## 9 8 16600413 fh 1659-~ 1 \u0026quot;\u0026quot; \u0026lt;i\u0026gt;The ~ 116 *p166~\r## 10 9 16600416 none 1659-~ 1 \u0026quot;\u0026quot; \u0026quot;\u0026quot; 1 *p166~\r## # ... with 52,607 more rows, and 4 more variables: Phase1 \u0026lt;chr\u0026gt;,\r## # CommentCClean \u0026lt;chr\u0026gt;, BookPDF \u0026lt;chr\u0026gt;, Performances \u0026lt;list\u0026gt;\rThe json file can be downloaded at https://londonstagedatabase.usu.edu/downloads/LondonStageJSON.zip.\n","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574402979,"objectID":"28de46b95d7d87f692ae994dfa4ab63e","permalink":"/post/exploring-the-london-stage-database/exploring-the-london-stage-database/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/post/exploring-the-london-stage-database/exploring-the-london-stage-database/","section":"post","summary":"Data wranling, cleaning practice using the English Stage Database","tags":[],"title":"Exploring the London Stage Database","type":"post"},{"authors":null,"categories":["R"],"content":"\rTable of Contents\r1.skim() basics\n2.Manipulating the results\n3.Customizing skim()\n4.Skimming non-data frames\n5.References\n\rskim() basics\rThe skimr(Waring et al. 2019) package is developed by the rOpenSci cummunity, with its core function skim() as an enhanced version of summary(), and has just experienced a major update to V2. For a more comprehensive guide towards its features, please visit its vignette.1\nWhat skim() returns can be divided into 2 parts:\n\rData summary: an overview of the input data frame, including # of rows and columns, column types and if the data frame is grouped\n\rSummary of column variables based on their types, with one section per variable type and one row per variable: For instance, all numerical variables are explained by missing rate, quantiles (fivenum()), mean, standard deviation and a tiny inline histogram. With factors, the count of top values are returned instaed.\r\rlibrary(skimr)\rlibrary(tidyverse)\rskim(iris)\r\rTable 1: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rSpecies\r0\r1\rFALSE\r3\rset: 50, ver: 50, vir: 50\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.80\r6.4\r7.9\r▆▇▇▅▂\r\rSepal.Width\r0\r1\r3.06\r0.44\r2.0\r2.8\r3.00\r3.3\r4.4\r▁▆▇▂▁\r\rPetal.Length\r0\r1\r3.76\r1.77\r1.0\r1.6\r4.35\r5.1\r6.9\r▇▁▆▇▂\r\rPetal.Width\r0\r1\r1.20\r0.76\r0.1\r0.3\r1.30\r1.8\r2.5\r▇▁▇▅▃\r\r\r\rThe second argument ... in skim() could specify any number of variables to be included, similar to dplyr::select():\nskim(iris, Sepal.Width, Species)\r\rTable 2: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rSpecies\r0\r1\rFALSE\r3\rset: 50, ver: 50, vir: 50\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Width\r0\r1\r3.06\r0.44\r2\r2.8\r3\r3.3\r4.4\r▁▆▇▂▁\r\r\r\ror with common select() helpers:\nskim(iris, contains(\u0026quot;Length\u0026quot;))\r\rTable 3: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r2\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.80\r6.4\r7.9\r▆▇▇▅▂\r\rPetal.Length\r0\r1\r3.76\r1.77\r1.0\r1.6\r4.35\r5.1\r6.9\r▇▁▆▇▂\r\r\r\rWe can also take the summary() of the skimmed data frame, and get the Data Summary part:\nskim(iris) %\u0026gt;% summary()\r\rTable 4: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rskim() also supports grouped data created by dplyr::group_by()(and later we will see that skim() workes seamlessly in a typical tidyverse(Wickham et al. 2019) workflow). In the grouped case, one additional column for each grouping variable is added.\niris %\u0026gt;%\rgroup_by(Species) %\u0026gt;%\rskim()\r\rTable 5: Data summary\r\rName\rPiped data\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rSpecies\r\r\r\rVariable type: numeric\n\r\rskim_variable\rSpecies\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\rsetosa\r0\r1\r5.01\r0.35\r4.3\r4.80\r5.00\r5.20\r5.8\r▃▃▇▅▁\r\rSepal.Length\rversicolor\r0\r1\r5.94\r0.52\r4.9\r5.60\r5.90\r6.30\r7.0\r▂▇▆▃▃\r\rSepal.Length\rvirginica\r0\r1\r6.59\r0.64\r4.9\r6.23\r6.50\r6.90\r7.9\r▁▃▇▃▂\r\rSepal.Width\rsetosa\r0\r1\r3.43\r0.38\r2.3\r3.20\r3.40\r3.68\r4.4\r▁▃▇▅▂\r\rSepal.Width\rversicolor\r0\r1\r2.77\r0.31\r2.0\r2.52\r2.80\r3.00\r3.4\r▁▅▆▇▂\r\rSepal.Width\rvirginica\r0\r1\r2.97\r0.32\r2.2\r2.80\r3.00\r3.18\r3.8\r▂▆▇▅▁\r\rPetal.Length\rsetosa\r0\r1\r1.46\r0.17\r1.0\r1.40\r1.50\r1.58\r1.9\r▁▃▇▃▁\r\rPetal.Length\rversicolor\r0\r1\r4.26\r0.47\r3.0\r4.00\r4.35\r4.60\r5.1\r▂▂▇▇▆\r\rPetal.Length\rvirginica\r0\r1\r5.55\r0.55\r4.5\r5.10\r5.55\r5.88\r6.9\r▃▇▇▃▂\r\rPetal.Width\rsetosa\r0\r1\r0.25\r0.11\r0.1\r0.20\r0.20\r0.30\r0.6\r▇▂▂▁▁\r\rPetal.Width\rversicolor\r0\r1\r1.33\r0.20\r1.0\r1.20\r1.30\r1.50\r1.8\r▅▇▃▆▁\r\rPetal.Width\rvirginica\r0\r1\r2.03\r0.27\r1.4\r1.80\r2.00\r2.30\r2.5\r▂▇▆▅▇\r\r\r\rTo better illustrate skim(), we should know it returns a skim_df object, which is essentially a single wide data frame combining the results, with some additional attributes and two metadata columns:\n\rskim_type: class of the variable\rskim_variable: name of the original variable\r\ris_skim_df() function is used to assert that an object is a skim_df.\nclass(skim(iris))\r## [1] \u0026quot;skim_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\ris_skim_df(skim(iris))\r## [1] TRUE\rTo explicitly show 2 columns skim_type and skim_variable and see the nature of this skim_df object, we can transfrom it to a tibble or data.frame\niris %\u0026gt;% skim() %\u0026gt;% as_tibble()\r## # A tibble: 5 x 15\r## skim_type skim_variable n_missing complete_rate factor.ordered factor.n_unique\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt;\r## 1 factor Species 0 1 FALSE 3\r## 2 numeric Sepal.Length 0 1 NA NA\r## 3 numeric Sepal.Width 0 1 NA NA\r## 4 numeric Petal.Length 0 1 NA NA\r## 5 numeric Petal.Width 0 1 NA NA\r## # ... with 9 more variables: factor.top_counts \u0026lt;chr\u0026gt;, numeric.mean \u0026lt;dbl\u0026gt;,\r## # numeric.sd \u0026lt;dbl\u0026gt;, numeric.p0 \u0026lt;dbl\u0026gt;, numeric.p25 \u0026lt;dbl\u0026gt;, numeric.p50 \u0026lt;dbl\u0026gt;,\r## # numeric.p75 \u0026lt;dbl\u0026gt;, numeric.p100 \u0026lt;dbl\u0026gt;, numeric.hist \u0026lt;chr\u0026gt;\rAs you can see, the Data Summary part are excluded after the transformation, and the wide data frame are left. This is in contrast to summary.data.frame(), which stores statistics in a table. The distinction is important, because the skim_df object is pipeable and easy to use for additional manipulation: for example, the user could select all of the variable means, or all summary statistics for a specific variable (Note that we don’t have to really transform it before such manipulation).\nFiltering by column types:\nskim(iris) %\u0026gt;% filter(skim_type == \u0026quot;numeric\u0026quot;)\r\rTable 6: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.80\r6.4\r7.9\r▆▇▇▅▂\r\rSepal.Width\r0\r1\r3.06\r0.44\r2.0\r2.8\r3.00\r3.3\r4.4\r▁▆▇▂▁\r\rPetal.Length\r0\r1\r3.76\r1.77\r1.0\r1.6\r4.35\r5.1\r6.9\r▇▁▆▇▂\r\rPetal.Width\r0\r1\r1.20\r0.76\r0.1\r0.3\r1.30\r1.8\r2.5\r▇▁▇▅▃\r\r\r\rskim(iris) %\u0026gt;% filter(skim_variable == \u0026quot;Sepal.Length\u0026quot;)\r\rTable 7: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.8\r6.4\r7.9\r▆▇▇▅▂\r\r\r\rWhen using select() to choose statistics we want to display, n_missing and complete_rate can be directly specified, since they are computed for all types of columns. Other statistics must be specified in select with a prefix, as shown in the column names of skim(iris) %\u0026gt;% as_tibble() (i.e, extracting the arithmet mean by numeric.mean):\n# no need to put a prefix\rskim(iris) %\u0026gt;% select(skim_variable, skim_type, n_missing, complete_rate)\r\rTable 8: Data summary\r\rName\riris\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\r\r\r\rSpecies\r0\r1\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\r\r\r\rSepal.Length\r0\r1\r\rSepal.Width\r0\r1\r\rPetal.Length\r0\r1\r\rPetal.Width\r0\r1\r\r\r\r`\n# extract the mean and median with a prefix\rskim(iris) %\u0026gt;% filter(skim_type == \u0026quot;numeric\u0026quot;) %\u0026gt;%\rselect(skim_variable, numeric.mean, numeric.p50)\r## # A tibble: 4 x 3\r## skim_variable numeric.mean numeric.p50\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Sepal.Length 5.84 5.8 ## 2 Sepal.Width 3.06 3 ## 3 Petal.Length 3.76 4.35\r## 4 Petal.Width 1.20 1.3\r# extract top levels of a factor\rskim(iris) %\u0026gt;% filter(skim_type == \u0026quot;factor\u0026quot;) %\u0026gt;% select(skim_variable, factor.top_counts)\r## # A tibble: 1 x 2\r## skim_variable factor.top_counts ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Species set: 50, ver: 50, vir: 50\rFinally, just for fun, what statistics or which variable we choose to use will not affect the skim_df variable type, yet this is not the case for skim_variable and skim_type, which are intrinsic to skim_df objects. If we exclude these two, the skim_df object will be automatically coerced to a tibble:\nskim(iris) %\u0026gt;% select(-skim_type, -skim_variable) %\u0026gt;%\ris_skim_df\r## [1] FALSE\r\rManipulating the results\rAs noted above, skim() returns a wide data frame, an skim_df object. This is usually the most sensible format for the majority of operations when investigating data, and skimr also provides functions aimed at facilitating the exploratory process .\nFirst, partition() returns a named list of the wide data frames for each data type. Unlike the original data the partitioned data only has columns corresponding to the skimming functions use for this data type. These data frames are, therefore, not skim_df objects. And we can easily subset that list to get desirable component:\niris %\u0026gt;%\rskim() %\u0026gt;%\rpartition()\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rSpecies\r0\r1\rFALSE\r3\rset: 50, ver: 50, vir: 50\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.80\r6.4\r7.9\r▆▇▇▅▂\r\rSepal.Width\r0\r1\r3.06\r0.44\r2.0\r2.8\r3.00\r3.3\r4.4\r▁▆▇▂▁\r\rPetal.Length\r0\r1\r3.76\r1.77\r1.0\r1.6\r4.35\r5.1\r6.9\r▇▁▆▇▂\r\rPetal.Width\r0\r1\r1.20\r0.76\r0.1\r0.3\r1.30\r1.8\r2.5\r▇▁▇▅▃\r\r\r\rEach component of the resulting list is an one_skim_df object, and can be readily coerced to tibbles:\niris %\u0026gt;%\rskim() %\u0026gt;%\rpartition() %\u0026gt;%\rmap(class)\r## $factor\r## [1] \u0026quot;one_skim_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot; ## ## $numeric\r## [1] \u0026quot;one_skim_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rThis is useful when we only want to look at certain data types, we can subset that component out of the partitioned result, coerce it to a tibble, and then manipulate it. We don’t have to put a prefix before column names now, since rows in every component are of identical data types, and therefore share same statistics:\n# subset\riris_numeric \u0026lt;- iris %\u0026gt;%\rskim() %\u0026gt;% partition() %\u0026gt;%\rpluck(\u0026quot;numeric\u0026quot;) %\u0026gt;%\ras_tibble()\riris_numeric %\u0026gt;% arrange(sd) ## no more numeric.sd\r## # A tibble: 4 x 11\r## skim_variable n_missing complete_rate mean sd p0 p25 p50 p75\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Sepal.Width 0 1 3.06 0.436 2 2.8 3 3.3\r## 2 Petal.Width 0 1 1.20 0.762 0.1 0.3 1.3 1.8\r## 3 Sepal.Length 0 1 5.84 0.828 4.3 5.1 5.8 6.4\r## 4 Petal.Length 0 1 3.76 1.77 1 1.6 4.35 5.1\r## # ... with 2 more variables: p100 \u0026lt;dbl\u0026gt;, hist \u0026lt;chr\u0026gt;\rSecond, yank() selects only specified variables base on their type, this can be viewed as a shortcut to partition() and subsetting:\niris %\u0026gt;% skim() %\u0026gt;% yank(\u0026quot;factor\u0026quot;)\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rSpecies\r0\r1\rFALSE\r3\rset: 50, ver: 50, vir: 50\r\r\r\rUse focus() to select columns of the skimmed results and keep them as a skim_df; it always keeps the metadata column. In other words, to focus on some statistics:\niris %\u0026gt;% skim() %\u0026gt;%\rfocus(n_missing, numeric.mean)\r\rTable 9: Data summary\r\rName\rPiped data\r\rNumber of rows\r150\r\rNumber of columns\r5\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\r\r\r\rSpecies\r0\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rmean\r\r\r\rSepal.Length\r0\r5.84\r\rSepal.Width\r0\r3.06\r\rPetal.Length\r0\r3.76\r\rPetal.Width\r0\r1.20\r\r\r\r\rCustomizing skim()\rWe can create out own skimming function with the skim_with() function factory. To add a statistic for a data type, create an sfl() (a skimr function list) for each class that you want to change:\n# also look at skewness and kurtosis for numerical variables\rlibrary(e1071)\rmy_skim \u0026lt;- skim_with(numeric = sfl(\u0026quot;3rd_central_moment\u0026quot; = skewness,\r\u0026quot;4th_central_moment\u0026quot; = kurtosis))\rNow, we can use my_skim to skim through numerical variables:\nmy_skim(iris) %\u0026gt;% yank(\u0026quot;numeric\u0026quot;)\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r3rd_central_moment\r4th_central_moment\r\r\r\rSepal.Length\r0\r1\r5.84\r0.83\r4.3\r5.1\r5.80\r6.4\r7.9\r▆▇▇▅▂\r0.31\r-0.61\r\rSepal.Width\r0\r1\r3.06\r0.44\r2.0\r2.8\r3.00\r3.3\r4.4\r▁▆▇▂▁\r0.31\r0.14\r\rPetal.Length\r0\r1\r3.76\r1.77\r1.0\r1.6\r4.35\r5.1\r6.9\r▇▁▆▇▂\r-0.27\r-1.42\r\rPetal.Width\r0\r1\r1.20\r0.76\r0.1\r0.3\r1.30\r1.8\r2.5\r▇▁▇▅▃\r-0.10\r-1.36\r\r\r\rWhen using skim_with(), the named list in sf1() correspond to statistics calculated on a specified type, so names of that list must fall into one of R’s classes, available classes are (see ?get_skimmers):\n\rnumeric\rcharacter\n\rfactor\n\rlogical\n\rcomplex\n\rDate\n\rPOSIXct\n\rdifftime\n\rts\n\rAsIs\r\rIt you want to customize skim() to support more data classes, here is an simple example, adding support for sf oject.\n\rSkimming non-data frames\rIn skimr v2, skim() will attempt to coerce non-data frames (such as vectors and matrices) to data frames. In most cases with vectors, the object being evaluated should be equivalent to wrapping the object in as.data.frame().\nSkimming charactor vector:\nskim(letters)\r\rTable 10: Data summary\r\rName\rletters\r\rNumber of rows\r26\r\rNumber of columns\r1\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rdata\r0\r1\rFALSE\r26\ra: 1, b: 1, c: 1, d: 1\r\r\r\rThis is same to:\nskim(as.data.frame(letters))\r\rTable 11: Data summary\r\rName\ras.data.frame(letters)\r\rNumber of rows\r26\r\rNumber of columns\r1\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rletters\r0\r1\rFALSE\r26\ra: 1, b: 1, c: 1, d: 1\r\r\r\rThe way skim() works with list is some natural, for data frames are just a special case of list:\nmy_list \u0026lt;- list(a = 1:10, b = letters[1:10], c = factor(1:10))\rskim(my_list)\r\rTable 12: Data summary\r\rName\rmy_list\r\rNumber of rows\r10\r\rNumber of columns\r3\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r2\r\rnumeric\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rb\r0\r1\rFALSE\r10\ra: 1, b: 1, c: 1, d: 1\r\rc\r0\r1\rFALSE\r10\r1: 1, 2: 1, 3: 1, 4: 1\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\ra\r0\r1\r5.5\r3.03\r1\r3.25\r5.5\r7.75\r10\r▇▇▇▇▇\r\r\r\rskimming ts object:\nlynx\r## Time Series:\r## Start = 1821 ## End = 1934 ## Frequency = 1 ## [1] 269 321 585 871 1475 2821 3928 5943 4950 2577 523 98 184 279 409\r## [16] 2285 2685 3409 1824 409 151 45 68 213 546 1033 2129 2536 957 361\r## [31] 377 225 360 731 1638 2725 2871 2119 684 299 236 245 552 1623 3311\r## [46] 6721 4254 687 255 473 358 784 1594 1676 2251 1426 756 299 201 229\r## [61] 469 736 2042 2811 4431 2511 389 73 39 49 59 188 377 1292 4031\r## [76] 3495 587 105 153 387 758 1307 3465 6991 6313 3794 1836 345 382 808\r## [91] 1388 2713 3800 3091 2985 3790 674 81 80 108 229 399 1132 2432 3574\r## [106] 2935 1537 529 485 662 1000 1590 2657 3396\r# note the last inline line graph, pretty cute\rskim(lynx)\r\rTable 13: Data summary\r\rName\rlynx\r\rNumber of rows\r114\r\rNumber of columns\r1\r\r_______________________\r\r\rColumn type frequency:\r\r\rts\r1\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: ts\n\r\rskim_variable\rn_missing\rcomplete_rate\rstart\rend\rfrequency\rdeltat\rmean\rsd\rmin\rmax\rmedian\rline_graph\r\r\r\rx\r0\r1\r1821\r1934\r1\r1\r1538.02\r1585.84\r39\r6991\r771\r⡈⢄⡠⢁⣀⠒⣀⠔\r\r\r\rwhen skimming matrices, columns and treated as variables and rows as observations, so statistics are calculated on a column basis:\nm \u0026lt;- matrix(1:12, nrow = 4, ncol = 3)\rm\r## [,1] [,2] [,3]\r## [1,] 1 5 9\r## [2,] 2 6 10\r## [3,] 3 7 11\r## [4,] 4 8 12\rskim(m)\r\rTable 14: Data summary\r\rName\rm\r\rNumber of rows\r4\r\rNumber of columns\r3\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r3\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rV1\r0\r1\r2.5\r1.29\r1\r1.75\r2.5\r3.25\r4\r▇▇▁▇▇\r\rV2\r0\r1\r6.5\r1.29\r5\r5.75\r6.5\r7.25\r8\r▇▇▁▇▇\r\rV3\r0\r1\r10.5\r1.29\r9\r9.75\r10.5\r11.25\r12\r▇▇▁▇▇\r\r\r\rTranspose the matrix:\nskim(t(m))\r\rTable 15: Data summary\r\rName\rt(m)\r\rNumber of rows\r3\r\rNumber of columns\r4\r\r_______________________\r\r\rColumn type frequency:\r\r\rnumeric\r4\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rV1\r0\r1\r5\r4\r1\r3\r5\r7\r9\r▇▁▇▁▇\r\rV2\r0\r1\r6\r4\r2\r4\r6\r8\r10\r▇▁▇▁▇\r\rV3\r0\r1\r7\r4\r3\r5\r7\r9\r11\r▇▁▇▁▇\r\rV4\r0\r1\r8\r4\r4\r6\r8\r10\r12\r▇▁▇▁▇\r\r\r\r\rReferences\rWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2019. Skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\rWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\r\r\r\rIn the sense that skim() provides useful, tidy summary statistics and displays it in a pretty form for exploratory analysis based on data frames. skim() cannot supplant summary() in terms of model interpretation, viewing geospatial objects, etc.↩\n\r\r\r","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574436464,"objectID":"ba458199b834103feb6980bf9eff16f6","permalink":"/post/introduction-to-skimr/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/post/introduction-to-skimr/","section":"post","summary":"Develpoed by the rOpenSci conmmunity, skimr is designed to provide useful, tidy summary statistics about variables.","tags":["skimr"],"title":"Introduction to skimr","type":"post"},{"authors":null,"categories":["Reading Notes"],"content":"\rMore than 50 years ago, John Tukey in his renowned “The Future of Dat Analysis” envisioned a would-be field, where the subject of interest is to learn from data. Other pioneers, like John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again exhorted academic statistics to embrace this so-called “data sicence / analysis” trend, to soar above theoretical statisticals which was characterized by theorems, proofs, etc. In this article, David Donoho(Donoho 2017) presents a vision of data science based on the activities of people who are “learning from data,” and he describes an academic field dedicated to improving that activity in an evidence-based manner.\nData Science vs. Statistics\rThe author first starts to distinguish data science from traditional statistics. Yet 3 commonly used indicators are discredited:\n\r“Big data” is not a credible criterion for meaningful distinction between statistics and data science, for both historical and scientific reasons, viz:\n\rThe very term “statistics” was coined at the beginning of modern efforts to compile census data, roughly the size of today’s big data\rStatisticians have long since delved into large databases, and come up with sufficient measures\r\rAlthough some would tout “new skills” on the part of data scientists, that is another “big data” meme cloaked in another term.\n\rData science programs doesn’t necessarily guide one to a satisfying job, comapared with statistics. Even it does, a data science master degree cannot exempt you from various constraints in real workplace\n\r\rSo, is there any solid case for an new entity called “data science” ? David proposed that data science would be a true science, an enlargement of traditional academic statistics. And the underlying motivation is intellectual rather than commerical. In the next 50 years, scientific publication in various disciplines will become a body of data that we can analyze and study, and the opportunity to improve the accuracy and validity of all science relies upon this would-be field: data science.\n\rReview on the concept\rBased on the timeline, the author then organizes insights that have been published over the years about data science, as shown below:\n\rAll in all I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data\n\rTukey identified four driving forces in the new science (Tukey 1962):\nThe formal theories of statistics\rAccelerating developments in computers and display devices\rThe challenge, in many fields, of more and ever larger bodies of data\rThe emphasis on quantification in an ever wider variety of disciplines\r\rLet’s spend a minute admiring Tukey’s prophecy…\nAmong other things, Tukey also pointed out that:\n\r…data analysis is a very difficult field. It must adapt itself to what people can and need to do with data. In the sense that biology is more complex than physics, and the behavioral sciences are more complex than either, it is likely that the general problems of data analysis are more complex than those of all three. It is too much to ask for close and effective guidance for data analysis from any highly formalized structure, either now or in the near future.\n\r\rData analysis can gain much from formal statistics, but only if the connection is kept adequately loose.\n\rJohn Chambers presented a choice between “greater” or “lesser” statistics. He argued\r(Chambers 1993):\n\rThe statistics profession faces a choice in its future research between continuing concentration on traditional topics—based largely on data analysis supported by mathematical statistics—and a broader viewpoint—based on an inclusive concept of learning from data. The latter course presents severe challenges as well as exciting opportunities. The former risks seeing statistics become increasingly marginal…\n\rC. F. Jeff Wu characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making, and called for courses outside out the statistics department.\nWilliam S. Cleveland put forward 6 foci of data science, which offered a great conceptual framework to study the filed even from today’s perspective (Cleveland 2001):\n\rMultidisciplinary investigations (25%)\n\rModels and Methods for Data (20%)\n\rComputing with Data (15%)\n\rPedagogy (15%)\n\rTool Evaluation (5%)\n\rTheory (20%)\n\r\rUC Berkeley statistician Leo Breiman brought his “two modeling cultures theory” into the exhortation. He asserted (Breiman and others 2001) :\n\rThere are two goals in analyzing the data:\nPrediction: To be able to predict what the responses are going to be to future input variables;\n[Inference]: To [infer] how nature is associating the response variables to the input variables.1\n\rProceeding from this basis, Brieman thought that the prdiction emphasis can be described as “predictive modeling” culture\", which lay stress on accuracy of prediction made by different algorithm on various datasets. But this culture, according to his estimation, is only practiced by 2% of academic statisticians.\n“Generative modeling culture” corresponds to the latter inference emphasis, and accounted for 98% statistical practice. Brienman said:\n\rIf our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on [generative] models\n\rFrom today’s standpoint, Breiman’s opinions is fairly justified. But why? (2017) thinks the Common Task Framework played a key role here.\nAn instance of the CTF has these ingredients( I actually have never heard the term before. To my mind, it’s just like a usual kaggle contest or a data visualization challenge, yet may not subjected ot choosing the “best” machine learning model or plot):\nA publicly available training dataset involving, for each observation, a list of (possibly many) feature measurements, and a class label for that observation.\n\rSet of enrolled competitors whose common task is to infer a class prediction rule from the training data.\n\rA scoring referee, to which competitors can submit their prediction rule. The referee runs the prediction rule against a testing dataset, which is sequestered behind a Chinese wall. The referee objectively and automatically reports the score (prediction accuracy) achieved by the submitted rule.\n\r\rThe synergy of minimizing prediction error with CTF is worth noting. This combination leads directly to a total focus on optimization of empirical performance, allows large numbers of researchers to compete at any given common task challenge, and allows for efficient and unemotional judging of challenge winners.\n\rThe Six Divisions\rFurther, to dipict an even larger professional on a quest to extract information from data, the author comes up with a classificcation concerning data science practices:\nData Gathering, Preparation, and Exploration\rData Representation and Transformation: To tranfrom data collected from a wealth of formats in an attempt to represent them in one format susceptible for analysis.(i.e., get features with acoustic data, one often transforms to the cepstrum or the Fourier transform)\n\rComputing with Data: Beyond basic knowledge of programming languages, data scientists need to keep current on new idioms for efficiently using those languages and need to understand the deeper issues associated with computational efficiency. Cluster and cloud computing and the ability to run massive numbers of jobs on such clusters has become an overwhelmingly powerful ingredient of the modern computational landscape.\rData Modeling\rData Visualization and Presentation\rScience about Data Science: Data scientists are doing science about data science when they identify commonly occurring analysis/processing workflows, for example, using data about their frequency of occurrence in some scholarly or business domain; when they measure the effectiveness of standard workflows in terms of the human time, the computing resource, the analysis validity, or other performance metric, and when they uncover emergent phenomena in data analysis, for example, new patterns arising in data analysis workflows, or disturbing artifacts in published analysis results.\r\r2\n\rScience About Data Science\rDavid then elaborates on the 6th part: science about data Science, which has made data science a true branch of science, rather than a broad collection of technical activities. Data science offered a continually evolving, evidence-based approach for analysts, covering:\n\rScience-Wide Meta Analysis: the idea of “analyzing what people have analyzed” across disciplines\n\rCross-Study Analysis: To study the validation of different studies on the same dataset. * Cross-Workflow Analysis: To study the impact of analysis workflow on results\r\rAll these 3 approaches, comtribute to enhancing the validity of the scientific literature.\n\rNext 50 years\rAt the end, what about the next 50 years of data science?\n\rResearch will be more open and reproducible. Code sharing, data sharing will allow large numbers of datasets and analysis workflows to be derived from studies science-wide\n\rAll scientific publifications will be transformed to data thta can be mined\n\rAmple data will be available to measure the performance of algorithms and models across a whole ensemble of situations\r\r\rConclusion\r3\nEach proposed notion of data science involves some enlargement of academic statistics and machine learning. The “GDS” variant specifically discussed in this article derives from insights about data analysis and modeling stretching back decades. In this variant, the core motivation for the expansion to data science is intellectual. In the future, there may be great industrial demand for the skills inculcated by GDS; however, the core questions which drive the field are scientific, not industrial.\nGDS(initialtive for greater data science) proposes that data science is the science of learning from data; it studies the methods involved in the analysis and processing of data and proposes technology to improve methods in an evidence-based manner. The scope and impact of this science will expand enormously in coming decades as scientific data and data about science itself become ubiquitously available.\nSociety already spends tens of billions of dollars yearly on scientific research, and much of that research takes place at universities. GDS inherently works to understand and improve the validity of the conclusions produced by university research, and can play a key role in all campuses where data analysis and modeling are major activities.\n\rReferences\rBreiman, Leo, and others. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231.\n\rChambers, John M. 1993. “Greater or Lesser Statistics: A Choice for Future Research.” Statistics and Computing 3 (4): 182–84.\n\rCleveland, William S. 2001. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” International Statistical Review 69 (1): 21–26.\n\rDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\rTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\rXie, Yihui. 2019. Knitr: A General-Purpose Package for Dynamic Report Generation in R. https://CRAN.R-project.org/package=knitr.\n\r\r\r\rThe original line was slightly modified by David, the original has “information” in place of [inference] and “extract some information about” in place of [infer]↩\n\rHere I skipped some discussion related to R as an computing environment, literate programming in knitr(Xie 2019) and the “tidy data” concept↩\n\rFor now, this section is copied from the original article↩\n\r\r\r","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"c585a69aeb3a78849a473d7c63036da6","permalink":"/post/50-years-of-data-science/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/post/50-years-of-data-science/","section":"post","summary":"Reading notes on \"50 Years of Data Science\" by David Dinoho","tags":["Data Science"],"title":"50 Years of Data Science","type":"post"},{"authors":null,"categories":null,"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"0d1cccef2129e36aab479be56865fda8","permalink":"/project/advanced-r-graphics/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/project/advanced-r-graphics/","section":"project","summary":"A personal manual of Data Visualization in R","tags":["R"],"title":"Advanced R Graphics","type":"project"},{"authors":[],"categories":["Reading Notes"],"content":"\r“Enterprise Data Analysis and Visualization: An Interview Study” is an effort to penetrate into enterprises and glean real-world data science experience from analysts directly. By conducting semi-structured interviews with 35 data analysts, (2012) provides insight on 3 archetypes describing different workflows and tasks given, common pain points analysts may encounter, future trends, etc.\n3 Archetypes\rAccording to the responses, we can see analysts generally fall into 3 archetypes: hacker, scripter and application user.\nHackers are faced with the most diverse and complex tasks. They are most literate in terms of programming and thus rarely ask IT staffs fro help. More oftern than not, hackers master more than three languages, R / Matlab for analysis, Python, Perl as an scripting language, and SQL for queries.\n I guess for now Python has gained ground also as a data analysis language, and Matlab is less popular among analysts doing “practical” analysis, but more frquently used in labs of nature science.   Also note that with their ability to collect, manipulate data, some hackers could be in charge of managing the data warehouse of the company, and dealing with large datasets. In constrast, they perform less statistical models, and spend more time in early-stage analytic activities prior to modeling, if any.\nScripters take care of advanced modeling and use a software package such as R or Matlab extensively. They are less proficient when parsing log files or scraping data off the web, and the data susceptible for modeling are often prepared by IT staff.\nApplication user prefer operations done in a spreadsheet( Mostly in Excel ), or other analysis application (e.g., SAS / JMP, SPSS, etc.). Data are also pulled out from several relationald databases prior to their work. They typically worked on smaller datasets than other gorups, advanced application users may wrote scripts using an embedded language such as Visual Basic.\n To my mind, application users dipicted here seem to be titled “data analysts” for historical reasons, and there is little case for keeping such a position when there are already hackers and scripters. Perhaps they were traditional bussiness people in charge of analysis or accountants, so they are most comforatble with Microsoft Excel or SPSS. As the “big data” meme started to present itself, all of a sudden their enterprise felr it imperative to set up a “data scientist / analyst” position to keep up with this new trend. Yet I doubt if this archetype could still survive when graduates from data.   \rAnalysts within organization\rAnalysts interact closely with IT staff to complete aspects of theire job. For IT staff, his relationship includes data ingesting and acquiring, operationalizing recurring workflows, and serve as a source of documentation when analysts, say, con’t figure out how to wirte a complex SQL statement.\nThis reliance on IT staff was particularly true in organizations where data was distributed across many different data sources. Hackers were most likely to use the IT team explicitly for this function, as they were more likely to access data directly from the warehouse. Scripters and application users relied on this function implicitly when receiving data from members of IT.\nAnother thread of this topic is distributed data, which are generated from multiple departments of the enterprise. They are often stored in various databases and formats. adding to the diffculty of intergrating them.\nWhen analysis is finished, analysts typically shared static reports in the form of template documents. In some cases, reports could be interactive dashboards that enabled end users to filter or modify statistics computed. It’s not hard to imagine consumers of the report often give a blurred image of what they want, and hardly could analysts translate them into practical data problems.\nWhen it comes to collaboration, it is an exception rather than the rule for analysts. They do share some central repository data processing scriptes are kept to oneself as a rule. Ont the other hand, final reports in the form of charts or model summary are commonly shared among analytsts in planning meeting or presentations. These reports, however, are rarely parametrizable or interactive.\n(2012) identifies three impediemnts to collaboration:\n\rthe diversity of tools and programming languages\n\rfinding a script or inter-\rmediate data product someone else produced was often more time-\rconsumingthanwritingthescriptfromscratch\n\rmany analysis process are “ad hoc”, “experimental”\r\r\rChallenges\r(2012) identified five high-level tasks in the workflow and challenges within each of them.\nDiscovering1: Since data are often distributed across multiple databases and sources, finding the exact data sheet or file needed can be time-consuming, not to mention that some analysts only have restricted access to the data warehhouse. Another problem is field definitions, these definitions were\roften missing in relational databases and non-existent in other types of data stores.\nWranling: Many analysts reported parsing, ingesting semi-structured data(i.e., log files, block data2). Another difficulty is integrating the data, after analysts manage to find them in databases. Sometimes identifiers are missing or encoded inconsistantly in some databases, and sometimes there is no column than could be an identifier.\nProfiling3: Many analysts (22 / 35) reports issues dealing with missing data and heterogeneous data in a column. When detecting outliers, there is no general agreement between visualization and traditional staistical methods.\nModeling: The biggest challenge in constructing an model according to respondents are feature selection, whether to choose a set of variables, which to transfrom and how to transform.\n There is a great book on feature engineering and selectin by Max Kuhn and Kjell Johnson: Feature Engineering and Selection: A Practical Approach for Predictive Models   Most repondents also pointed to the scalibility of existing analysis and visualization tools. Hackers are less limited by large amounts of data, obviously, but hackers were often limited by the types of analysis they could run because useful models or algorithms did not have available parallelized implementations. Visualizing model results is another pain point, analysts using more advanced machine learning methods (14/35) expressed a desire for visualization tools to help explore these models and visualize their output.\n R’s package broom (Robinson and Hayes 2019) are desgined to facilitate modeling diagnosis, visualization, etc. tidymodels (Kuhn and Wickham 2019) contains a burgeoning list of such packages.   Reporing: The two most-cited challenges in reporoting were communicating assumptions and building interactive reports. Documentation that should have been provided alongside the report are often missing or poorly written. Even when assumptions were tracked, they were often treated as footnotes instead of first-class results. Moreover, analysts complained that reports were too\rinflexible and did not allow interactive verification or sensitivity analysis.\n\rFuture trends\rHeer (2012) then prophesy future trends in technology and the analytic workforce:\nPublic data become more accessible and add to the diffculty of data discovery and intergration.\nThe market of Hadoop-like software will continue to increase, allowing analysts to operate on less structured data formats.\n“Hacker-level” analysts will be in demand, analysts therefore need to be adept at both statistical reasoning and writing complex SQL or Map-Reduce code. Those who are comfortable in multiple data processing / analysis frameworks will be competent.\nThe size of analytic teams should grow, efficient collaboration will become both increasingly important and difficult.4\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2019. Rmarkdown: Dynamic Documents for R. https://CRAN.R-project.org/package=rmarkdown.\n\rChang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2019. Shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny.\n\rHeer, Sean Kandel AND Andreas Paepcke AND Joseph Hellerstein AND Jeffrey. 2012. “Enterprise Data Analysis and Visualization: An Interview Study.” In IEEE Visual Analytics Science \u0026amp; Technology (Vast). http://idl.cs.washington.edu/papers/enterprise-analysis-interviews.\n\rKuhn, Max, and Hadley Wickham. 2019. Tidymodels: Easily Install and Load the ’Tidymodels’ Packages. https://CRAN.R-project.org/package=tidymodels.\n\rLuraschi, Javier, Kevin Kuo, Kevin Ushey, JJ Allaire, and The Apache Software Foundation. 2019. Sparklyr: R Interface to Apache Spark. https://CRAN.R-project.org/package=sparklyr.\n\rRobinson, David, and Alex Hayes. 2019. Broom: Convert Statistical Analysis Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\rWickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\rXie, Yihui. 2019. Knitr: A General-Purpose Package for Dynamic Report Generation in R. https://CRAN.R-project.org/package=knitr.\n\r\r\r\rHere discovering means acquire data necessary to complete analysis tasks↩\n\rIn a block format, logical records of data are spread\racross multiple lines of a file. Typically one line (the “header”) con-\rtains metadata about the record, such as how many of the subsequent\rlines (the “payload”) belong to the record↩\n\rTo verify data qualityand its suitability for the analysis tasks. Example tasks include inspecting outliers, examining distributions↩\n\rIn the last section, the author move on to discuss improvements that can be made to analytic tools. I figure these suggestions are somehow outdated by today’s standard, and tidyverse(Wickham 2017)、knitr(Xie 2019)、rmarkdown(Allaire et al. 2019)、shiny(Chang et al. 2019)、sparklyr(Luraschi et al. 2019) and some other packages in R have already given fine solutions to them, so this section is skipped.↩\n\r\r\r","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574332923,"objectID":"3853cd40bf40f7bb1c51b3b6f99cb6ab","permalink":"/post/enterprise-data-analysis-and-visualization-an-interview-study/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/post/enterprise-data-analysis-and-visualization-an-interview-study/","section":"post","summary":"Reading notes on \"Enterprise Data Analysis and Visualization: An Interview Study\" by Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer","tags":["Data Science"],"title":"Enterprise Data Analysis and Visualization: An Interview Study","type":"post"},{"authors":null,"categories":["Random Thoughts"],"content":"With the ever-growing expansion of higher education, today’s world has seen college graduates growing in larger numbers, leading to fierce competition in the talent market, where a subtle craze for various of certificates is on the rise and here to stay. Students took several weeks or months cramming for an examination to get one certificate for qualification, and they aimed at another certificate afterwards, irrespective of what previous cramming has brought him or future learning routes on the subject concerned. What’s more, various training institutions sprouting up these days are also in a ferment. They go out their way to inculcate in the minds of patrons the importance of those certificates. Implanted the beliefs, many are under the illusion that certificates are panacea and selling points for their resume, that it would open up hidden opportunities. These convictions, however inappropriate, add to the mania.\nDoes a cupboard bedecked with certificates necessarily means competence? The answer is a qualified yes. The ultimate value of any accomplishment, including gaining certificates, depends on the what kind of capability to which it attests and what it will bring to us aside from physical evidence of any sort. Work that are too ambiguous to demonstrate certain abilities are rarely helpful, thus is not worth questing. And Work that revolves around rote learning without inspiring, is another slippery slope.\nCertificates offered nowadays are multifarious. Take university students for example, one has a wide selection among CET 4 and CET 6 certificates, computer certificate, lawyer certificate, tour guide certificate, certified public accountant, certified tax accountant etc. And it is because this variety and inclusiveness, on the part of all these certificates, that make the majority of them don’t lend themselves to an adequate testament to the specific calibre that the workplace calls for. More importantly, some college students prepare for these examinations at the cost of their major disciplines, which could do more harm than good to their careers in long terms and defeat the purpose of certificate-gaining.\nFor another, learning is an end in itself. It is those confounding moments, self-discovery after self-doubt that make knowledge so desirable. A healthy way of learning inculcates in students a love for the subject, not just what the test requires, so the students can, and most importantly will, make the subject their business to learn. Most students, on the other hand, are swamped by exam-oriented education, deprived of the joy of mental adventure, so that creativity and ingenuous pursuit of knowledge can never be attained. While they are preparing for the examination related to certificates, reference books, didactic teaching, rote learning and cramming schooling play a major role. They are told so, and without serious consideration, another quiz begins. This process, however, produces only dilettantes with a smattering of knowledge that would soon grow unlearned. Of greater concern is that they may become unable to explore a new field without an all-knowing guide, but there is rarely a guide in practical working experience.\nBy no means am I making the conclusion that certificates are pure evil, for they can be equally used for good. Certificates and examinations correlated to one’s interests and consistent with his / her professional aspirations can, in most cases, cultivate one’s ability and guide them through the job market.\nOn balance, I believe that certificates do little to enhance one’s capability until we have poured in meaning efforts in something that would give meaning help in the days to come. Certificates itself is neutral, and we could use it to our advantage once we follow an appropriate way to learn. We should always acquire knowledge from the process and keep exploring and thinking instead of be guided by utilitarianism. Only in this way can we truly improve our abilities.\n","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"ded16ae08036988c0c27ae40f5dc55aa","permalink":"/post/of-certificate/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/post/of-certificate/","section":"post","summary":"Do certificates necessarily enhance one's capabilities?","tags":["education"],"title":"Of Certificate","type":"post"},{"authors":null,"categories":null,"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"c6f5edaf7abb2dd4930683b6d7f0d880","permalink":"/project/r4ds/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/project/r4ds/","section":"project","summary":"Notes based on \"R for Data Science\"","tags":["R"],"title":"R4DS","type":"project"},{"authors":null,"categories":null,"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"8b46ed2c3bc92c0708f89cd3469824b5","permalink":"/project/readings/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/project/readings/","section":"project","summary":"Readings in applied data science","tags":["Other"],"title":"Readings","type":"project"}]