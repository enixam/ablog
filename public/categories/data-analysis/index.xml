<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis | Qiushi Yan</title>
    <link>/categories/data-analysis/</link>
      <atom:link href="/categories/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Qiushi Yan © 2020</copyright><lastBuildDate>Thu, 21 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Data Analysis</title>
      <link>/categories/data-analysis/</link>
    </image>
    
    <item>
      <title>Excess Death for 2020</title>
      <link>/post/2020-05-21-excess-death-for-2020/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-21-excess-death-for-2020/</guid>
      <description>


&lt;div class=&#34;box&#34;&gt;
&lt;p&gt;This is my normal chunk&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;puzzle&#34;&gt;
&lt;p&gt;this my puzzle chunk&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Graduate Admission for Internaitonal student</title>
      <link>/post/2020-05-18-predicting-graduate-admission/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-18-predicting-graduate-admission/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/eswarchandt/admission&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/eswarchandt/admission&lt;/a&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)


admission &amp;lt;- readxl::read_excel(&amp;quot;D:/RProjects/data/blog/admission.xlsx&amp;quot;) %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  mutate(admit = factor(admit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

admission_split &amp;lt;- initial_split(admission)
admission_test &amp;lt;- testing(admission_split)
admission_train &amp;lt;- training(admission_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- recipe(admit ~ ., data = admission_train) %&amp;gt;% 
  step_mutate_at(all_predictors(), -gre, -gpa, fn = factor) %&amp;gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&amp;gt;% 
  step_normalize(gre, gpa)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Biterm Topic Modeling for Danmaku on Bilibili</title>
      <link>/post/2020-05-16-biterm-topic-modeling-for-danmaku-on-bilibili/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-16-biterm-topic-modeling-for-danmaku-on-bilibili/</guid>
      <description>


&lt;p&gt;btm package: &lt;a href=&#34;https://github.com/bnosac/BTM&#34; class=&#34;uri&#34;&gt;https://github.com/bnosac/BTM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;definition of danmaku &lt;a href=&#34;https://blogs.ubc.ca/titus283/2018/02/03/43/&#34; class=&#34;uri&#34;&gt;https://blogs.ubc.ca/titus283/2018/02/03/43/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;bilibili &lt;a href=&#34;https://en.wikipedia.org/wiki/Bilibili&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Bilibili&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring policing activity in Rhode Island</title>
      <link>/post/rhode-policing-activity/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/rhode-policing-activity/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-the-relationship-between-gender-race-and-policing&#34;&gt;Exploring the relationship between gender, race and policing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#temporal-pattern-of-drug-related-stops-and-search-rates&#34;&gt;Temporal pattern of drug related stops and search rates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-of-violation-across-zones&#34;&gt;Distribution of violation across zones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weather-impact-on-policing-behaviour&#34;&gt;Weather Impact on policing behaviour&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This post is about completing an exploratory data analysis project using the &lt;code&gt;pandas&lt;/code&gt; library in Python. The data comes from the &lt;a href=&#34;https://openpolicing.stanford.edu&#34;&gt;Stanford Open Policing Project&lt;/a&gt;, which releases cleaned data about vehicle and pedestrian stops from the police across over 30 states in the USA. I focus only on traffic stops by police officers in the state of Rhode Island here.&lt;/p&gt;
&lt;p&gt;Some questions include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The relationship between search rate, types of search, types of violation, gender and race.&lt;/li&gt;
&lt;li&gt;How do the number of drug related stops and search rate change during the past few years?&lt;/li&gt;
&lt;li&gt;Is there a patten in violations across different zones of Rhode Island?&lt;/li&gt;
&lt;li&gt;Does weather affect arrest rate?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data preprocessing&lt;/h1&gt;
&lt;p&gt;Let’s begin by loading the library and data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri = pd.read_csv(&amp;quot;D:/RProjects/data/stanford-open-policing/rhode_traffic_stops.csv&amp;quot;)
print(ri.head())
#&amp;gt;   state   stop_date stop_time  ...  stop_duration drugs_related_stop district
#&amp;gt; 0    RI  2005-01-04     12:55  ...       0-15 Min              False  Zone X4
#&amp;gt; 1    RI  2005-01-23     23:15  ...       0-15 Min              False  Zone K3
#&amp;gt; 2    RI  2005-02-17     04:15  ...       0-15 Min              False  Zone X4
#&amp;gt; 3    RI  2005-02-20     17:15  ...      16-30 Min              False  Zone X1
#&amp;gt; 4    RI  2005-02-24     01:20  ...       0-15 Min              False  Zone X3
#&amp;gt; 
#&amp;gt; [5 rows x 15 columns]
print(ri.columns)
#&amp;gt; Index([&amp;#39;state&amp;#39;, &amp;#39;stop_date&amp;#39;, &amp;#39;stop_time&amp;#39;, &amp;#39;county_name&amp;#39;, &amp;#39;driver_gender&amp;#39;,
#&amp;gt;        &amp;#39;driver_race&amp;#39;, &amp;#39;violation_raw&amp;#39;, &amp;#39;violation&amp;#39;, &amp;#39;search_conducted&amp;#39;,
#&amp;gt;        &amp;#39;search_type&amp;#39;, &amp;#39;stop_outcome&amp;#39;, &amp;#39;is_arrested&amp;#39;, &amp;#39;stop_duration&amp;#39;,
#&amp;gt;        &amp;#39;drugs_related_stop&amp;#39;, &amp;#39;district&amp;#39;],
#&amp;gt;       dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save future efforts, we have some preprocessing jobs to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;drop columns and rows&lt;/li&gt;
&lt;li&gt;make sure that relevant columnns are in the right data type.&lt;/li&gt;
&lt;li&gt;make use of pandas’s datetime index&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A glance at missing values will show that &lt;code&gt;county_name&lt;/code&gt; are all missing. Also, those observations with missing &lt;code&gt;driver_gender&lt;/code&gt; and &lt;code&gt;driver_race&lt;/code&gt; is of little value for our purposes. And the &lt;code&gt;state&lt;/code&gt; column is redundant.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri.isnull().mean()
#&amp;gt; state                 0.000000
#&amp;gt; stop_date             0.000000
#&amp;gt; stop_time             0.000000
#&amp;gt; county_name           1.000000
#&amp;gt; driver_gender         0.056736
#&amp;gt; driver_race           0.056703
#&amp;gt; violation_raw         0.056703
#&amp;gt; violation             0.056703
#&amp;gt; search_conducted      0.000000
#&amp;gt; search_type           0.963953
#&amp;gt; stop_outcome          0.056703
#&amp;gt; is_arrested           0.056703
#&amp;gt; stop_duration         0.056703
#&amp;gt; drugs_related_stop    0.000000
#&amp;gt; district              0.000000
#&amp;gt; dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As to data types, &lt;code&gt;is_arrested&lt;/code&gt; is now recognized as objects, but should be boolean values instead.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri.dtypes
#&amp;gt; state                  object
#&amp;gt; stop_date              object
#&amp;gt; stop_time              object
#&amp;gt; county_name           float64
#&amp;gt; driver_gender          object
#&amp;gt; driver_race            object
#&amp;gt; violation_raw          object
#&amp;gt; violation              object
#&amp;gt; search_conducted         bool
#&amp;gt; search_type            object
#&amp;gt; stop_outcome           object
#&amp;gt; is_arrested            object
#&amp;gt; stop_duration          object
#&amp;gt; drugs_related_stop       bool
#&amp;gt; district               object
#&amp;gt; dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri[&amp;quot;is_arrested&amp;quot;].head()
#&amp;gt; 0    False
#&amp;gt; 1    False
#&amp;gt; 2    False
#&amp;gt; 3     True
#&amp;gt; 4    False
#&amp;gt; Name: is_arrested, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, &lt;code&gt;stop_date&lt;/code&gt; and &lt;code&gt;stop_time&lt;/code&gt; will be turned into best advantage as a datetime index.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri[[&amp;quot;stop_date&amp;quot;, &amp;quot;stop_time&amp;quot;]].head()
#&amp;gt;     stop_date stop_time
#&amp;gt; 0  2005-01-04     12:55
#&amp;gt; 1  2005-01-23     23:15
#&amp;gt; 2  2005-02-17     04:15
#&amp;gt; 3  2005-02-20     17:15
#&amp;gt; 4  2005-02-24     01:20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combine these steps yields a relatively clean version of this dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri[&amp;quot;is_arrested&amp;quot;] = ri.is_arrested.astype(&amp;quot;bool&amp;quot;)
dt_index = ri.stop_date.str.cat(ri.stop_time, sep = &amp;quot; &amp;quot;)
ri[&amp;quot;stop_datetime&amp;quot;] = pd.to_datetime(dt_index)

ri_clean = (ri.
  drop([&amp;quot;county_name&amp;quot;, &amp;quot;state&amp;quot;], axis = &amp;quot;columns&amp;quot;).
  dropna(subset = [&amp;quot;driver_gender&amp;quot;, &amp;quot;driver_race&amp;quot;]).
  set_index(&amp;quot;stop_datetime&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_clean.head()
#&amp;gt;                       stop_date stop_time  ... drugs_related_stop district
#&amp;gt; stop_datetime                              ...                            
#&amp;gt; 2005-01-04 12:55:00  2005-01-04     12:55  ...              False  Zone X4
#&amp;gt; 2005-01-23 23:15:00  2005-01-23     23:15  ...              False  Zone K3
#&amp;gt; 2005-02-17 04:15:00  2005-02-17     04:15  ...              False  Zone X4
#&amp;gt; 2005-02-20 17:15:00  2005-02-20     17:15  ...              False  Zone X1
#&amp;gt; 2005-02-24 01:20:00  2005-02-24     01:20  ...              False  Zone X3
#&amp;gt; 
#&amp;gt; [5 rows x 13 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-relationship-between-gender-race-and-policing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring the relationship between gender, race and policing&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;search_conducted&lt;/code&gt; column indicates whether or not a vehicle were searched by an officer:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_clean.search_conducted.head()
#&amp;gt; stop_datetime
#&amp;gt; 2005-01-04 12:55:00    False
#&amp;gt; 2005-01-23 23:15:00    False
#&amp;gt; 2005-02-17 04:15:00    False
#&amp;gt; 2005-02-20 17:15:00    False
#&amp;gt; 2005-02-24 01:20:00    False
#&amp;gt; Name: search_conducted, dtype: bool&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can break the average search rate by gender&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# search rate for men and women
ri_clean.groupby(&amp;quot;driver_gender&amp;quot;).search_conducted.mean()
#&amp;gt; driver_gender
#&amp;gt; F    0.019181
#&amp;gt; M    0.045426
#&amp;gt; Name: search_conducted, dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a marked difference considering the base volume. This could be misleading due to the existance of some confounding variables. For exmaple, the difference in search rate between males and females could be explained by the fact that they tend to commit different violations. For this reason I’ll divide the result by types of violation to see if there is a universal rise in the possibility of being searched from women to men.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_clean.groupby([&amp;quot;violation&amp;quot;, &amp;quot;driver_gender&amp;quot;]).search_conducted.mean()
#&amp;gt; violation            driver_gender
#&amp;gt; Equipment            F                0.039984
#&amp;gt;                      M                0.071496
#&amp;gt; Moving violation     F                0.039257
#&amp;gt;                      M                0.061524
#&amp;gt; Other                F                0.041018
#&amp;gt;                      M                0.046191
#&amp;gt; Registration/plates  F                0.054924
#&amp;gt;                      M                0.108802
#&amp;gt; Seat belt            F                0.017301
#&amp;gt;                      M                0.035119
#&amp;gt; Speeding             F                0.008309
#&amp;gt;                      M                0.027885
#&amp;gt; Name: search_conducted, dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For all types of violation, the search rate is higher for males than for females, disproving our hypothesis related to confounding variales, by and large.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;search_type&lt;/code&gt; column contains information about categories of searching on the part of the police. I investigate the relative proportion of the most common search types for 4 races.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;condition = ri_clean.driver_race.isin([&amp;quot;White&amp;quot;, &amp;quot;Black&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Asian&amp;quot;]) &amp;amp; ri_clean.search_type.isin([&amp;quot;Incident to Arrest&amp;quot;, &amp;#39;Probable Cause&amp;#39;, &amp;#39;Inventory&amp;#39;])

search_type_by_race = (ri_clean[condition].
  groupby(&amp;quot;driver_race&amp;quot;).
  search_type.
  value_counts(normalize = True).
  unstack()
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
search_type_by_race.plot(kind = &amp;quot;bar&amp;quot;)
plt.title(&amp;quot;Proportion of common search types across 4 races&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-16-exploring-policing-activity-in-Rhode-Island/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we come to the question that if race played a role in whether or not someone is frisked during a search (coded in &lt;code&gt;search_type&lt;/code&gt; by “Protective Frisk”). Since a search instance can be of multiple types, such as “Protective Frisk and Reasonable Suspicion”, I create a new column indicating if &lt;code&gt;search_type&lt;/code&gt; contains protective firsk, and calculate its rate across races.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_clean[&amp;quot;frisk&amp;quot;] = ri_clean.search_type.str.contains(&amp;quot;Protective Frisk&amp;quot;).astype(&amp;quot;bool&amp;quot;)

(ri_clean[ri_clean.search_conducted == True].
  groupby(&amp;quot;driver_race&amp;quot;).
  frisk.
  mean())
#&amp;gt; driver_race
#&amp;gt; Asian       0.081633
#&amp;gt; Black       0.080194
#&amp;gt; Hispanic    0.063545
#&amp;gt; Other       0.000000
#&amp;gt; White       0.106325
#&amp;gt; Name: frisk, dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like white people has a slightly higher frisk rate. But there is no conclusion to be made without more detailed background information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;temporal-pattern-of-drug-related-stops-and-search-rates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Temporal pattern of drug related stops and search rates&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;drugs_related_stop&lt;/code&gt; shows if a traffic stop eneded in the spotting of drugs in the vehicle. Resampling the column annually gives the drug rate over years, and annual search rate is calculated as a reference.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;annual_drug_rate = ri_clean.drugs_related_stop.resample(&amp;quot;A&amp;quot;).mean()
annual_search_rate = ri_clean.search_conducted.resample(&amp;quot;A&amp;quot;).mean()
annual = pd.concat([annual_drug_rate, annual_search_rate], axis=&amp;quot;columns&amp;quot;)
annual.plot(subplots = True)
#&amp;gt; array([&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000002AADF548&amp;gt;,
#&amp;gt;        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000050650DC8&amp;gt;],
#&amp;gt;       dtype=object)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-16-exploring-policing-activity-in-Rhode-Island/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The rate of drug-related stops increased even though the search rate decreased.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-violation-across-zones&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of violation across zones&lt;/h1&gt;
&lt;p&gt;Speeding is the most common violation in all districts.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;all_zones = pd.crosstab(ri_clean.district, ri_clean.violation)
all_zones.plot(kind = &amp;#39;bar&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-16-exploring-policing-activity-in-Rhode-Island/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weather-impact-on-policing-behaviour&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Weather Impact on policing behaviour&lt;/h1&gt;
&lt;p&gt;This section uses a second dataset to explore the impact of weather conditions on police behavior during traffic stops.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;weather = pd.read_csv(&amp;quot;D:/RProjects/data/stanford-open-policing/rhode_weather.csv&amp;quot;)
weather.head()
#&amp;gt;        STATION        DATE  TAVG  TMIN  TMAX  ...  WT17  WT18  WT19  WT21  WT22
#&amp;gt; 0  USW00014765  2005-01-01  44.0    35    53  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 1  USW00014765  2005-01-02  36.0    28    44  ...   NaN   1.0   NaN   NaN   NaN
#&amp;gt; 2  USW00014765  2005-01-03  49.0    44    53  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 3  USW00014765  2005-01-04  42.0    39    45  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 4  USW00014765  2005-01-05  36.0    28    43  ...   NaN   1.0   NaN   NaN   NaN
#&amp;gt; 
#&amp;gt; [5 rows x 27 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;weather&lt;/code&gt; data is collected by the national centers for environmental information. Because &lt;code&gt;ri&lt;/code&gt; lacks spatial information like latitude / longitutde, we use only the data observed by a weather station in the center of Rhode Island. Columns in &lt;code&gt;weather&lt;/code&gt; that starts with &lt;code&gt;WT&lt;/code&gt; represents a bad weather condition, and take value at either 1 (present) or 0 (present).&lt;/p&gt;
&lt;p&gt;To measure overall weather conditions on a single day, I tally all the &lt;code&gt;WT&lt;/code&gt; columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;weather[&amp;quot;bad_conditions&amp;quot;] = (weather.loc[:,&amp;#39;WT01&amp;#39;:&amp;#39;WT22&amp;#39;].
  fillna(0).
  sum(axis = &amp;quot;columns&amp;quot;).
  astype(&amp;quot;int&amp;quot;))
  
weather.bad_conditions.plot(kind = &amp;quot;hist&amp;quot;, bins = 10)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-16-exploring-policing-activity-in-Rhode-Island/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, I split the daily weather into a categorical variable with 3 categories based on &lt;code&gt;bad_conditions&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from pandas.api.types import CategoricalDtype
mapping = {0:&amp;#39;good&amp;#39;, 1:&amp;#39;bad&amp;#39;, 2:&amp;#39;bad&amp;#39;, 3: &amp;quot;bad&amp;quot;, 4: &amp;quot;bad&amp;quot;,
    5: &amp;quot;worse&amp;quot;, 6: &amp;quot;worse&amp;quot;, 7: &amp;quot;worse&amp;quot;, 8: &amp;quot;worse&amp;quot;, 9: &amp;quot;worse&amp;quot;
}

weather[&amp;#39;rating&amp;#39;] = (weather.bad_conditions.
  map(mapping).
  astype(CategoricalDtype(categories = [&amp;#39;good&amp;#39;, &amp;#39;bad&amp;#39;, &amp;quot;worse&amp;quot;], ordered = True)))

# Count the unique values in &amp;#39;rating&amp;#39;
print(weather.rating.value_counts())
#&amp;gt; bad      1836
#&amp;gt; good     1749
#&amp;gt; worse     432
#&amp;gt; Name: rating, dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;weather.head()
#&amp;gt;        STATION        DATE  TAVG  TMIN  ...  WT21  WT22  bad_conditions  rating
#&amp;gt; 0  USW00014765  2005-01-01  44.0    35  ...   NaN   NaN               2     bad
#&amp;gt; 1  USW00014765  2005-01-02  36.0    28  ...   NaN   NaN               2     bad
#&amp;gt; 2  USW00014765  2005-01-03  49.0    44  ...   NaN   NaN               3     bad
#&amp;gt; 3  USW00014765  2005-01-04  42.0    39  ...   NaN   NaN               4     bad
#&amp;gt; 4  USW00014765  2005-01-05  36.0    28  ...   NaN   NaN               4     bad
#&amp;gt; 
#&amp;gt; [5 rows x 29 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;rating&lt;/code&gt; turn to be a easy indicator of weather condition, I’ll join two dataframes, &lt;code&gt;ri_clean&lt;/code&gt; and &lt;code&gt;weather&lt;/code&gt;, to finalize the analysis.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_clean = ri_clean.reset_index()

weather_rating = weather[[&amp;quot;DATE&amp;quot;, &amp;quot;rating&amp;quot;]].rename(columns = {&amp;quot;DATE&amp;quot;: &amp;quot;stop_date&amp;quot;})
ri_weather = pd.merge(ri_clean, weather_rating, how = &amp;quot;left&amp;quot;).set_index(&amp;quot;stop_datetime&amp;quot;)
ri_weather.head()
#&amp;gt;                       stop_date stop_time driver_gender  ... district frisk rating
#&amp;gt; stop_datetime                                            ...                      
#&amp;gt; 2005-01-04 12:55:00  2005-01-04     12:55             M  ...  Zone X4  True    bad
#&amp;gt; 2005-01-23 23:15:00  2005-01-23     23:15             M  ...  Zone K3  True  worse
#&amp;gt; 2005-02-17 04:15:00  2005-02-17     04:15             M  ...  Zone X4  True   good
#&amp;gt; 2005-02-20 17:15:00  2005-02-20     17:15             M  ...  Zone X1  True    bad
#&amp;gt; 2005-02-24 01:20:00  2005-02-24     01:20             F  ...  Zone X3  True    bad
#&amp;gt; 
#&amp;gt; [5 rows x 15 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare the arrest rate, divided by weather condition and types of violation.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arrest_rate = ri_weather.groupby([&amp;quot;violation&amp;quot;, &amp;quot;rating&amp;quot;]).is_arrested.mean()
arrest_rate
#&amp;gt; violation            rating
#&amp;gt; Equipment            good      0.059007
#&amp;gt;                      bad       0.066311
#&amp;gt;                      worse     0.097357
#&amp;gt; Moving violation     good      0.056227
#&amp;gt;                      bad       0.058050
#&amp;gt;                      worse     0.065860
#&amp;gt; Other                good      0.076966
#&amp;gt;                      bad       0.087443
#&amp;gt;                      worse     0.062893
#&amp;gt; Registration/plates  good      0.081574
#&amp;gt;                      bad       0.098160
#&amp;gt;                      worse     0.115625
#&amp;gt; Seat belt            good      0.028587
#&amp;gt;                      bad       0.022493
#&amp;gt;                      worse     0.000000
#&amp;gt; Speeding             good      0.013405
#&amp;gt;                      bad       0.013314
#&amp;gt;                      worse     0.016886
#&amp;gt; Name: is_arrested, dtype: float64
arrest_rate.unstack().plot(kind = &amp;quot;bar&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-16-exploring-policing-activity-in-Rhode-Island/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Generally, the arrest rate is higher when weather condition gets worse. This doesn’t prove a causal link, but it’s quite an interesting result! Also, this plot can be illustrated via a pivot table.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ri_weather.pivot_table(index = &amp;quot;violation&amp;quot;, columns = &amp;quot;rating&amp;quot;, values = &amp;quot;is_arrested&amp;quot;)
#&amp;gt; rating                   good       bad     worse
#&amp;gt; violation                                        
#&amp;gt; Equipment            0.059007  0.066311  0.097357
#&amp;gt; Moving violation     0.056227  0.058050  0.065860
#&amp;gt; Other                0.076966  0.087443  0.062893
#&amp;gt; Registration/plates  0.081574  0.098160  0.115625
#&amp;gt; Seat belt            0.028587  0.022493  0.000000
#&amp;gt; Speeding             0.013405  0.013314  0.016886&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Police Activity with Pandas</title>
      <link>/post/police-activity-pandas/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/police-activity-pandas/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Analyzing Ted Talks</title>
      <link>/post/2020-05-14-analyzing-ted-talks/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-14-analyzing-ted-talks/</guid>
      <description>


&lt;p&gt;ted talk with topics: &lt;a href=&#34;https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset&lt;/a&gt;?, upto 2020-5&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Volcano Eruptions</title>
      <link>/post/2020-05-14-volcano-eruptions/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-14-volcano-eruptions/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuesdata &amp;lt;- tidytuesdayR::tt_load(&amp;#39;2020-05-12&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;volcano &amp;lt;- tuesdata$volcano
eruptions &amp;lt;- tuesdata$eruptions&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Data with Python</title>
      <link>/post/python-tidy-data/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/python-tidy-data/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#column-headers-are-values-not-variable-names&#34;&gt;Column headers are values, not variable names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-variables-stored-in-one-column&#34;&gt;Multiple variables stored in one column&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-are-stored-in-both-rows-and-columns&#34;&gt;Variables are stored in both rows and columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-mortality-data-from-mexico&#34;&gt;Case study: mortality data from Mexico&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This week I’ve been doing some recap on how to do basic data processing and cleaning in Python with the &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;NumPy&lt;/code&gt; library. So this post is mostly a self-reminder on how to deal with messy data in Python, by reproducing data cleaning examples presented in Hadley Wickham’s &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;Tidy Data&lt;/a&gt; paper, &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-JSSv059i10&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The most significant contribution of this well-known work is that it gave clear definition on what “tidy” means for a dataset. There are 3 main requirements, as illustrated on &lt;a href=&#34;https://tidyr.tidyverse.org/&#34;&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt;’s website (evolving from what Hadley originally proposed):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Every column is a variable.&lt;/li&gt;
&lt;li&gt;Every row is an observation.&lt;/li&gt;
&lt;li&gt;Every cell is a single value.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Messy data are, by extension, datasets in volation of these 3 rules. The author then described the five most common problems with messy datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Column headers are values, not variable names.&lt;/li&gt;
&lt;li&gt;Multiple variables are stored in one column.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Variables are stored in both rows and columns.&lt;/li&gt;
&lt;li&gt;Multiple types of observational units are stored in the same table.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;A single observational unit is stored in multiple tables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post I will be focusing on the first 3 symptoms since the other two violations often occur when working with databases. All datasets come from Hadley’s &lt;a href=&#34;https://github.com/hadley/tidy-data&#34;&gt;repo&lt;/a&gt; containing materials for the paper and &lt;a href=&#34;https://chendaniely.github.io/&#34;&gt;Daniel Chen&lt;/a&gt;’s 2019 SciPy tutorial on data processing with pandas.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#&amp;gt; D:\Anaconda\lib\site-packages\statsmodels\tools\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
#&amp;gt;   import pandas.util.testing as tm&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;column-headers-are-values-not-variable-names&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Column headers are values, not variable names&lt;/h1&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pew = pd.read_csv(&amp;quot;D:/RProjects/data/blog/pew.csv&amp;quot;)
pew.head()
#&amp;gt;              religion  &amp;lt;$10k  $10-20k  ...  $100-150k  &amp;gt;150k  Don&amp;#39;t know/refused
#&amp;gt; 0            Agnostic     27       34  ...        109     84                  96
#&amp;gt; 1             Atheist     12       27  ...         59     74                  76
#&amp;gt; 2            Buddhist     27       21  ...         39     53                  54
#&amp;gt; 3            Catholic    418      617  ...        792    633                1489
#&amp;gt; 4  Don’t know/refused     15       14  ...         17     18                 116
#&amp;gt; 
#&amp;gt; [5 rows x 11 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pew&lt;/code&gt; dataset explores the relationship between income and religion in the US, produced by the Pew Research Center. To tidy it, we think of its “right” form if we are to answer data analysis questions. Say, what if we want to a person’s income is influenced by his religion or the other way around. It should be obvious that we need to derive from &lt;code&gt;pew&lt;/code&gt; a column to indicate the level of a person’s income, and another column being count of any combination of income and religion. &lt;code&gt;pew&lt;/code&gt; is messy in the sense that all column names besides &lt;code&gt;religion&lt;/code&gt;, from &lt;code&gt;&amp;lt;$10k&lt;/code&gt; to &lt;code&gt;Don&#39;t know/refused&lt;/code&gt;, should be different levels (values) of a column storing information about income.&lt;/p&gt;
&lt;p&gt;The code to do this is fairly easy in pandas, the &lt;code&gt;.melt&lt;/code&gt; method is very similar to &lt;code&gt;tidyr::pivot_longer&lt;/code&gt; and its namesake in the retired &lt;code&gt;reshape2&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tidy_pew = pew.melt(id_vars = &amp;quot;religion&amp;quot;, var_name = &amp;quot;income&amp;quot;, value_name = &amp;quot;count&amp;quot;)
tidy_pew.head(20)
#&amp;gt;                    religion   income  count
#&amp;gt; 0                  Agnostic    &amp;lt;$10k     27
#&amp;gt; 1                   Atheist    &amp;lt;$10k     12
#&amp;gt; 2                  Buddhist    &amp;lt;$10k     27
#&amp;gt; 3                  Catholic    &amp;lt;$10k    418
#&amp;gt; 4        Don’t know/refused    &amp;lt;$10k     15
#&amp;gt; 5          Evangelical Prot    &amp;lt;$10k    575
#&amp;gt; 6                     Hindu    &amp;lt;$10k      1
#&amp;gt; 7   Historically Black Prot    &amp;lt;$10k    228
#&amp;gt; 8         Jehovah&amp;#39;s Witness    &amp;lt;$10k     20
#&amp;gt; 9                    Jewish    &amp;lt;$10k     19
#&amp;gt; 10            Mainline Prot    &amp;lt;$10k    289
#&amp;gt; 11                   Mormon    &amp;lt;$10k     29
#&amp;gt; 12                   Muslim    &amp;lt;$10k      6
#&amp;gt; 13                 Orthodox    &amp;lt;$10k     13
#&amp;gt; 14          Other Christian    &amp;lt;$10k      9
#&amp;gt; 15             Other Faiths    &amp;lt;$10k     20
#&amp;gt; 16    Other World Religions    &amp;lt;$10k      5
#&amp;gt; 17             Unaffiliated    &amp;lt;$10k    217
#&amp;gt; 18                 Agnostic  $10-20k     34
#&amp;gt; 19                  Atheist  $10-20k     27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s calculate the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic in R and the corresponding p value；&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R 
library(infer)
chisq_stat &amp;lt;- py$tidy_pew %&amp;gt;%
  tidyr::uncount(count) %&amp;gt;% 
  specify(religion ~ income) %&amp;gt;% 
  hypothesize(null = &amp;quot;independence&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;Chisq&amp;quot;)

py$tidy_pew %&amp;gt;%
  tidyr::uncount(count) %&amp;gt;% 
  specify(religion ~ income) %&amp;gt;% 
  hypothesize(null = &amp;quot;independence&amp;quot;) %&amp;gt;%
  visualize(method = &amp;quot;theoretical&amp;quot;) +
  shade_p_value(obs_stat = chisq_stat, direction = &amp;quot;right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-tidy-data-with-python/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows strong relationship between &lt;code&gt;income&lt;/code&gt; and &lt;code&gt;religion&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another common use of this wide data format is to record regularly spaced observations over time, illustrated by the &lt;code&gt;billboard&lt;/code&gt; dataset. Ther rank of a specific track in each week after it enters the Billboard top 100 is recorded in 75 columns, &lt;code&gt;wk1&lt;/code&gt; to &lt;code&gt;wk75&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;billboard = pd.read_csv(&amp;quot;D:/RProjects/data/blog/billboard.csv&amp;quot;)
billboard
#&amp;gt;      year            artist                    track  ... wk74 wk75  wk76
#&amp;gt; 0    2000             2 Pac  Baby Don&amp;#39;t Cry (Keep...  ...  NaN  NaN   NaN
#&amp;gt; 1    2000           2Ge+her  The Hardest Part Of ...  ...  NaN  NaN   NaN
#&amp;gt; 2    2000      3 Doors Down               Kryptonite  ...  NaN  NaN   NaN
#&amp;gt; 3    2000      3 Doors Down                    Loser  ...  NaN  NaN   NaN
#&amp;gt; 4    2000          504 Boyz            Wobble Wobble  ...  NaN  NaN   NaN
#&amp;gt; ..    ...               ...                      ...  ...  ...  ...   ...
#&amp;gt; 312  2000       Yankee Grey     Another Nine Minutes  ...  NaN  NaN   NaN
#&amp;gt; 313  2000  Yearwood, Trisha          Real Live Woman  ...  NaN  NaN   NaN
#&amp;gt; 314  2000   Ying Yang Twins  Whistle While You Tw...  ...  NaN  NaN   NaN
#&amp;gt; 315  2000     Zombie Nation            Kernkraft 400  ...  NaN  NaN   NaN
#&amp;gt; 316  2000   matchbox twenty                     Bent  ...  NaN  NaN   NaN
#&amp;gt; 
#&amp;gt; [317 rows x 81 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we are to answer questions like “what are the average ranking of artisits across all weeks?”, &lt;code&gt;wk&lt;/code&gt; like columns need to be transformed into values:&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tidy_billboard = billboard.melt(id_vars = [&amp;quot;year&amp;quot;, &amp;quot;artist&amp;quot;, &amp;quot;track&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;date.entered&amp;quot;],
                                var_name = &amp;quot;week&amp;quot;,
                                value_name = &amp;quot;rank&amp;quot;)
                                
tidy_billboard
#&amp;gt;        year            artist                    track  ... date.entered  week  rank
#&amp;gt; 0      2000             2 Pac  Baby Don&amp;#39;t Cry (Keep...  ...   2000-02-26   wk1  87.0
#&amp;gt; 1      2000           2Ge+her  The Hardest Part Of ...  ...   2000-09-02   wk1  91.0
#&amp;gt; 2      2000      3 Doors Down               Kryptonite  ...   2000-04-08   wk1  81.0
#&amp;gt; 3      2000      3 Doors Down                    Loser  ...   2000-10-21   wk1  76.0
#&amp;gt; 4      2000          504 Boyz            Wobble Wobble  ...   2000-04-15   wk1  57.0
#&amp;gt; ...     ...               ...                      ...  ...          ...   ...   ...
#&amp;gt; 24087  2000       Yankee Grey     Another Nine Minutes  ...   2000-04-29  wk76   NaN
#&amp;gt; 24088  2000  Yearwood, Trisha          Real Live Woman  ...   2000-04-01  wk76   NaN
#&amp;gt; 24089  2000   Ying Yang Twins  Whistle While You Tw...  ...   2000-03-18  wk76   NaN
#&amp;gt; 24090  2000     Zombie Nation            Kernkraft 400  ...   2000-09-02  wk76   NaN
#&amp;gt; 24091  2000   matchbox twenty                     Bent  ...   2000-04-29  wk76   NaN
#&amp;gt; 
#&amp;gt; [24092 rows x 7 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the average ranking:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(tidy_billboard.
  groupby(&amp;quot;artist&amp;quot;)[[&amp;quot;rank&amp;quot;]].
  mean().
  sort_values(by = &amp;quot;rank&amp;quot;)
)
#&amp;gt;                                    rank
#&amp;gt; artist                                 
#&amp;gt; Santana                       10.500000
#&amp;gt; Elliott, Missy &amp;quot;Misdemeanor&amp;quot;  14.333333
#&amp;gt; matchbox twenty               18.641026
#&amp;gt; N&amp;#39;Sync                        18.648649
#&amp;gt; Janet                         19.416667
#&amp;gt; ...                                 ...
#&amp;gt; Lil&amp;#39; Mo                       98.142857
#&amp;gt; LL Cool J                     98.500000
#&amp;gt; Zombie Nation                 99.000000
#&amp;gt; Fragma                        99.000000
#&amp;gt; Smith, Will                   99.000000
#&amp;gt; 
#&amp;gt; [228 rows x 1 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-variables-stored-in-one-column&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple variables stored in one column&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;code&gt;tb&lt;/code&gt; daaset comes from the World Health Organisation, and records the counts of confirmed tuberculosis cases by country, year, and demographic group. The demographic groups are broken down by sex (m, f) and age (0–14, 15–25, 25–34, 35–44.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tb = pd.read_csv(&amp;quot;D:/RProjects/data/blog/tb.csv&amp;quot;)
tb
#&amp;gt;     country  year   m014   m1524   m2534  ...   f3544  f4554  f5564   f65  fu
#&amp;gt; 0        AD  2000    0.0     0.0     1.0  ...     NaN    NaN    NaN   NaN NaN
#&amp;gt; 1        AE  2000    2.0     4.0     4.0  ...     3.0    0.0    0.0   4.0 NaN
#&amp;gt; 2        AF  2000   52.0   228.0   183.0  ...   339.0  205.0   99.0  36.0 NaN
#&amp;gt; 3        AG  2000    0.0     0.0     0.0  ...     0.0    0.0    0.0   0.0 NaN
#&amp;gt; 4        AL  2000    2.0    19.0    21.0  ...     8.0    8.0    5.0  11.0 NaN
#&amp;gt; ..      ...   ...    ...     ...     ...  ...     ...    ...    ...   ...  ..
#&amp;gt; 196      YE  2000  110.0   789.0   689.0  ...   517.0  345.0  247.0  92.0 NaN
#&amp;gt; 197      YU  2000    NaN     NaN     NaN  ...     NaN    NaN    NaN   NaN NaN
#&amp;gt; 198      ZA  2000  116.0   723.0  1999.0  ...   933.0  423.0  167.0  80.0 NaN
#&amp;gt; 199      ZM  2000  349.0  2175.0  2610.0  ...  1305.0  186.0  112.0  75.0 NaN
#&amp;gt; 200      ZW  2000    NaN     NaN     NaN  ...     NaN    NaN    NaN   NaN NaN
#&amp;gt; 
#&amp;gt; [201 rows x 18 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To clean this data, we first melt all columns except for &lt;code&gt;country&lt;/code&gt; and &lt;code&gt;year&lt;/code&gt; in return for a longer version of &lt;code&gt;tb&lt;/code&gt;, and then seperate the &lt;code&gt;variable&lt;/code&gt; column into two pieces of information, &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tb_long = tb.melt(id_vars = [&amp;quot;country&amp;quot;, &amp;quot;year&amp;quot;])
sex = tb_long[&amp;quot;variable&amp;quot;].str.split(pat = &amp;quot;(m|f)(.+)&amp;quot;).str.get(1)
age = tb_long[&amp;quot;variable&amp;quot;].str.split(pat = &amp;quot;(m|f)(.+)&amp;quot;).str.get(2)

print(sex)
#&amp;gt; 0       m
#&amp;gt; 1       m
#&amp;gt; 2       m
#&amp;gt; 3       m
#&amp;gt; 4       m
#&amp;gt;        ..
#&amp;gt; 3211    f
#&amp;gt; 3212    f
#&amp;gt; 3213    f
#&amp;gt; 3214    f
#&amp;gt; 3215    f
#&amp;gt; Name: variable, Length: 3216, dtype: object
print(age)
#&amp;gt; 0       014
#&amp;gt; 1       014
#&amp;gt; 2       014
#&amp;gt; 3       014
#&amp;gt; 4       014
#&amp;gt;        ... 
#&amp;gt; 3211      u
#&amp;gt; 3212      u
#&amp;gt; 3213      u
#&amp;gt; 3214      u
#&amp;gt; 3215      u
#&amp;gt; Name: variable, Length: 3216, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add these two columns and drop the redundant &lt;code&gt;variable&lt;/code&gt; column&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tidy_tb = tb_long.assign(sex = sex, age = age).drop(&amp;quot;variable&amp;quot;, axis = &amp;quot;columns&amp;quot;)
tidy_tb
#&amp;gt;      country  year  value sex  age
#&amp;gt; 0         AD  2000    0.0   m  014
#&amp;gt; 1         AE  2000    2.0   m  014
#&amp;gt; 2         AF  2000   52.0   m  014
#&amp;gt; 3         AG  2000    0.0   m  014
#&amp;gt; 4         AL  2000    2.0   m  014
#&amp;gt; ...      ...   ...    ...  ..  ...
#&amp;gt; 3211      YE  2000    NaN   f    u
#&amp;gt; 3212      YU  2000    NaN   f    u
#&amp;gt; 3213      ZA  2000    NaN   f    u
#&amp;gt; 3214      ZM  2000    NaN   f    u
#&amp;gt; 3215      ZW  2000    NaN   f    u
#&amp;gt; 
#&amp;gt; [3216 rows x 5 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variables-are-stored-in-both-rows-and-columns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variables are stored in both rows and columns&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;code&gt;weather&lt;/code&gt; data shows daily weather data from the Global Historical Climatology Network for one weather station (MX17004) in Mexico for five months in 2010.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;weather = pd.read_csv(&amp;quot;D:/RProjects/data/blog/weather.csv&amp;quot;)
weather
#&amp;gt;          id  year  month element    d1  ...   d27   d28   d29   d30   d31
#&amp;gt; 0   MX17004  2010      1    tmax   NaN  ...   NaN   NaN   NaN  27.8   NaN
#&amp;gt; 1   MX17004  2010      1    tmin   NaN  ...   NaN   NaN   NaN  14.5   NaN
#&amp;gt; 2   MX17004  2010      2    tmax   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 3   MX17004  2010      2    tmin   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 4   MX17004  2010      3    tmax   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 5   MX17004  2010      3    tmin   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 6   MX17004  2010      4    tmax   NaN  ...  36.3   NaN   NaN   NaN   NaN
#&amp;gt; 7   MX17004  2010      4    tmin   NaN  ...  16.7   NaN   NaN   NaN   NaN
#&amp;gt; 8   MX17004  2010      5    tmax   NaN  ...  33.2   NaN   NaN   NaN   NaN
#&amp;gt; 9   MX17004  2010      5    tmin   NaN  ...  18.2   NaN   NaN   NaN   NaN
#&amp;gt; 10  MX17004  2010      6    tmax   NaN  ...   NaN   NaN  30.1   NaN   NaN
#&amp;gt; 11  MX17004  2010      6    tmin   NaN  ...   NaN   NaN  18.0   NaN   NaN
#&amp;gt; 12  MX17004  2010      7    tmax   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 13  MX17004  2010      7    tmin   NaN  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 14  MX17004  2010      8    tmax   NaN  ...   NaN   NaN  28.0   NaN  25.4
#&amp;gt; 15  MX17004  2010      8    tmin   NaN  ...   NaN   NaN  15.3   NaN  15.4
#&amp;gt; 16  MX17004  2010     10    tmax   NaN  ...   NaN  31.2   NaN   NaN   NaN
#&amp;gt; 17  MX17004  2010     10    tmin   NaN  ...   NaN  15.0   NaN   NaN   NaN
#&amp;gt; 18  MX17004  2010     11    tmax   NaN  ...  27.7   NaN   NaN   NaN   NaN
#&amp;gt; 19  MX17004  2010     11    tmin   NaN  ...  14.2   NaN   NaN   NaN   NaN
#&amp;gt; 20  MX17004  2010     12    tmax  29.9  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 21  MX17004  2010     12    tmin  13.8  ...   NaN   NaN   NaN   NaN   NaN
#&amp;gt; 
#&amp;gt; [22 rows x 35 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are two major problems with &lt;code&gt;weather&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;d1&lt;/code&gt;, &lt;code&gt;d2&lt;/code&gt;, …, &lt;code&gt;d31&lt;/code&gt; should be values instead of column names (solved by &lt;code&gt;.melt&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;On the other hand, values in the &lt;code&gt;element&lt;/code&gt; column should be names, it should be spread into two columns named &lt;code&gt;tmax&lt;/code&gt;, &lt;code&gt;tmin&lt;/code&gt; (solved by &lt;code&gt;.pivot_table&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(weather.
  melt(id_vars = [&amp;quot;id&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;element&amp;quot;], var_name = &amp;quot;day&amp;quot;, value_name = &amp;quot;temp&amp;quot;).
  pivot_table(index = [&amp;quot;id&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;day&amp;quot;],
              columns = &amp;quot;element&amp;quot;,
              values = &amp;quot;temp&amp;quot;).
  reset_index().
  head()
)
#&amp;gt; element       id  year  month  day  tmax  tmin
#&amp;gt; 0        MX17004  2010      1  d30  27.8  14.5
#&amp;gt; 1        MX17004  2010      2  d11  29.7  13.4
#&amp;gt; 2        MX17004  2010      2   d2  27.3  14.4
#&amp;gt; 3        MX17004  2010      2  d23  29.9  10.7
#&amp;gt; 4        MX17004  2010      2   d3  24.1  14.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-mortality-data-from-mexico&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Case study: mortality data from Mexico&lt;/h1&gt;
&lt;p&gt;After stating these common problems and their remidies, Hadley presented a case study section on how tidy dataset can facilitate data analysis. The case study uses individual-level mortality data from Mexico. The goal is to find causes of death with unusual temporal patterns, at hour level. It’s time to move back from Python to R!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
deaths &amp;lt;- read_csv(&amp;quot;D:/RProjects/data/blog/mexico-deaths.csv&amp;quot;) %&amp;gt;% na.omit()
deaths
#&amp;gt; # A tibble: 513,273 x 5
#&amp;gt;      yod   mod   dod   hod cod  
#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
#&amp;gt;  1  1920    11    17     3 W78  
#&amp;gt;  2  1923     2     4    16 J44  
#&amp;gt;  3  1923     6    23    19 E12  
#&amp;gt;  4  1926     2     5    16 C67  
#&amp;gt;  5  1926     4     1    16 J44  
#&amp;gt;  6  1928    10    30    19 I27  
#&amp;gt;  7  1929     4    23    15 I25  
#&amp;gt;  8  1930     9    11    19 E14  
#&amp;gt;  9  1930    12    22    19 E11  
#&amp;gt; 10  1931     5    26    11 K65  
#&amp;gt; # ... with 513,263 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns are year, month, day, hour and cause of specific death respectively. Another table &lt;code&gt;codes&lt;/code&gt; explains what acronyms in &lt;code&gt;cod&lt;/code&gt; mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- read_csv(&amp;quot;D:/RProjects/data/blog/codes.csv&amp;quot;)
codes
#&amp;gt; # A tibble: 1,851 x 2
#&amp;gt;    cod   disease                                                              
#&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                                                                
#&amp;gt;  1 A00   &amp;quot;Cholera&amp;quot;                                                            
#&amp;gt;  2 A01   &amp;quot;Typhoid and paratyphoid\nfevers&amp;quot;                                    
#&amp;gt;  3 A02   &amp;quot;Other salmonella infections&amp;quot;                                        
#&amp;gt;  4 A03   &amp;quot;Shigellosis&amp;quot;                                                        
#&amp;gt;  5 A04   &amp;quot;Other bacterial intestinal\ninfections&amp;quot;                             
#&amp;gt;  6 A05   &amp;quot;Other bacterial foodborne\nintoxications, not elsewhere\nclassified&amp;quot;
#&amp;gt;  7 A06   &amp;quot;Amebiasis&amp;quot;                                                          
#&amp;gt;  8 A07   &amp;quot;Other protozoal intestinal\ndiseases&amp;quot;                               
#&amp;gt;  9 A08   &amp;quot;Viral and other specified\nintestinal infections&amp;quot;                   
#&amp;gt; 10 A09   &amp;quot;Diarrhea and gastroenteritis\nof infectious origin&amp;quot;                 
#&amp;gt; # ... with 1,841 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to the &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; package, we can mix R and Python code seamlessly. Here is a line plot made with &lt;code&gt;seaborn&lt;/code&gt; demonstrating total deaths per hour:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
deaths = r.deaths
deaths_per_hour = deaths[&amp;quot;hod&amp;quot;].value_counts()
deaths_per_hour
#&amp;gt; 18.0    24380
#&amp;gt; 10.0    24321
#&amp;gt; 16.0    23890
#&amp;gt; 11.0    23843
#&amp;gt; 6.0     23787
#&amp;gt; 17.0    23625
#&amp;gt; 12.0    23392
#&amp;gt; 13.0    23284
#&amp;gt; 15.0    23278
#&amp;gt; 14.0    23053
#&amp;gt; 20.0    22926
#&amp;gt; 19.0    22919
#&amp;gt; 9.0     22401
#&amp;gt; 5.0     22126
#&amp;gt; 8.0     21915
#&amp;gt; 7.0     21822
#&amp;gt; 23.0    21446
#&amp;gt; 21.0    20995
#&amp;gt; 22.0    20510
#&amp;gt; 1.0     20430
#&amp;gt; 4.0     20239
#&amp;gt; 3.0     19729
#&amp;gt; 2.0     18962
#&amp;gt; Name: hod, dtype: int64
sns.lineplot(x = deaths_per_hour.index, y = deaths_per_hour.values)
plt.title(&amp;quot;Temporal pattern of all causes of death&amp;quot;)
plt.xlabel(&amp;quot;Hour of the day&amp;quot;)
plt.ylabel(&amp;quot;Number of deaths&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-tidy-data-with-python/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To provide informative labels for causes, we next join the dataset to the &lt;code&gt;codes&lt;/code&gt; dataset, on the &lt;code&gt;cod&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deaths &amp;lt;- left_join(deaths, codes) %&amp;gt;%
  rename(cause = disease)
head(deaths)
#&amp;gt; # A tibble: 6 x 6
#&amp;gt;     yod   mod   dod   hod cod   cause                                         
#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                                         
#&amp;gt; 1  1920    11    17     3 W78   &amp;quot;Inhalation of gastric\ncontents&amp;quot;             
#&amp;gt; 2  1923     2     4    16 J44   &amp;quot;Other chronic obstructive\npulmonary disease&amp;quot;
#&amp;gt; 3  1923     6    23    19 E12   &amp;quot;Malnutrition-related diabetes\nmellitus&amp;quot;     
#&amp;gt; 4  1926     2     5    16 C67   &amp;quot;Malignant neoplasm of bladder&amp;quot;               
#&amp;gt; 5  1926     4     1    16 J44   &amp;quot;Other chronic obstructive\npulmonary disease&amp;quot;
#&amp;gt; 6  1928    10    30    19 I27   &amp;quot;Other pulmonary heart\ndiseases&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total deaths for each cause varies over several orders of magnitude: there are 46,794 deaths from heart attack but only 1 from Tularemia.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
(deaths.groupby([&amp;quot;cod&amp;quot;]).
  size().
  reset_index(name = &amp;quot;per_cause&amp;quot;).
  sort_values(by = &amp;quot;per_cause&amp;quot;, ascending = False)
)
#&amp;gt;       cod  per_cause
#&amp;gt; 417   I21      46794
#&amp;gt; 260   E11      42421
#&amp;gt; 262   E14      27330
#&amp;gt; 495   J44      16043
#&amp;gt; 566   K70      12860
#&amp;gt; ...   ...        ...
#&amp;gt; 1079  X24          1
#&amp;gt; 521   K02          1
#&amp;gt; 939   V30          1
#&amp;gt; 940   V33          1
#&amp;gt; 182   D04          1
#&amp;gt; 
#&amp;gt; [1194 rows x 2 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that rather than the total number, it makes more sense to think in proportions. If a cause of death departs from the overall temporal pattern, then its proportion of deaths in a given hour compared to the total deaths of that cause should differ significantly from that of the hourly deaths at the same time compared to total deaths. I denote these two proportions as &lt;code&gt;prop1&lt;/code&gt; and &lt;code&gt;prop2&lt;/code&gt; respectively. To ensure that the causes we consider are sufficiently representative we’ll only work with causes with more than 50 total deaths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop1 &amp;lt;- deaths %&amp;gt;% 
  count(hod, cause, name = &amp;quot;per_hour_per_cause&amp;quot;) %&amp;gt;% 
  add_count(cause, wt = per_hour_per_cause, name = &amp;quot;per_cause&amp;quot;) %&amp;gt;% 
  mutate(prop1 = per_hour_per_cause / per_cause)

prop2 &amp;lt;- deaths %&amp;gt;% 
  count(hod, name = &amp;quot;per_hour&amp;quot;) %&amp;gt;% 
  add_count(wt = per_hour, name = &amp;quot;total&amp;quot;) %&amp;gt;% 
  mutate(prop2 = per_hour / total)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hadley used mean square error between the two proportions as a kind of distance, to indicate the average degree of anomaly of a cause, and I follow:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist &amp;lt;- prop1 %&amp;gt;% 
  filter(per_cause &amp;gt; 50) %&amp;gt;% 
  left_join(prop2, on = &amp;quot;hod&amp;quot;) %&amp;gt;% 
  select(hour = hod,
         cause,
         n = per_cause,
         prop1,
         prop2) %&amp;gt;% 
  group_by(cause, n) %&amp;gt;% 
  summarize(dist = mean((prop1 - prop2) ^ 2)) %&amp;gt;% 
  ungroup()

dist %&amp;gt;% 
  arrange(desc(dist))
#&amp;gt; # A tibble: 447 x 3
#&amp;gt;    cause                                                               n    dist
#&amp;gt;    &amp;lt;chr&amp;gt;                                                           &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
#&amp;gt;  1 &amp;quot;Accident to powered aircraft\ncausing injury to occupant&amp;quot;         57 0.00573
#&amp;gt;  2 &amp;quot;Victim of lightning&amp;quot;                                              97 0.00513
#&amp;gt;  3 &amp;quot;Bus occupant injured in other\nand unspecified transport\nacc~    52 0.00419
#&amp;gt;  4 &amp;quot;Assault (homicide) by smoke,\nfire, and flames&amp;quot;                   51 0.00229
#&amp;gt;  5 &amp;quot;Exposure to electric\ntransmission lines&amp;quot;                         77 0.00161
#&amp;gt;  6 &amp;quot;Sudden infant death syndrome&amp;quot;                                    323 0.00156
#&amp;gt;  7 &amp;quot;Drowning and submersion while\nin natural water&amp;quot;                 469 0.00133
#&amp;gt;  8 &amp;quot;Motorcycle rider injured in\ncollision with car, pickup\ntruc~    66 0.00126
#&amp;gt;  9 &amp;quot;Contact with hornets, wasps,\nand bees&amp;quot;                           86 0.00118
#&amp;gt; 10 &amp;quot;Exposure to smoke, fire, and\nflames, undetermined intent&amp;quot;        51 0.00110
#&amp;gt; # ... with 437 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see causes of death with highest &lt;code&gt;dist&lt;/code&gt; are mainly accidents and rare diseases. However, there is a negative correlation between the frequency of a cause and its deviation, as shown in the following plot, so that the result based solely on the &lt;code&gt;dist&lt;/code&gt; column would be biased in favour of rare causes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist %&amp;gt;% 
  ggplot(aes(n, dist)) + 
  geom_jitter() + 
  ggrepel::geom_text_repel(aes(label = cause),
                           top_n(dist, 10)) + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Temporal deviation of causes of deaths in Mexico&amp;quot;,
       y = NULL,
       x = &amp;quot;total death&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-tidy-data-with-python/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thus, our final solution is to build a model with &lt;code&gt;n&lt;/code&gt; as predictor, and &lt;code&gt;dist&lt;/code&gt; as response. The cause with highest residual are assumed to have the most deviation. Since the linear trend fits the data quite well, I opt for linear regression (Hadley used robust linear model in the paper).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
lm_fit &amp;lt;- lm(log(dist) ~ log(n), data = dist)
tidy(lm_fit)
#&amp;gt; # A tibble: 2 x 5
#&amp;gt;   term        estimate std.error statistic   p.value
#&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
#&amp;gt; 1 (Intercept)   -3.74     0.110      -34.0 8.34e-126
#&amp;gt; 2 log(n)        -0.869    0.0186     -46.8 7.55e-174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot these residuals against the predictor &lt;code&gt;log(n)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(lm_fit) %&amp;gt;% 
  ggplot(aes(log.n., .resid)) + 
  geom_hline(yintercept = 0, color = &amp;quot;red&amp;quot;) + 
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-tidy-data-with-python/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The plot shows an empty region around a residual of 1.5. So somewhat arbitrarily, we’ll select those diseases with a residual greater than 1.5&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rows &amp;lt;- augment(lm_fit) %&amp;gt;% 
  mutate(row = row_number()) %&amp;gt;% 
  filter(.resid &amp;gt; 1.5) %&amp;gt;% 
  select(row) %&amp;gt;% 
  pull(row)


unusual &amp;lt;- dist %&amp;gt;%
  mutate(row = row_number()) %&amp;gt;% 
  filter(row %in% rows) %&amp;gt;% 
  select(cause, n)

unusual
#&amp;gt; # A tibble: 13 x 2
#&amp;gt;    cause                                                                       n
#&amp;gt;    &amp;lt;chr&amp;gt;                                                                   &amp;lt;int&amp;gt;
#&amp;gt;  1 &amp;quot;Accident to powered aircraft\ncausing injury to occupant&amp;quot;                 57
#&amp;gt;  2 &amp;quot;Assault (homicide) by other\nand unspecified firearm\ndischarge&amp;quot;        7228
#&amp;gt;  3 &amp;quot;Assault (homicide) by sharp\nobject&amp;quot;                                    1575
#&amp;gt;  4 &amp;quot;Bus occupant injured in other\nand unspecified transport\naccidents&amp;quot;      52
#&amp;gt;  5 &amp;quot;Drowning and submersion while\nin natural water&amp;quot;                         469
#&amp;gt;  6 &amp;quot;Exposure to unspecified\nelectric current&amp;quot;                               456
#&amp;gt;  7 &amp;quot;Motor- or nonmotor-vehicle\naccident, type of vehicle\nunspecified&amp;quot;     4061
#&amp;gt;  8 &amp;quot;Other specified drowning and\nsubmersion&amp;quot;                                303
#&amp;gt;  9 &amp;quot;Pedestrian injured in other\nand unspecified transport\naccidents&amp;quot;      3956
#&amp;gt; 10 &amp;quot;Sudden infant death syndrome&amp;quot;                                            323
#&amp;gt; 11 &amp;quot;Traffic accident of specified\ntype but victim&amp;#39;s mode of\ntransport u~  2545
#&amp;gt; 12 &amp;quot;Unspecified drowning and\nsubmersion&amp;quot;                                   1283
#&amp;gt; 13 &amp;quot;Victim of lightning&amp;quot;                                                      97&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we plot the temporal course for each unusual cause of death.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop1 %&amp;gt;% 
  filter(cause %in% unusual$cause) %&amp;gt;% 
  left_join(prop2, on = &amp;quot;hod&amp;quot;) %&amp;gt;% 
  pivot_longer(c(prop1, prop2)) %&amp;gt;% 
  ggplot(aes(hod, value, color = name)) + 
  geom_line() + 
  scale_color_manual(name = NULL,
                     labels = c(&amp;quot;cause-specific&amp;quot;, &amp;quot;overall&amp;quot;), 
                     values = c(&amp;quot;#FFBF0F&amp;quot;, &amp;quot;#0382E5&amp;quot;)) + 
  facet_wrap(~ cause, scales = &amp;quot;free_y&amp;quot;) + 
  labs(x = &amp;quot;hour&amp;quot;, y = NULL, title = &amp;quot;Most deviated causes of death&amp;quot;, 
       subtitle = &amp;quot;comparing cause-specific temporal pattern to overall trend&amp;quot;) + 
  theme_minimal(base_size = 22) +
  theme(legend.position = &amp;quot;top&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-tidy-data-with-python/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1536&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-JSSv059i10&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2014. “Tidy Data.” &lt;em&gt;Journal of Statistical Software, Articles&lt;/em&gt; 59 (10): 1–23. &lt;a href=&#34;https://doi.org/10.18637/jss.v059.i10&#34;&gt;https://doi.org/10.18637/jss.v059.i10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Although pandas and dplyr 1.0 can perform rowwise operatios in a breeze, it’s not considered best practice in such cases.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning with Titanic Data</title>
      <link>/post/2020-05-08-machine-learning-with-titanic-data/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-08-machine-learning-with-titanic-data/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://github.com/ritvikmath/Machine-Learning-With-Titanic-Data/blob/master/UMSA%20May%202018%20-%20Machine%20Learning%20in%20Python.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/ritvikmath/Machine-Learning-With-Titanic-Data/blob/master/UMSA%20May%202018%20-%20Machine%20Learning%20in%20Python.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/&#34; class=&#34;uri&#34;&gt;https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import pearsonr
import math
import random 

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

from datetime import datetime
import itertools&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Random sample of all titanic passengers.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;titanic = pd.read_excel(&amp;#39;D:/RProjects/data/blog/titanic.xls&amp;#39;)
titanic.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    pclass  survived  ...   body                        home.dest
## 0       1         1  ...    NaN                     St Louis, MO
## 1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON
## 2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON
## 3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON
## 4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON
## 
## [5 rows x 14 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;titanic.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 1309 entries, 0 to 1308
## Data columns (total 14 columns):
## pclass       1309 non-null int64
## survived     1309 non-null int64
## name         1309 non-null object
## sex          1309 non-null object
## age          1046 non-null float64
## sibsp        1309 non-null int64
## parch        1309 non-null int64
## ticket       1309 non-null object
## fare         1308 non-null float64
## cabin        295 non-null object
## embarked     1307 non-null object
## boat         486 non-null object
## body         121 non-null float64
## home.dest    745 non-null object
## dtypes: float64(3), int64(4), object(7)
## memory usage: 143.3+ KB&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Programmers and Their Choice of Words</title>
      <link>/post/programmer-word-choice/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/programmer-word-choice/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://medium.com/swlh/what-programming-language-has-the-happiest-developers-f0636b08e898&#34; class=&#34;uri&#34;&gt;https://medium.com/swlh/what-programming-language-has-the-happiest-developers-f0636b08e898&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Dobiasd/programming-language-subreddits-and-their-choice-of-words&#34; class=&#34;uri&#34;&gt;https://github.com/Dobiasd/programming-language-subreddits-and-their-choice-of-words&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Animal Crossing Reviews</title>
      <link>/post/2020-05-07-analyzing-animal-crossing-reviews/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-07-analyzing-animal-crossing-reviews/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eda-and-data-cleaning&#34;&gt;EDA and data cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-analysis-of-user-reviews&#34;&gt;Text analysis of user reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictive-modeling-for-rating&#34;&gt;Predictive modeling for rating&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post I analyzed reviews for the life simulation video game, Animal Crossing. The data came from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-05/readme.md&#34;&gt;this weeks’s &lt;code&gt;#TidyTuesday&lt;/code&gt;&lt;/a&gt;, scraped from &lt;a href=&#34;https://github.com/jefflomacy/villagerdb&#34;&gt;VillagerDB&lt;/a&gt; and &lt;a href=&#34;https://www.metacritic.com/game/switch/animal-crossing-new-horizons/critic-reviews&#34;&gt;Metacritic&lt;/a&gt;. I used only the &lt;code&gt;reviews&lt;/code&gt; table, but there are a lot more to analyze such as characters and items in the game.&lt;/p&gt;
&lt;p&gt;The data appeared a bit messy after some EDA, and regular expressions played a major role in data cleaning. After that I made some plots concerning the review text, such as common words or high score words from an algorithm. Highly correlated words in user reviews were shown with nodes and edges. Then I built a multicategory logit model to predict ratings (low, medium or high) with predictors including the reviewing date and usage of specific words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;eda-and-data-cleaning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;EDA and data cleaning&lt;/h1&gt;
&lt;p&gt;The reviews data contains four columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;grade&lt;/code&gt;: 0-100 score given by the critic (missing for some) where higher score = better.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_name&lt;/code&gt;: user name of the reviewer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;text&lt;/code&gt;: review of the reviewer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;date&lt;/code&gt;: when the review is published&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews &amp;lt;- readr::read_tsv(&amp;quot;D:/RProjects/data/blog/animal-crossing-reviews.tsv&amp;quot;)
glimpse(reviews)
#&amp;gt; Rows: 1,473
#&amp;gt; Columns: 4
#&amp;gt; $ grade     &amp;lt;dbl&amp;gt; 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...
#&amp;gt; $ user_name &amp;lt;chr&amp;gt; &amp;quot;mds27272&amp;quot;, &amp;quot;lolo2178&amp;quot;, &amp;quot;Roachant&amp;quot;, &amp;quot;Houndf&amp;quot;, &amp;quot;ProfessorF...
#&amp;gt; $ text      &amp;lt;chr&amp;gt; &amp;quot;My gf started playing before me. No option to create my ...
#&amp;gt; $ date      &amp;lt;date&amp;gt; 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A bar plot of all grades show a bimodal distribution. This is perhaps not that astonishing when it comes to reviewing, since people tend to go to extremes and give polarized opinions. This may suggest that we cut &lt;code&gt;grades&lt;/code&gt; into discrete levels and build a classification model afterwards, rather than modeling bare grades itself with regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews %&amp;gt;% 
  count(grade) %&amp;gt;% 
  ggplot() + 
  geom_col(aes(x = factor(grade), 
               y = n),
           fill = &amp;quot;midnightblue&amp;quot;, alpha = 0.6) + 
  labs(x = &amp;quot;grade&amp;quot;, y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data is messy in several ways (I have chosen 3 observations from the &lt;code&gt;text&lt;/code&gt; column for example):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Review contains repetition. the following review where the first 4.5 lines are repeated in the following lines&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“While the game itself is great, really relaxing and gorgeous, i can’t ignore one thing that ruins the whole experience for me and a lot of other people as seen by the different user reviews.That thing is that you only have 1 island per console. This decision limits to one person being able to enjoy the full experience. It also nukes any creative control of the island, since you haveWhile the game itself is great, really relaxing and gorgeous, i can’t ignore one thing that ruins the whole experience for me and a lot of other people as seen by the different user reviews.That thing is that you only have 1 island per console. This decision limits to one person being able to enjoy the full experience. It also nukes any creative control of the island, since you have the other usershouse and furniture. I hope nintendo can soon fix this big issue, because for now, this killed any intentions i had to play the game.… Expand”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Reviews that exceed certian length are incomplete and end with “Expand”. The following review also contains repeated lines.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“One island per console is a design decision that is, at best, in poor taste, and at worst, straight-up predatory behavior.Per console, only one player gets to experience the game at its fullest. The other players see less dialogue, experience less events, and are locked out entirely from certain parts of the game.No matter how good a game is, I cannot stand behind a company thatOne island per console is a design decision that is, at best, in poor taste, and at worst, straight-up predatory behavior.Per console, only one player gets to experience the game at its fullest. The other players see less dialogue, experience less events, and are locked out entirely from certain parts of the game.No matter how good a game is, I cannot stand behind a company that sees fit to make such decisions.… Expand”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;non-English reviews&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“Una sola isla , es un asco . No puedes seguir avanzando, solo te queda recoger madera”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I use regular expressions to remove repeated lines as well as “Expand” at the end. Repetitions happen when the review is long, and the repetition part often takes up 4 to 5 lines (here I use 350 or more characters to indicate the repetition part).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;clr::detect_language&lt;/code&gt; is used to exclude non-English text. This a R wrapper around Google’s Compact Language Detector 3, a neural network model for language identification. There will be misclassifications, though. As the proportion of exclusion is fairly low, we’re OK. Lastyly, et’s split &lt;code&gt;grade&lt;/code&gt; 3 ordered categories, low, medium and high.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cld3)

# most text are detected as English
reviews %&amp;gt;% 
  mutate(language = detect_language(text)) %&amp;gt;% 
  count(language, sort = TRUE)
#&amp;gt; # A tibble: 11 x 2
#&amp;gt;    language     n
#&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
#&amp;gt;  1 en        1394
#&amp;gt;  2 es          48
#&amp;gt;  3 ru           7
#&amp;gt;  4 it           6
#&amp;gt;  5 fr           5
#&amp;gt;  6 pt           5
#&amp;gt;  7 de           3
#&amp;gt;  8 &amp;lt;NA&amp;gt;         2
#&amp;gt;  9 ja           1
#&amp;gt; 10 pl           1
#&amp;gt; 11 th           1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_en &amp;lt;- reviews %&amp;gt;% 
  filter(detect_language(text) == &amp;quot;en&amp;quot;)


repetition_clean &amp;lt;- reviews_en %&amp;gt;% 
  filter(str_detect(text, &amp;quot;(.{350,})\\1.+&amp;quot;)) %&amp;gt;% 
  mutate(text = str_replace(text, &amp;quot;(.{350,})\\1(.+)Expand$&amp;quot;, &amp;quot;\\1\\2&amp;quot;))

reviews_clean &amp;lt;- anti_join(reviews_en, repetition_clean, 
                           by = c(&amp;quot;user_name&amp;quot; = &amp;quot;user_name&amp;quot;)) %&amp;gt;% 
  bind_rows(repetition_clean) %&amp;gt;% 
  mutate(rating = case_when(
    grade &amp;lt;= 2 ~ &amp;quot;low&amp;quot;,
    grade &amp;gt; 2 &amp;amp; grade &amp;lt; 8 ~ &amp;quot;medium&amp;quot;,
    grade &amp;gt;= 8 ~ &amp;quot;high&amp;quot;,
  ) %&amp;gt;% factor(levels = c(&amp;quot;low&amp;quot;, &amp;quot;medium&amp;quot;, &amp;quot;high&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;rating&lt;/code&gt; is a factor with 3 levels. Low and high ratings are rougly the same size, and medium ratings are relatively rare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_clean %&amp;gt;% 
  count(rating)
#&amp;gt; # A tibble: 3 x 2
#&amp;gt;   rating     n
#&amp;gt;   &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;
#&amp;gt; 1 low      626
#&amp;gt; 2 medium   136
#&amp;gt; 3 high     632&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can examine how this cleaning process works by comparing the distribution of review length, before and after. The cleaning should reduce the amount of medium long and long reviews, in exchange for shorter ones.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews %&amp;gt;%
  transmute(length = str_length(text),
            type = &amp;quot;before&amp;quot;) %&amp;gt;% 
  bind_rows(reviews_clean %&amp;gt;%
              transmute(length = str_length(text), type = &amp;quot;after&amp;quot;)) %&amp;gt;%
  ggplot() + 
  geom_density(aes(length, fill = type), alpha = 0.2) + 
  scale_fill_discrete(name = NULL) + 
  labs(title = &amp;quot;Distribution of review length (characters) before and after cleaning&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-analysis-of-user-reviews&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Text analysis of user reviews&lt;/h1&gt;
&lt;p&gt;Our text analysis begin by tokenizing review text to find out what are the most common words (by term frequency) for each category of reviews.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words &amp;lt;- reviews_clean %&amp;gt;% 
  select(rating, text) %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  anti_join(stop_words) %&amp;gt;% 
  filter(!str_detect(word, &amp;quot;^\\d+$&amp;quot;))
 

common_words &amp;lt;- words %&amp;gt;% 
  count(rating, word, sort = TRUE, name = &amp;quot;term_count&amp;quot;) %&amp;gt;% 
  add_count(rating, wt = term_count, name = &amp;quot;total_count&amp;quot;) %&amp;gt;% 
  mutate(term_freq = term_count / total_count)  %&amp;gt;% 
  group_by(rating) %&amp;gt;% 
  top_n(30, term_freq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(common_words, 
       aes(y = reorder_within(word, term_freq, rating),
           x = term_freq,
           fill = rating)) + 
  geom_col(show.legend = FALSE, alpha = 0.6) + 
  scale_y_reordered() + 
  scale_x_continuous(label = scales::label_percent()) + 
  nord::scale_fill_nord(palette = &amp;quot;afternoon_prarie&amp;quot;) +
  facet_wrap(~ rating, scales = &amp;quot;free_y&amp;quot;, strip.position = &amp;quot;bottom&amp;quot;) + 
  labs(title = &amp;quot;Most common words in different levels of reviews&amp;quot;,
       x = &amp;quot;term frequency&amp;quot;,
       y = NULL) + 
  hrbrthemes::theme_modern_rc() + 
  theme(panel.grid.major.y = element_blank(),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = 28),
        plot.title.position = &amp;quot;plot&amp;quot;,
        axis.text.y = element_text(size = 16),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16), 
        strip.text = element_text(size = 20, color = &amp;quot;white&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it is, this plot aren’t that helpful for all categories share a very similar set of words. For this reason I turn to two other algorithms developed for information retrieval: tf-idf and weighted log odds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidylo)

key_words &amp;lt;- words %&amp;gt;% 
  count(rating, word, sort = TRUE) %&amp;gt;% 
  bind_tf_idf(term = word, document = rating, n = n) %&amp;gt;%
  left_join(words %&amp;gt;% 
              count(rating, word, sort = TRUE) %&amp;gt;% 
              bind_log_odds(set = rating, feature = word, n = n),
            by = c(&amp;quot;rating&amp;quot; = &amp;quot;rating&amp;quot;, &amp;quot;word&amp;quot;, &amp;quot;word&amp;quot;, &amp;quot;n&amp;quot; = &amp;quot;n&amp;quot;)) %&amp;gt;% 
  select(rating, word, tf_idf, log_odds)

key_words
#&amp;gt; # A tibble: 9,162 x 4
#&amp;gt;    rating word     tf_idf log_odds
#&amp;gt;    &amp;lt;fct&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
#&amp;gt;  1 low    game          0   -0.392
#&amp;gt;  2 high   game          0    1.25 
#&amp;gt;  3 low    island        0    3.60 
#&amp;gt;  4 low    switch        0    5.25 
#&amp;gt;  5 low    play          0    5.24 
#&amp;gt;  6 low    player        0    7.31 
#&amp;gt;  7 high   island        0   -3.44 
#&amp;gt;  8 low    nintendo      0    6.54 
#&amp;gt;  9 medium game          0   -0.882
#&amp;gt; 10 high   animal        0    7.55 
#&amp;gt; # ... with 9,152 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make separate plot for two measures and then combine them togther.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tf_idf &amp;lt;- key_words %&amp;gt;% 
  group_by(rating) %&amp;gt;% 
  arrange(-tf_idf) %&amp;gt;% 
  slice(1:20) %&amp;gt;% 
  ggplot(aes(
    y = reorder_within(word, tf_idf, rating),
    x = tf_idf,
    fill = rating)) + 
  geom_col(show.legend = FALSE, alpha = 0.6) + 
  scale_y_reordered() + 
  scale_x_continuous(position = &amp;quot;top&amp;quot;) + 
  nord::scale_fill_nord(palette = &amp;quot;afternoon_prarie&amp;quot;) +
  facet_wrap(~ rating, scales = &amp;quot;free_y&amp;quot;, strip.position = &amp;quot;bottom&amp;quot;) + 
  labs(title = &amp;quot;High tf-idf words&amp;quot;,
       x = NULL,
       y = NULL) + 
  hrbrthemes::theme_modern_rc() + 
  theme(panel.grid.major.y = element_blank(),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = 28),
        plot.title.position = &amp;quot;plot&amp;quot;,
        axis.text.y = element_text(size = 16),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16), 
        strip.text = element_text(size = 24, color = &amp;quot;white&amp;quot;)) 

log_odds &amp;lt;- key_words %&amp;gt;% 
  group_by(rating) %&amp;gt;% 
  top_n(20, log_odds) %&amp;gt;%
  ggplot(aes(
    y = reorder_within(word, log_odds, rating),
    x = log_odds,
    fill = rating)) + 
  geom_col(show.legend = FALSE, alpha = 0.6) + 
  scale_y_reordered() + 
  scale_x_continuous(position = &amp;quot;top&amp;quot;) +  
  nord::scale_fill_nord(palette = &amp;quot;afternoon_prarie&amp;quot;) +
  facet_wrap(~ rating, scales = &amp;quot;free_y&amp;quot;, strip.position = &amp;quot;bottom&amp;quot;) + 
  labs(title = &amp;quot;High log-odds words&amp;quot;,
       x = NULL,
       y = NULL) + 
  hrbrthemes::theme_modern_rc() + 
  theme(panel.grid.major.y = element_blank(),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = 28),
        plot.title.position = &amp;quot;plot&amp;quot;,
        axis.text.y = element_text(size = 16),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16), 
        strip.text = element_blank()) 

patchwork::wrap_plots(tf_idf, log_odds, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Emmm… this is a little better, isn’t it 😅? The &lt;code&gt;tf_idf&lt;/code&gt; plot performed well in identifying characteristic words in low ratings like “unacceptable”, “wtf” and “boring”, While the &lt;code&gt;log_odds&lt;/code&gt; plo shows a somewhat dominance of words like “fun”, “cute” and “relaxing” in high ratings.&lt;/p&gt;
&lt;p&gt;Additionally, we may also be interested in words that tend to co-occur within a particular review. For simplicity I focus on long reivews only (more than 800 characters).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(widyr)

word_cors &amp;lt;- reviews_clean %&amp;gt;%  
  filter(str_length(text) &amp;gt; 800) %&amp;gt;%
  select(user_name, text) %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  anti_join(stop_words) %&amp;gt;% 
  filter(!str_detect(word, &amp;quot;^\\d+$&amp;quot;)) %&amp;gt;% 
  group_by(word) %&amp;gt;% 
  filter(n() &amp;gt; 10) %&amp;gt;% 
  count(user_name, word) %&amp;gt;% 
  pairwise_cor(item = word, feature = user_name, value = n)

word_cors
#&amp;gt; # A tibble: 102,720 x 3
#&amp;gt;    item1      item2 correlation
#&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
#&amp;gt;  1 ability    3ds       -0.0402
#&amp;gt;  2 absolutely 3ds       -0.0125
#&amp;gt;  3 ac         3ds        0.158 
#&amp;gt;  4 access     3ds       -0.0457
#&amp;gt;  5 account    3ds        0.170 
#&amp;gt;  6 accounts   3ds        0.271 
#&amp;gt;  7 activities 3ds        0.0348
#&amp;gt;  8 add        3ds        0.0216
#&amp;gt;  9 addition   3ds        0.0770
#&amp;gt; 10 additional 3ds       -0.0414
#&amp;gt; # ... with 102,710 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
library(tidygraph)

word_cors %&amp;gt;%
  filter(correlation &amp;gt; 0.4) %&amp;gt;% 
  as_tbl_graph() %&amp;gt;% 
  ggraph(layout = &amp;quot;fr&amp;quot;) + 
  geom_edge_link(aes(alpha = correlation), show.legend = FALSE) + 
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 6.5) + 
  geom_node_text(aes(label = name), repel = TRUE, size = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-modeling-for-rating&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predictive modeling for rating&lt;/h1&gt;
&lt;p&gt;It takes some steps to derive from &lt;code&gt;reviews_clean&lt;/code&gt; a design matrix for modeling. The &lt;a href=&#34;https://tidymodels.github.io/textrecipes/index.html&#34;&gt;&lt;code&gt;textrecipes&lt;/code&gt;&lt;/a&gt; package contains extra steps for &lt;code&gt;recipes&lt;/code&gt; for preprocessing text data that could have replaced my manual wrangling. But I havn’t digged into that now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)

model_df &amp;lt;- reviews_clean %&amp;gt;% 
  filter_all(all_vars(!is.na(.))) %&amp;gt;% 
  transmute(rating, 
            user_name, 
            text,
            t = as.numeric(date - ymd(&amp;quot;2020-03-20&amp;quot;))) %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;% 
  anti_join(stop_words) %&amp;gt;%
  count(user_name, t, rating, word, name = &amp;quot;word_count&amp;quot;) %&amp;gt;% 
  group_by(word) %&amp;gt;% 
  filter(n() &amp;gt; 20, word != &amp;quot;rating&amp;quot;, !str_detect(word, &amp;quot;^\\d+$&amp;quot;)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  pivot_wider(names_from = word, values_from = word_count, 
              values_fill = list(word_count = 0), names_repair = &amp;quot;minimal&amp;quot;)
  

model_df
#&amp;gt; # A tibble: 1,392 x 320
#&amp;gt;    user_name     t rating playing  stop ability accounts   buy `can’t` console
#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;
#&amp;gt;  1 000PLAYE~     0 high         1     1       0        0     0       0       0
#&amp;gt;  2 11_11         4 low          1     0       1        3     2       1       2
#&amp;gt;  3 12hwilso      4 low          0     0       0        1     0       0       2
#&amp;gt;  4 3nd3r02       6 low          0     0       0        0     0       0       0
#&amp;gt;  5 425_Flex      3 low          0     0       0        0     0       0       0
#&amp;gt;  6 486eHyMy      2 low          0     0       0        0     0       0       0
#&amp;gt;  7 4Plants       5 high         0     0       0        0     0       0       0
#&amp;gt;  8 8bheotap~     6 low          0     0       0        0     0       0       2
#&amp;gt;  9 A_Mighty~     6 medium       3     0       0        1     3       0       0
#&amp;gt; 10 a0972354      4 high         0     0       0        0     0       0       0
#&amp;gt; # ... with 1,382 more rows, and 310 more variables: consoles &amp;lt;int&amp;gt;,
#&amp;gt; #   `don’t` &amp;lt;int&amp;gt;, excited &amp;lt;int&amp;gt;, extremely &amp;lt;int&amp;gt;, families &amp;lt;int&amp;gt;,
#&amp;gt; #   forcing &amp;lt;int&amp;gt;, fun &amp;lt;int&amp;gt;, game &amp;lt;int&amp;gt;, games &amp;lt;int&amp;gt;, greedy &amp;lt;int&amp;gt;,
#&amp;gt; #   household &amp;lt;int&amp;gt;, island &amp;lt;int&amp;gt;, money &amp;lt;int&amp;gt;, multiple &amp;lt;int&amp;gt;, nintendo &amp;lt;int&amp;gt;,
#&amp;gt; #   play &amp;lt;int&amp;gt;, purchase &amp;lt;int&amp;gt;, saves &amp;lt;int&amp;gt;, separate &amp;lt;int&amp;gt;, sister &amp;lt;int&amp;gt;,
#&amp;gt; #   spent &amp;lt;int&amp;gt;, stupid &amp;lt;int&amp;gt;, true &amp;lt;int&amp;gt;, account &amp;lt;int&amp;gt;, broken &amp;lt;int&amp;gt;,
#&amp;gt; #   fix &amp;lt;int&amp;gt;, people &amp;lt;int&amp;gt;, progress &amp;lt;int&amp;gt;, `1st` &amp;lt;int&amp;gt;, animal &amp;lt;int&amp;gt;,
#&amp;gt; #   crossing &amp;lt;int&amp;gt;, experience &amp;lt;int&amp;gt;, gorgeous &amp;lt;int&amp;gt;, idea &amp;lt;int&amp;gt;, love &amp;lt;int&amp;gt;,
#&amp;gt; #   makes &amp;lt;int&amp;gt;, player &amp;lt;int&amp;gt;, reason &amp;lt;int&amp;gt;, score &amp;lt;int&amp;gt;, worth &amp;lt;int&amp;gt;,
#&amp;gt; #   control &amp;lt;int&amp;gt;, family &amp;lt;int&amp;gt;, giving &amp;lt;int&amp;gt;, grab &amp;lt;int&amp;gt;, kids &amp;lt;int&amp;gt;,
#&amp;gt; #   negative &amp;lt;int&amp;gt;, resources &amp;lt;int&amp;gt;, review &amp;lt;int&amp;gt;, switch &amp;lt;int&amp;gt;,
#&amp;gt; #   unacceptable &amp;lt;int&amp;gt;, user &amp;lt;int&amp;gt;, users &amp;lt;int&amp;gt;, girlfriend &amp;lt;int&amp;gt;,
#&amp;gt; #   players &amp;lt;int&amp;gt;, time &amp;lt;int&amp;gt;, wait &amp;lt;int&amp;gt;, day &amp;lt;int&amp;gt;, horizons &amp;lt;int&amp;gt;,
#&amp;gt; #   house &amp;lt;int&amp;gt;, leaf &amp;lt;int&amp;gt;, past &amp;lt;int&amp;gt;, played &amp;lt;int&amp;gt;, view &amp;lt;int&amp;gt;,
#&amp;gt; #   allowing &amp;lt;int&amp;gt;, designed &amp;lt;int&amp;gt;, gamecube &amp;lt;int&amp;gt;, real &amp;lt;int&amp;gt;, single &amp;lt;int&amp;gt;,
#&amp;gt; #   version &amp;lt;int&amp;gt;, bought &amp;lt;int&amp;gt;, buying &amp;lt;int&amp;gt;, choice &amp;lt;int&amp;gt;, community &amp;lt;int&amp;gt;,
#&amp;gt; #   decision &amp;lt;int&amp;gt;, expected &amp;lt;int&amp;gt;, features &amp;lt;int&amp;gt;, feel &amp;lt;int&amp;gt;, feels &amp;lt;int&amp;gt;,
#&amp;gt; #   fine &amp;lt;int&amp;gt;, forced &amp;lt;int&amp;gt;, future &amp;lt;int&amp;gt;, gameplay &amp;lt;int&amp;gt;, huge &amp;lt;int&amp;gt;,
#&amp;gt; #   issue &amp;lt;int&amp;gt;, left &amp;lt;int&amp;gt;, lot &amp;lt;int&amp;gt;, mechanics &amp;lt;int&amp;gt;, multiplayer &amp;lt;int&amp;gt;,
#&amp;gt; #   op &amp;lt;int&amp;gt;, person &amp;lt;int&amp;gt;, plays &amp;lt;int&amp;gt;, reviews &amp;lt;int&amp;gt;, shame &amp;lt;int&amp;gt;,
#&amp;gt; #   sharing &amp;lt;int&amp;gt;, started &amp;lt;int&amp;gt;, system &amp;lt;int&amp;gt;, bombing &amp;lt;int&amp;gt;, dont &amp;lt;int&amp;gt;,
#&amp;gt; #   hate &amp;lt;int&amp;gt;, islands &amp;lt;int&amp;gt;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After tokenizing, filtering and some other steps, I have a ready-for-modeling design matrix at hand. &lt;code&gt;user_name&lt;/code&gt; is an ID variable, &lt;code&gt;t&lt;/code&gt; indicates the number of days after 2020-03-20 when the first review was made. All other columns, besides the response &lt;code&gt;rating&lt;/code&gt;, are word counts.&lt;/p&gt;
&lt;p&gt;Next I split the data into training and testing test with stratified sampling on &lt;code&gt;rating&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

set.seed(2020)
reviews_split &amp;lt;- initial_split(model_df, strata = rating)
reviews_train &amp;lt;- training(reviews_split)
reviews_test &amp;lt;- testing(reviews_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I choose to fit a multinomial logisitic regression model run by the &lt;code&gt;glmnet&lt;/code&gt; package, with L1 regularization as in the lasso model. To detect medium ratings more accurately, the minority class, &lt;code&gt;step_upsample&lt;/code&gt; will bring the number of meidum and high ratings up to the same (&lt;code&gt;over_ratio = 1&lt;/code&gt; ) as that of low ratings. And &lt;code&gt;tune_gird()&lt;/code&gt; will calculate model performance metrics averaged over 25 bootstrap resamples for 100 choices of lambda.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multinom_spec &amp;lt;- multinom_reg(mixture = 1, penalty = tune()) %&amp;gt;% 
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;classification&amp;quot;) 

library(themis)
rec &amp;lt;- recipe(rating ~ ., data = reviews_train) %&amp;gt;%
  update_role(user_name, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_upsample(rating, over_ratio = 1) %&amp;gt;% 
  step_normalize(all_predictors())

lambda_grid &amp;lt;- grid_regular(penalty(), levels = 100)
  
reviews_folds &amp;lt;- bootstraps(reviews_train, strata = rating)

wf &amp;lt;- workflow() %&amp;gt;% 
  add_model(multinom_spec) %&amp;gt;% 
  add_recipe(rec)

doParallel::registerDoParallel()

multinom_search &amp;lt;- tune_grid(wf, 
                    resamples = reviews_folds,
                    grid = lambda_grid,
                    metrics = metric_set(roc_auc, accuracy, kap))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Available metrics are ROC AUC, accuracy and Kappa. In multiclass cases, accuracy and Kappa use the same definitions as their binary counterpart, with accuracy counting up the number of correctly predicted true values out of the total number of true values, and Kappa being a linear combination of two accuracy values, sensitivity and specificity. Multiclass ROC AUC, however, is implemented as the “hand_till” method which I won’t venture to interpret now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multinom_search %&amp;gt;%
  collect_metrics() %&amp;gt;% 
  ggplot(aes(penalty, mean, color = .metric)) + 
  geom_line() + 
  geom_errorbar(aes(ymax = mean + std_err, ymin = mean - std_err)) + 
  scale_x_log10(labels = scales::label_number_auto()) + 
  facet_wrap(~ .metric, scales = &amp;quot;free_y&amp;quot;) + 
  labs(y = NULL, x = expression(lambda)) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s clear from the plot that all 3 metrics benefit from appropriate regularization, and we can identify a local maximum in all penals at rougly the same lambda. Here I use the “one-standard error rule” that selects model with largest lambda that is within one standard error of the numerically optimal Kappa metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_lambda &amp;lt;- multinom_search %&amp;gt;% 
  select_by_one_std_err(metric = &amp;quot;kap&amp;quot;, desc(penalty))

best_lambda
#&amp;gt; # A tibble: 1 x 8
#&amp;gt;   penalty .metric .estimator  mean     n std_err .best .bound
#&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#&amp;gt; 1  0.0192 kap     multiclass 0.527    25 0.00685 0.533  0.526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can finalize and fill the model with this lambda.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf_final &amp;lt;- finalize_workflow(wf, best_lambda)

final_model &amp;lt;- last_fit(wf_final, split = reviews_split, 
                        metrics = metric_set(roc_auc, accuracy, kap))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our model, the confusion matrix becomes 3 x 3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_model %&amp;gt;% 
  collect_predictions() %&amp;gt;% 
  conf_mat(rating, estimate = .pred_class)
#&amp;gt;           Truth
#&amp;gt; Prediction low medium high
#&amp;gt;     low    104     17    7
#&amp;gt;     medium  31      8   12
#&amp;gt;     high    18      7  143&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to downsmapling, the classifier performs quite consistently in predicting these 3 categories. Detection for low ratings may leave some room for improvement.&lt;/p&gt;
&lt;p&gt;Then we could examine these metrics on the model applied to testing set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_model %&amp;gt;% 
  collect_metrics()
#&amp;gt; # A tibble: 3 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy multiclass     0.735
#&amp;gt; 2 kap      multiclass     0.556
#&amp;gt; 3 roc_auc  hand_till      0.746&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These number may not look so nice in terms of accuracy and ROC AUC, but there is a tradeoff happening. When I was still experimenting on different models I trained one that would miss all the medium ratings in the testing set, but did achieve relatively high predictive metrics. Then I decided to add the &lt;code&gt;step_upsampling&lt;/code&gt; step to enhance detection towards medium ratings. Although the game campany may not actually care about those mild people as much as they do about those go to extremes. For another, the best penality is judged by the Kappa statistic, which shows reasonable agreement.&lt;/p&gt;
&lt;p&gt;Varible importance plot could help us to identify useful features. For multiclass logit models, importance is defined as the sum of absolute value of coef of a variable. For example, in our baseline logit models:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\log(\frac{P(medium)}{P(low)}) &amp;amp;= \beta_20 + \beta_{21}x_1 + \cdots + \beta_{2p}x_p \\
\log(\frac{P(high)}{P(low)}) &amp;amp;= \beta_30 + \beta_{31}x_1 + \cdots + \beta_{3p}x_p
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The absolute value of variable importance for predictor &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(|\hat{\beta}_{21}| + |\hat{\beta}_{31}|\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now I inspect predictors with top absolute variable importance to conclude this minimal project. If a predictor has high positive / negative importance, then it help us to judge whether a user is more intended to give higher ratings or otherwise, similar to sentiment analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit &amp;lt;- wf_final %&amp;gt;% fit(data = reviews_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vip)

final_fit %&amp;gt;% 
  pull_workflow_fit() %&amp;gt;% 
  vi(lambda = best_lambda$penalty) %&amp;gt;%
  group_by(Sign) %&amp;gt;% 
  top_n(30, wt = abs(Importance)) %&amp;gt;% 
  ungroup() %&amp;gt;%
  mutate(Sign = if_else(Sign == &amp;quot;NEG&amp;quot;, &amp;quot;lower ratings&amp;quot;, &amp;quot;higher ratings&amp;quot;)) %&amp;gt;% 
  ggplot(aes(y = reorder_within(Variable, abs(Importance), Sign),
             x = Importance,
             fill = Sign)) + 
  geom_col(show.legend = FALSE, alpha = 0.5) +
  scale_y_reordered() + 
  facet_wrap(~ Sign, scales = &amp;quot;free&amp;quot;) + 
  labs(y = NULL) + 
  theme(axis.text = element_text(size = 20),
        panel.grid.major.y = element_blank(),
        strip.text = element_text(size = 24, face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Lego Database</title>
      <link>/post/exploring-lego/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-lego/</guid>
      <description>


&lt;p&gt;This week I’ve had some recap on data wrangling in Python, specifically in &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;NumPy&lt;/code&gt;. As an exercise, are available &lt;a href=&#34;https://rebrickable.com/downloads/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd

colors = pd.read_csv(&amp;quot;D:/RProjects/data/blog/lego/colors.csv&amp;quot;)
colors.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id            name     rgb is_trans
## 0  -1       [Unknown]  0033B2        f
## 1   0           Black  05131D        f
## 2   1            Blue  0055BF        f
## 3   2           Green  237841        f
## 4   3  Dark Turquoise  008F9B        f&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many colors are available?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;colors[[&amp;#39;name&amp;#39;]].info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 184 entries, 0 to 183
## Data columns (total 1 columns):
## name    184 non-null object
## dtypes: object(1)
## memory usage: 1.6+ KB&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing International Debt Statistics</title>
      <link>/post/international-debt-sql/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/international-debt-sql/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sql-quries&#34;&gt;SQL quries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visulization-countries-with-highest-total-debt.&#34;&gt;Visulization: countries with highest total debt.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post, I use SQL to retrieve and analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD 💵) owed by developing countries across several categories. In fact, I adopted this from one &lt;a href=&#34;https://learn.datacamp.com/projects/754&#34;&gt;DataCamp project&lt;/a&gt; without following its instructions. The project is still insightful and well-written, though. Also, the R Markdown documentation has a section on how to &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html#sqls&#34;&gt;embed SQL chunks&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;sql-quries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SQL quries&lt;/h1&gt;
&lt;p&gt;After connecting the a database, I start by &lt;code&gt;CREATE&lt;/code&gt; the &lt;code&gt;international_debt&lt;/code&gt; table, and load data into R as well.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;CREATE TABLE international_debt (
    country_name varchar(50),
    country_code varchar(10),
    indicator_name varchar(100),
    indicator_code varchar(20),
    debt decimal(12, 1)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

international_debt &amp;lt;- readr::read_csv(&amp;quot;D:/RProjects/data/blog/international_debt.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can upload debt data into that table. If you happen to be a datacamp subscriber, &lt;a href=&#34;https://support.datacamp.com/hc/en-us/articles/360020444334-How-to-Download-Project-Datasets&#34;&gt;here&lt;/a&gt; are some instructions on how to dowanload the data. &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;COPY international_debt
FROM &amp;#39;D:/RProjects/data/blog/international_debt.csv&amp;#39;
WITH (FORMAT csv, header)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;international_debt&lt;/code&gt; has debt information about 124 countries and 4714 rows in total, with each row being one type of debt statistics owed by one country or region.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;-- a glance a debt data
SELECT *
FROM international_debt
LIMIT 10&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;country_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;country_code&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;indicator_code&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;debt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.DLXF.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72894454&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, long-term (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.DLXF.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53239440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61739337&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49114729&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39903620&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39107845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23779724&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13335820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.OFFT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100847182&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AFG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.OFFT.CD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72894454&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;-- how many countries
SELECT COUNT(DISTINCT country_code) as n_countries FROM international_debt&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;n_countries&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;-- how many reords
SELECT COUNT(*) AS n_records FROM international_debt&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;n_records&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2357&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;indicator_code&lt;/code&gt; column represents the category of these debts. Knowing about these various debt indicators will help us to understand the areas in which a country can possibly be indebted to.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT DISTINCT indicator_code, indicator_name FROM international_debt &lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;indicator_code&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.BLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.PCBK.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.DLXF.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, long-term (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.OFFT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.PROP.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.DPNG.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, private nonguaranteed (PNG) (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.PRVT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.DLXF.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.PRVT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.OFFT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.PROP.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.PROP.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.MLAT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.DPNG.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, private nonguaranteed (PNG) (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.PBND.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.PRVT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.PBND.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.DIS.PCBK.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (DIS, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.AMT.DLXF.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.PCBK.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DT.INT.OFFT.CD&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (INT, current US$)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now, I come to answer questions involving some simple calculations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the total amount of debt of all types? This is a measure of the health of the global economy.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT ROUND(SUM(debt), 2) AS total_debt FROM international_debt&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;total_debt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3079734487675.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Which country has the highest total debt?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT country_name, SUM(debt) AS total_debt
FROM international_debt
GROUP BY country_name
ORDER BY total_debt DESC
LIMIT 20&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 1: &lt;/span&gt;Countries with highest debt&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;country_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;total_debt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;China&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;285793494734&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Brazil&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;280623966141&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;South Asia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;247608723991&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Least developed countries: UN classification&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;212880992792&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Russian Federation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;191289057259&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IDA only&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179048127207&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Turkey&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;151125758035&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;India&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133627060958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mexico&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124596786217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Indonesia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;113435696694&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cameroon&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86491206347&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Angola&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71368842500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kazakhstan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70159942694&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Egypt, Arab Rep.&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62077727757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Vietnam&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45851299896&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Colombia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45430117605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Pakistan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45139315399&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Romania&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42813979498&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;South Africa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36703940743&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Venezuela, RB&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36048260108&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Here we see the top 20 countries with highest overall debt. In fact, some of the entries in &lt;code&gt;country_name&lt;/code&gt; are not countries but regions, such “South Asia”, “Least developed countries: UN classification” and “IDA only”.&lt;/p&gt;
&lt;p&gt;Now that we know China is in most debt, we could break China’s dbet down to see the proportion for which different types of loan accounted.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT  indicator_name, debt, 
        (debt / sum(debt) OVER()) AS proportion
FROM international_debt
WHERE country_name = &amp;#39;China&amp;#39;
ORDER BY proportion DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;debt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;proportion&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96218620836&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3366718&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, private nonguaranteed (PNG) (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72392986214&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2533052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, long-term (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17866548651&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0625156&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15692563746&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0549088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14677464466&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0513569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, private nonguaranteed (PNG) (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14142718752&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0494858&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9834677000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0344118&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9148170156&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0320097&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6532446442&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0228572&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4111062474&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0143847&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4046243299&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0141579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3777050273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0132160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3079501272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0107753&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3079501272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0107753&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2615723714&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0091525&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2350524518&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0082246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1373305382&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0048052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1224249000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0042837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;969933090&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0033938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;858406975&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0030036&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;796544167&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0027871&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;514898407&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0018016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;334012201&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0011687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;156342428&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0005470&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Two of all categories of debt, long-term and private nonguaranteed principle repayments on external debt take up more than 50% of China’s total debt.&lt;/p&gt;
&lt;p&gt;We can dig even further to find out on an average how much debt a country owes. This will give us a better sense of the distribution of the amount of debt across different indicators.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT indicator_name, avg(debt) AS mean_debt
FROM international_debt
GROUP BY indicator_name
ORDER BY mean_debt DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_debt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5904868401&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, private nonguaranteed (PNG) (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5161194334&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2152041217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1958983453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1803694102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, long-term (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1644024068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1223139290&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, private nonguaranteed (PNG) (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1220410844&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1191187963&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1082623948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;839843679&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;804733377&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;746888800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;734868743&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;719740180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;712619635&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;490062193&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;311323265&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;297677339&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;293305196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;164093286&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;156647613&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;136230719&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81135161&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34250651&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;A bit of visualization might help here, I’ ll make a density plot of mean debt across all indicators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- international_debt %&amp;gt;% 
  group_by(indicator_name) %&amp;gt;% 
  summarize(mean_debt = mean(debt)) %&amp;gt;%
  ggplot() + 
  geom_density(aes(mean_debt), fill = &amp;quot;midnightblue&amp;quot;, alpha = 0.4) + 
  scale_x_continuous(labels = scales::label_number_si(prefix = &amp;quot;$&amp;quot;)) +
  theme_minimal() + 
  theme(axis.text.y = element_blank()) + 
  labs(title = &amp;quot;Distribution of the average debt across different indicators&amp;quot;,
        y = NULL,
        x = NULL)

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-05-analyzing-international-debt-statistics/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;816&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One may notice that principle repayment of long term debts tops the table of average debt and debt proportion of China. As such, we can find the top 10 countries with highest amount of debt in the category of long term debts (&lt;code&gt;DT.AMT.DLXF.CD&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT DISTINCT country_name
FROM international_debt
WHERE country_name IN (
    SELECT country_name 
    FROM international_debt
    WHERE indicator_code = &amp;#39;DT.AMT.DLXF.CD&amp;#39; 
    ORDER BY debt DESC
    LIMIT 10
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;country_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Turkey&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Russian Federation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Brazil&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mexico&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Least developed countries: UN classification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;South Asia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;China&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kazakhstan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;India&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Indonesia&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We saw that long-term debt is the topmost category when it comes to the average amount of debt. But is it the most common indicator in which the countries owe their debt?&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT indicator_name, COUNT(indicator_name) As n_indicator
FROM international_debt
GROUP BY indicator_name 
ORDER BY n_indicator DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_indicator&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, long-term (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, multilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bilateral (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, private nonguaranteed (PNG) (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interest payments on external debt, private nonguaranteed (PNG) (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, bonds (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (INT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, commercial banks (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PPG, other private creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Turns out it is the second most common category of debt. But what is the average amount of the most common debt type, &lt;code&gt;DT.INT.OFFT.CD&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT avg(debt) as mean_debt
FROM international_debt
WHERE indicator_code = &amp;#39;DT.INT.OFFT.CD&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;mean_debt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;297677338.957258&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-05-analyzing-international-debt-statistics/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;816&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By inspecting the six indicaotors in which all the countries listed in our dataset have taken debt (&lt;code&gt;n_indicator = 124&lt;/code&gt;), we have a clue that all these countries are suffering from some common economic issues. Another problem is what is the most serious issus each country has? We can look into this by retrieveing maximum of debt of all categories of each country.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;-- some countries have tied max debt on multiple categories
WITH max_debt AS (
    SELECT country_name, max(debt) AS maximum
    FROM international_debt
    GROUP BY country_name
    HAVING max(debt) &amp;lt;&amp;gt; 0
)
SELECT  max_debt.country_name, indicator_name, maximum FROM max_debt
  LEFT JOIN (SELECT country_name, indicator_name, debt FROM international_debt) AS debt 
    ON max_debt.maximum = debt.debt
    AND max_debt.country_name = debt.country_name
ORDER BY maximum DESC
LIMIT 20&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;country_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;indicator_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;maximum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96218620836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Brazil&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90041840304&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Russian Federation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66589761834&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Turkey&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51555031006&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;South Asia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48756295898&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Least developed countries: UN classification&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40160766262&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IDA only&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34531188113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;India&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31923507001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Indonesia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30916112654&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kazakhstan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27482093686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mexico&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25218503927&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cameroon&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Disbursements on external debt, long-term (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18186662060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Romania&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14013783350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Colombia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11985674439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Angola&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11067045628&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Venezuela, RB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9878659207&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Egypt, Arab Rep.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9692114177&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Lebanon&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9506919670&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;South Africa&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Principal repayments on external debt, long-term (AMT, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9474257552&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bangladesh&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PPG, official creditors (DIS, current US$)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9050557612&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visulization-countries-with-highest-total-debt.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visulization: countries with highest total debt.&lt;/h1&gt;
&lt;p&gt;Finally, let’s make a plot again to show the top 20 countries with highest debt, as in table 1, plus the specific category in which they take highest debt in.
This time I exclude non-country entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# prepare data for plot
maximum_category &amp;lt;- international_debt %&amp;gt;% 
  group_by(country_name) %&amp;gt;% 
  top_n(1, debt) %&amp;gt;%
  distinct(country_name, .keep_all = TRUE) %&amp;gt;% 
  select(country_name, indicator_name)

countries &amp;lt;- international_debt %&amp;gt;% 
  filter(!country_name %in% c(&amp;quot;South Asia&amp;quot;,
                              &amp;quot;Least developed countries: UN classification&amp;quot;,
                              &amp;quot;IDA only&amp;quot;)) %&amp;gt;%
  group_by(country_name) %&amp;gt;% 
  summarize(total_debt = sum(debt)) %&amp;gt;% 
  top_n(20, total_debt) %&amp;gt;% 
  left_join(maximum_category) 

countries
#&amp;gt; # A tibble: 20 x 3
#&amp;gt;    country_name       total_debt indicator_name                                 
#&amp;gt;    &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                                          
#&amp;gt;  1 Angola                7.14e10 Principal repayments on external debt, long-te~
#&amp;gt;  2 Bangladesh            3.50e10 Disbursements on external debt, long-term (DIS~
#&amp;gt;  3 Brazil                2.81e11 Principal repayments on external debt, long-te~
#&amp;gt;  4 Cameroon              8.65e10 Disbursements on external debt, long-term (DIS~
#&amp;gt;  5 China                 2.86e11 Principal repayments on external debt, long-te~
#&amp;gt;  6 Colombia              4.54e10 Principal repayments on external debt, long-te~
#&amp;gt;  7 Egypt, Arab Rep.      6.21e10 Principal repayments on external debt, long-te~
#&amp;gt;  8 India                 1.34e11 Principal repayments on external debt, long-te~
#&amp;gt;  9 Indonesia             1.13e11 Principal repayments on external debt, long-te~
#&amp;gt; 10 Kazakhstan            7.02e10 Principal repayments on external debt, long-te~
#&amp;gt; 11 Lebanon               2.97e10 Principal repayments on external debt, long-te~
#&amp;gt; 12 Mexico                1.25e11 Principal repayments on external debt, long-te~
#&amp;gt; 13 Pakistan              4.51e10 Principal repayments on external debt, long-te~
#&amp;gt; 14 Romania               4.28e10 Principal repayments on external debt, long-te~
#&amp;gt; 15 Russian Federat~      1.91e11 Principal repayments on external debt, long-te~
#&amp;gt; 16 South Africa          3.67e10 Principal repayments on external debt, long-te~
#&amp;gt; 17 Turkey                1.51e11 Principal repayments on external debt, long-te~
#&amp;gt; 18 Ukraine               2.85e10 Principal repayments on external debt, long-te~
#&amp;gt; 19 Venezuela, RB         3.60e10 Principal repayments on external debt, long-te~
#&amp;gt; 20 Vietnam               4.59e10 Principal repayments on external debt, long-te~&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggchicklet)
library(ggtext)
library(showtext)
font_add_google(&amp;quot;Overpass Mono&amp;quot;, &amp;quot;Overpass Mono&amp;quot;)
font_add_google(&amp;quot;Roboto Condensed&amp;quot;, &amp;quot;Roboto Condensed&amp;quot;)
showtext_auto()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(countries) + 
  geom_chicklet(aes(x =  fct_reorder(country_name, total_debt),
                    y = total_debt,
                    fill = indicator_name), 
           color = NA, width = 0.8) + 
  geom_text(aes(country_name, total_debt, label = scales::label_number_si()(total_debt)),
            color = &amp;quot;white&amp;quot;, nudge_y = -10000000000, family = &amp;quot;Overpass Mono&amp;quot;) + 
  scale_y_continuous(labels = scales::label_number_si(prefix = &amp;quot;$&amp;quot;)) + 
  hrbrthemes::theme_modern_rc() + 
  nord::scale_fill_nord(palette = &amp;quot;afternoon_prarie&amp;quot;, name = NA) + 
  coord_flip(clip = &amp;quot;off&amp;quot;) +
  labs(x = NULL,
       y = NULL,
       title = &amp;quot;Top 20 Countries with Highest Total Debts&amp;quot;,
       subtitle = &amp;quot;highest contributions from long term &amp;lt;span style=&amp;#39;color:#F0D8C0&amp;#39;&amp;gt;repayments&amp;lt;/span&amp;gt; or &amp;lt;span style=&amp;#39;color:#6078A8&amp;#39;&amp;gt;disbursements&amp;lt;/span&amp;gt;&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;,
        plot.title = element_text(size = 28, family = &amp;quot;Roboto Condensed&amp;quot;),
        plot.title.position = &amp;quot;plot&amp;quot;,
        plot.subtitle = element_markdown(family = &amp;quot;Roboto Condensed&amp;quot;),
        axis.text.x = element_text(face = &amp;quot;bold&amp;quot;, size = 14),
        axis.text.y = element_text(face = &amp;quot;bold&amp;quot;, size = 18),
        panel.grid.major.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-05-analyzing-international-debt-statistics/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The following chunk is not a real SQL query but plain text. The knitr SQL engine currently only looks for the keywords that are among &lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;UPDATE&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt;, &lt;code&gt;CREATE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt;. You have to run the command inside the database.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text Classfication with Penalized Logistic Regression</title>
      <link>/post/text-classification-logistic/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/text-classification-logistic/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-word-frequency&#34;&gt;Comparing word frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling&#34;&gt;Modeling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#train-a-penalized-logistic-regression-model&#34;&gt;Train a penalized logistic regression model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tuning-lambda&#34;&gt;Tuning lambda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post I aim to train a text classification model with penalized logistic regression using the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt; framework. Data are from 5 books and downloaded via the &lt;a href=&#34;https://docs.ropensci.org/gutenbergr/&#34;&gt;&lt;code&gt;gutenbergr&lt;/code&gt;&lt;/a&gt; package, written by either Emily Brontë or Charlotte Brontë. And the goal is to predict the author of a line, in other words the probability of line being written by one sister instead of another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books &amp;lt;- gutenberg_works() %&amp;gt;% 
  filter(str_detect(author, &amp;quot;Brontë, Emily|Brontë, Charlotte&amp;quot;)) %&amp;gt;% 
  gutenberg_download(meta_fields = c(&amp;quot;title&amp;quot;, &amp;quot;author&amp;quot;)) %&amp;gt;% 
  transmute(title,
            author = if_else(author == &amp;quot;Brontë, Emily&amp;quot;, 
                             &amp;quot;Emily Brontë&amp;quot;, 
                             &amp;quot;Charlotte Brontë&amp;quot;) %&amp;gt;% factor(),
            line_index = row_number(),
            text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;books&lt;/code&gt; is at line level&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books
#&amp;gt; # A tibble: 88,989 x 4
#&amp;gt;    title        author     line_index text                                      
#&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                     
#&amp;gt;  1 Wuthering H~ Emily Bro~          1 &amp;quot;WUTHERING HEIGHTS&amp;quot;                       
#&amp;gt;  2 Wuthering H~ Emily Bro~          2 &amp;quot;&amp;quot;                                        
#&amp;gt;  3 Wuthering H~ Emily Bro~          3 &amp;quot;&amp;quot;                                        
#&amp;gt;  4 Wuthering H~ Emily Bro~          4 &amp;quot;CHAPTER I&amp;quot;                               
#&amp;gt;  5 Wuthering H~ Emily Bro~          5 &amp;quot;&amp;quot;                                        
#&amp;gt;  6 Wuthering H~ Emily Bro~          6 &amp;quot;&amp;quot;                                        
#&amp;gt;  7 Wuthering H~ Emily Bro~          7 &amp;quot;1801.--I have just returned from a visit~
#&amp;gt;  8 Wuthering H~ Emily Bro~          8 &amp;quot;neighbour that I shall be troubled with.~
#&amp;gt;  9 Wuthering H~ Emily Bro~          9 &amp;quot;country!  In all England, I do not belie~
#&amp;gt; 10 Wuthering H~ Emily Bro~         10 &amp;quot;situation so completely removed from the~
#&amp;gt; # ... with 88,979 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain tidy text structure illustrated in &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R&lt;/a&gt;, I use &lt;code&gt;unnest_tokens()&lt;/code&gt; to perform tokenization and remove all the stop words. I also removed characters like &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&#39;s&lt;/code&gt;, &lt;code&gt;&#39;&lt;/code&gt; and whitespaces to return valid column names after widening. But it turns out this served as some sort of stemming too! (heathcliff’s becomes heathcliff). Then low frequency words (whose frequency is less than 0.05% of an author’s total word counts) are removed. The cutoff may be a little too high if you plot that histogram, but I really need this to save computation efforts on my laptop 😅.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_books &amp;lt;- books %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;% 
  filter(!str_detect(word, &amp;quot;^\\d+$&amp;quot;)) %&amp;gt;% 
  mutate(word = str_remove_all(word, &amp;quot;_|&amp;#39;s|&amp;#39;|\\s&amp;quot;))
  
total_words &amp;lt;- clean_books %&amp;gt;%
  count(author, name = &amp;quot;total&amp;quot;)

tidy_books &amp;lt;- clean_books %&amp;gt;%
  left_join(total_words) %&amp;gt;% 
  group_by(author, total, word) %&amp;gt;%
  filter((n() / total) &amp;gt; 0.0005) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-word-frequency&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing word frequency&lt;/h1&gt;
&lt;p&gt;Before building an actual predictive model, let’s do some EDA to see different tendency to use a particular word! This will also shed light on what we would expect from the text classification. Now, we will compare word frequency (proportion) between the two sisters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  group_by(author, total) %&amp;gt;%
  count(word) %&amp;gt;% 
  mutate(prop = n / total) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(-total, -n) %&amp;gt;%
  pivot_wider(names_from  = author, values_from = prop,
              values_fill = list(prop = 0)) %&amp;gt;% 
  ggplot(aes(x = `Charlotte Brontë`, y = `Emily Brontë`, 
             color = abs(`Emily Brontë` -  `Charlotte Brontë`))) + 
  geom_jitter(width = 0.001, height = 0.001, alpha = 0.2, size = 2.5) + 
  geom_abline(color = &amp;quot;gray40&amp;quot;, lty = 2) + 
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, size = 7.5) + 
  scale_color_gradient(low = &amp;quot;darkslategray4&amp;quot;, high = &amp;quot;gray75&amp;quot;) +
  scale_x_continuous(labels = scales::label_percent()) + 
  scale_y_continuous(labels = scales::label_percent()) +  
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  coord_cartesian(xlim = c(0, NA)) + 
  labs(title = &amp;quot;Word frequency between two sisters&amp;quot;) + 
  theme(text = element_text(size = 18),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Words lie near the line such as “home”, “head” and “half” indicate similar tendency to use that word, while those that are far from the line are words that are found more in one set of texts than another, for example “headthcliff”, “linton”, “catherine”, etc.&lt;/p&gt;
&lt;p&gt;What does this plot tell us? Judged only by word frequency, it looks that there are a number of words that are quite characteristic of Emily Brontë (upper left corner). Charlotte, on the other hand, has few representative words (bottom right corner). We will investigate this further in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data preprocessing&lt;/h2&gt;
&lt;p&gt;There are 423 and features (words) and 47119 observations in total. Approximately 18% of the response are 1 (Emily Brontë).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  count(author) %&amp;gt;% 
  mutate(prop = n / sum(n))
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   author               n  prop
#&amp;gt;   &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt; 1 Charlotte Brontë 64858 0.817
#&amp;gt; 2 Emily Brontë     14489 0.183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’s time to widen our data to reach an appropriate model structure, this similar to a document-term matrix, with rows being a line and column word count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
set.seed(2020)
doParallel::registerDoParallel()

model_df &amp;lt;- tidy_books %&amp;gt;% 
  count(line_index, word) %&amp;gt;% 
  pivot_wider(names_from = word, values_from = n,
              values_fill = list(n = 0)) %&amp;gt;% 
  left_join(books, by = c(&amp;quot;line_index&amp;quot; = &amp;quot;line_index&amp;quot;)) %&amp;gt;% 
  select(-title, -text)

model_df
#&amp;gt; # A tibble: 47,119 x 425
#&amp;gt;    line_index heights wuthering chapter returned visit heathcliff heaven black
#&amp;gt;         &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt;  1          1       1         1       0        0     0          0      0     0
#&amp;gt;  2          4       0         0       1        0     0          0      0     0
#&amp;gt;  3          7       0         0       0        1     1          0      0     0
#&amp;gt;  4         11       0         0       0        0     0          1      1     0
#&amp;gt;  5         13       0         0       0        0     0          0      0     1
#&amp;gt;  6         15       0         0       0        0     0          0      0     0
#&amp;gt;  7         18       0         0       0        0     0          1      0     0
#&amp;gt;  8         20       0         0       0        0     0          0      0     0
#&amp;gt;  9         22       0         0       0        0     0          0      0     0
#&amp;gt; 10         23       0         0       0        0     0          0      0     0
#&amp;gt; # ... with 47,109 more rows, and 416 more variables: eyes &amp;lt;int&amp;gt;, heart &amp;lt;int&amp;gt;,
#&amp;gt; #   fingers &amp;lt;int&amp;gt;, answer &amp;lt;int&amp;gt;, sir &amp;lt;int&amp;gt;, hope &amp;lt;int&amp;gt;, grange &amp;lt;int&amp;gt;,
#&amp;gt; #   heard &amp;lt;int&amp;gt;, thrushcross &amp;lt;int&amp;gt;, interrupted &amp;lt;int&amp;gt;, walk &amp;lt;int&amp;gt;,
#&amp;gt; #   closed &amp;lt;int&amp;gt;, uttered &amp;lt;int&amp;gt;, gate &amp;lt;int&amp;gt;, words &amp;lt;int&amp;gt;, horse &amp;lt;int&amp;gt;,
#&amp;gt; #   hand &amp;lt;int&amp;gt;, entered &amp;lt;int&amp;gt;, joseph &amp;lt;int&amp;gt;, bring &amp;lt;int&amp;gt;, suppose &amp;lt;int&amp;gt;,
#&amp;gt; #   nay &amp;lt;int&amp;gt;, dinner &amp;lt;int&amp;gt;, guess &amp;lt;int&amp;gt;, times &amp;lt;int&amp;gt;, wind &amp;lt;int&amp;gt;, house &amp;lt;int&amp;gt;,
#&amp;gt; #   strong &amp;lt;int&amp;gt;, set &amp;lt;int&amp;gt;, wall &amp;lt;int&amp;gt;, door &amp;lt;int&amp;gt;, earnshaw &amp;lt;int&amp;gt;,
#&amp;gt; #   hareton &amp;lt;int&amp;gt;, short &amp;lt;int&amp;gt;, appeared &amp;lt;int&amp;gt;, desire &amp;lt;int&amp;gt;, entrance &amp;lt;int&amp;gt;,
#&amp;gt; #   brought &amp;lt;int&amp;gt;, family &amp;lt;int&amp;gt;, sitting &amp;lt;int&amp;gt;, call &amp;lt;int&amp;gt;, kitchen &amp;lt;int&amp;gt;,
#&amp;gt; #   parlour &amp;lt;int&amp;gt;, deep &amp;lt;int&amp;gt;, observed &amp;lt;int&amp;gt;, light &amp;lt;int&amp;gt;, eye &amp;lt;int&amp;gt;,
#&amp;gt; #   lay &amp;lt;int&amp;gt;, floor &amp;lt;int&amp;gt;, white &amp;lt;int&amp;gt;, countenance &amp;lt;int&amp;gt;, arm &amp;lt;int&amp;gt;,
#&amp;gt; #   chair &amp;lt;int&amp;gt;, seated &amp;lt;int&amp;gt;, round &amp;lt;int&amp;gt;, table &amp;lt;int&amp;gt;, time &amp;lt;int&amp;gt;,
#&amp;gt; #   living &amp;lt;int&amp;gt;, dark &amp;lt;int&amp;gt;, people &amp;lt;int&amp;gt;, hate &amp;lt;int&amp;gt;, love &amp;lt;int&amp;gt;,
#&amp;gt; #   loved &amp;lt;int&amp;gt;, fast &amp;lt;int&amp;gt;, dear &amp;lt;int&amp;gt;, mother &amp;lt;int&amp;gt;, home &amp;lt;int&amp;gt;,
#&amp;gt; #   summer &amp;lt;int&amp;gt;, fine &amp;lt;int&amp;gt;, company &amp;lt;int&amp;gt;, creature &amp;lt;int&amp;gt;, notice &amp;lt;int&amp;gt;,
#&amp;gt; #   told &amp;lt;int&amp;gt;, head &amp;lt;int&amp;gt;, looked &amp;lt;int&amp;gt;, return &amp;lt;int&amp;gt;, poor &amp;lt;int&amp;gt;, till &amp;lt;int&amp;gt;,
#&amp;gt; #   doubt &amp;lt;int&amp;gt;, seat &amp;lt;int&amp;gt;, silence &amp;lt;int&amp;gt;, left &amp;lt;int&amp;gt;, dog &amp;lt;int&amp;gt;,
#&amp;gt; #   master &amp;lt;int&amp;gt;, sat &amp;lt;int&amp;gt;, scarcely &amp;lt;int&amp;gt;, half &amp;lt;int&amp;gt;, hearth &amp;lt;int&amp;gt;,
#&amp;gt; #   arms &amp;lt;int&amp;gt;, fire &amp;lt;int&amp;gt;, purpose &amp;lt;int&amp;gt;, tongue &amp;lt;int&amp;gt;, remained &amp;lt;int&amp;gt;,
#&amp;gt; #   devil &amp;lt;int&amp;gt;, manner &amp;lt;int&amp;gt;, matter &amp;lt;int&amp;gt;, ill &amp;lt;int&amp;gt;, muttered &amp;lt;int&amp;gt;,
#&amp;gt; #   worse &amp;lt;int&amp;gt;, leave &amp;lt;int&amp;gt;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train-a-penalized-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train a penalized logistic regression model&lt;/h2&gt;
&lt;p&gt;Split the data into training set and testing set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_split &amp;lt;- initial_split(model_df)
book_train &amp;lt;- training(book_split)
book_test &amp;lt;- testing(book_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify a L1 penalized logistic model, center and scale all predictors and combine them in to a &lt;code&gt;workflow&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_spec &amp;lt;- logistic_reg(penalty = 0.05, mixture = 1) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

book_rec &amp;lt;- recipe(author ~ ., data = book_train) %&amp;gt;% 
  update_role(line_index, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_zv(all_predictors()) %&amp;gt;% 
  step_normalize(all_predictors())

book_wf &amp;lt;- workflow() %&amp;gt;% 
  add_model(logistic_spec) %&amp;gt;% 
  add_recipe(book_rec)

initial_fit &amp;lt;- book_wf %&amp;gt;% 
  fit(data = book_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;initial_fit&lt;/code&gt; is a simple fitted regression model without any hyperparameters. By default &lt;code&gt;glmnet&lt;/code&gt; calls for 100 values of lambda even if I specify &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0.05\)&lt;/span&gt;. So the extracted result aren’t that helpful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;% 
  tidy()
#&amp;gt; # A tibble: 26,019 x 5
#&amp;gt;    term         step estimate lambda dev.ratio
#&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
#&amp;gt;  1 (Intercept)     1 -1.64    0.0838  5.24e-14
#&amp;gt;  2 (Intercept)     2 -1.64    0.0764  8.68e- 3
#&amp;gt;  3 heathcliff      2  0.0469  0.0764  8.68e- 3
#&amp;gt;  4 linton          2  0.00120 0.0764  8.68e- 3
#&amp;gt;  5 (Intercept)     3 -1.64    0.0696  2.51e- 2
#&amp;gt;  6 heathcliff      3  0.0801  0.0696  2.51e- 2
#&amp;gt;  7 catherine       3  0.0269  0.0696  2.51e- 2
#&amp;gt;  8 linton          3  0.0413  0.0696  2.51e- 2
#&amp;gt;  9 (Intercept)     4 -1.64    0.0634  3.80e- 2
#&amp;gt; 10 heathcliff      4  0.107   0.0634  3.80e- 2
#&amp;gt; # ... with 26,009 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make predictions with &lt;code&gt;initial_fit&lt;/code&gt; anyway, and examine metrics like overall accuracy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict &amp;lt;- predict(initial_fit, book_test) %&amp;gt;% 
    bind_cols(predict(initial_fit, book_test, type = &amp;quot;prob&amp;quot;)) %&amp;gt;%
    bind_cols(book_test %&amp;gt;% select(author, line_index))

initial_predict
#&amp;gt; # A tibble: 11,779 x 5
#&amp;gt;    .pred_class     `.pred_Charlotte Bro~ `.pred_Emily Bron~ author    line_index
#&amp;gt;    &amp;lt;fct&amp;gt;                           &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;int&amp;gt;
#&amp;gt;  1 Charlotte Bron~                 0.844              0.156 Emily Br~          1
#&amp;gt;  2 Charlotte Bron~                 0.844              0.156 Emily Br~         13
#&amp;gt;  3 Charlotte Bron~                 0.844              0.156 Emily Br~         30
#&amp;gt;  4 Charlotte Bron~                 0.844              0.156 Emily Br~         31
#&amp;gt;  5 Charlotte Bron~                 0.844              0.156 Emily Br~         36
#&amp;gt;  6 Charlotte Bron~                 0.844              0.156 Emily Br~         56
#&amp;gt;  7 Charlotte Bron~                 0.844              0.156 Emily Br~         68
#&amp;gt;  8 Charlotte Bron~                 0.844              0.156 Emily Br~         75
#&amp;gt;  9 Charlotte Bron~                 0.844              0.156 Emily Br~         89
#&amp;gt; 10 Charlotte Bron~                 0.844              0.156 Emily Br~         96
#&amp;gt; # ... with 11,769 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How good is our initial model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict %&amp;gt;% 
  accuracy(truth = author, estimate = .pred_class)
#&amp;gt; # A tibble: 1 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.844&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nearly 84% of all predictions are right. This isn’t a very statisfactory result since “Charlotte Brontë” accounts for 81% of &lt;code&gt;author&lt;/code&gt;, making our model only slightly better than a classifier that would assngin all &lt;code&gt;author&lt;/code&gt; with “Charlotte Brontë” anyway.&lt;/p&gt;
&lt;div id=&#34;tuning-lambda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tuning lambda&lt;/h3&gt;
&lt;p&gt;We can figure out an appropriate penalty using resampling and tune the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_wf_tune &amp;lt;- book_wf %&amp;gt;%
  update_model(logistic_spec %&amp;gt;% set_args(penalty = tune()))

lambda_grid &amp;lt;- grid_regular(penalty(), levels = 100)
book_folds &amp;lt;- vfold_cv(book_train, v = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I build a set of 10 cross validations resamples, and set &lt;code&gt;levels = 100&lt;/code&gt; to try 100 choices of lambda ranging from 0 to 1.&lt;/p&gt;
&lt;p&gt;Then I tune the grid:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results &amp;lt;- logistic_wf_tune %&amp;gt;%    
  tune_grid(
    resamples = book_folds,
    grid = lambda_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is an &lt;code&gt;autoplot()&lt;/code&gt; method for the tuned results, but I instead plot two metrics versus lambda respectivcely by myself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  collect_metrics() %&amp;gt;% 
  mutate(lower_bound = mean - std_err,
         upper_bound = mean + std_err) %&amp;gt;%
  ggplot(aes(penalty, mean)) + 
  geom_line(aes(color = .metric), size = 1.5, show.legend = FALSE) + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) + 
  facet_wrap(~ .metric, nrow = 2, scales = &amp;quot;free&amp;quot;) + 
  labs(y = NULL,
       x = expression(lambda),
       title = &amp;quot;Performance metric of logistic regession versus differenct choices of L1 regularization&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, the two metrics both display a monotone decrease as lambda increases, but does not exhibit much change once lambda is greater than 0.1, which is essentailly random guess according to the author’s respective proportion of appearance in the data. This plot shows that the model is generally better at small penalty, suggesting that the majority of the predictors are fairly important to the model. We may lean towards larger penalty with slightly worse performance, bacause they lead to simpler models. It follows that we may want to choose lambda in top rows in the following data frame&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_models &amp;lt;- logistic_results %&amp;gt;% 
    show_best(&amp;quot;roc_auc&amp;quot;, n = 100) %&amp;gt;%
    arrange(desc(penalty)) %&amp;gt;% 
    filter(mean &amp;gt; 0.9)

top_models
#&amp;gt; # A tibble: 76 x 6
#&amp;gt;     penalty .metric .estimator  mean     n std_err
#&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
#&amp;gt;  1 0.00376  roc_auc binary     0.906    10 0.00215
#&amp;gt;  2 0.00298  roc_auc binary     0.908    10 0.00219
#&amp;gt;  3 0.00236  roc_auc binary     0.908    10 0.00222
#&amp;gt;  4 0.00187  roc_auc binary     0.908    10 0.00227
#&amp;gt;  5 0.00148  roc_auc binary     0.909    10 0.00209
#&amp;gt;  6 0.00118  roc_auc binary     0.910    10 0.00207
#&amp;gt;  7 0.000933 roc_auc binary     0.910    10 0.00202
#&amp;gt;  8 0.000739 roc_auc binary     0.910    10 0.00205
#&amp;gt;  9 0.000586 roc_auc binary     0.910    10 0.00209
#&amp;gt; 10 0.000464 roc_auc binary     0.910    10 0.00210
#&amp;gt; # ... with 66 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;select_best()&lt;/code&gt; with return the 9th row with &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.000586\)&lt;/span&gt; for its highest performance on &lt;code&gt;roc_auc&lt;/code&gt;. But I’ll stick to the parsimonious principle and pick &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.00376\)&lt;/span&gt; at the cost of a fall in &lt;code&gt;roc_auc&lt;/code&gt; by 0.005 and in &lt;code&gt;accuracy&lt;/code&gt; by 0.001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  select_best(metric = &amp;quot;roc_auc&amp;quot;)
#&amp;gt; # A tibble: 1 x 1
#&amp;gt;    penalty
#&amp;gt;      &amp;lt;dbl&amp;gt;
#&amp;gt; 1 0.000586

book_wf_final &amp;lt;- finalize_workflow(logistic_wf_tune,
                                  parameters = top_models %&amp;gt;% slice(1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the model specification in the workflow is filled with the picked lambda:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_wf_final %&amp;gt;% pull_workflow_spec()
#&amp;gt; Logistic Regression Model Specification (classification)
#&amp;gt; 
#&amp;gt; Main Arguments:
#&amp;gt;   penalty = 0.00376493580679246
#&amp;gt;   mixture = 1
#&amp;gt; 
#&amp;gt; Computational engine: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next thing is to fit the best model with the training set, and evaluate against the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final &amp;lt;- last_fit(book_wf_final, split = book_split)

logistic_final %&amp;gt;% 
  collect_metrics()
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.938
#&amp;gt; 2 roc_auc  binary         0.904&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;% 
  collect_predictions() %&amp;gt;%
  roc_curve(truth = author, `.pred_Emily Brontë`) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;roc_curve.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The accuracy of our logisitc model rises by a rough 9% to 93.8%, with &lt;code&gt;roc_auc&lt;/code&gt; being nearly 0.904. This is pretty good!&lt;/p&gt;
&lt;p&gt;There is also the confusion matrix to check. The model does well in identifying Charlotte Brontë (low false positive rate, high sensitivity), yet suffers relatively high false negative rate (mistakenly identify 39% of Emily Brontë as Charlotte Brontë, aka low specificity). In part, this is due to class imbalance (four out of five books were written by Charlotte).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;%
  collect_predictions() %&amp;gt;%
  conf_mat(truth = author, estimate = .pred_class) 
#&amp;gt;                   Truth
#&amp;gt; Prediction         Charlotte Brontë Emily Brontë
#&amp;gt;   Charlotte Brontë             9921          726
#&amp;gt;   Emily Brontë                    1         1131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To examine the effect of predictors, I agian use &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;pull_workflow&lt;/code&gt; to extract model fit. Variable importance plots implemented in the &lt;a href=&#34;https://koalaverse.github.io/vip/index.html&#34;&gt;vip&lt;/a&gt; package provides an intuitive way to visualize importance of predictors in this scenario, using the absolute value of the t-statistic as a measure of VI.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vip)

logistic_vi &amp;lt;- book_wf_final %&amp;gt;% 
  fit(book_train) %&amp;gt;% 
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = top_models[1, ]$penalty) %&amp;gt;% 
  group_by(Sign) %&amp;gt;% 
  top_n(30, wt = abs(Importance)) %&amp;gt;%
  ungroup() %&amp;gt;% 
  mutate(Sign = if_else(Sign == &amp;quot;POS&amp;quot;, 
                        &amp;quot;More Emily Brontë&amp;quot;, 
                        &amp;quot;More Charlotte Brontë&amp;quot;))

logistic_vi %&amp;gt;% 
  ggplot(aes(y = reorder_within(Variable, abs(Importance), Sign),
             x = Importance)) + 
  geom_col(aes(fill = Sign), 
           show.legend = FALSE, alpha = 0.6) +
  scale_y_reordered() + 
  facet_wrap(~ Sign, nrow = 1, scales = &amp;quot;free&amp;quot;) + 
  labs(title = &amp;quot;How word usage classifies Brontë sisters&amp;quot;,
       x = NULL,
       y = NULL) + 
  theme(axis.text = element_text(size = 18),
        plot.title = element_text(size = 24),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-25&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-25-1.png&#34; alt=&#34;Variable importance plot for penalized logistic regression&#34; width=&#34;1056&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Variable importance plot for penalized logistic regression
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Is it cheating to use names of a character to classify authors? Perhaps I should consider include more books and remove names for text classification next time.&lt;/p&gt;
&lt;p&gt;Note that variale importance in the left panel is generally smaller than the right, this corresponds to what we find in the &lt;a href=&#34;#comparing-word-frequency&#34;&gt;word frequency&lt;/a&gt; plot that Emily Brontë has more and stronger characteristic words.&lt;/p&gt;
&lt;p&gt;In conclusion, the model with the best lambda seems quite powerful in distinguishing these two authors. I look forward to build a multinomial classification model in &lt;code&gt;tidymodels&lt;/code&gt; to include Anne Brontë some other time!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Who is Carrying There Team in OWL?</title>
      <link>/post/2020-04-25-who-is-carrying-there-team-in-owl/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-04-25-who-is-carrying-there-team-in-owl/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-fleta-deadlift&#34;&gt;The Fleta deadlift&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#extend-the-stat-to-tanks-and-supports&#34;&gt;Extend the stat to tanks and supports&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;the-fleta-deadlift&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Fleta deadlift&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://overwatchleague.com/en-us/statslab&#34;&gt;Overwatch Stats Lab&lt;/a&gt; is a treasure trove holding vast digital riches for people who want to observe the Overwatch League in a more detailed manner. One of my favourite statistic is the &lt;strong&gt;Fleta deadlift&lt;/strong&gt;, calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Fleta deadlifts} = \frac{\text{final blows of a player}}{\text{final blows of the whole team}}
\]&lt;/span&gt;
It comes as no surprise that the index was named after Fleta, the current Shanghai dragons DPS player, who is known for his former amazing performance in a less talented team before he joined Seoul Dyanasty and entered OWL. If you are not familiar with esports or overwatch, I have some equivelent “Fleta deadlifted his team” examples in the NBA context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Michael Jordan in the 1998 final&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Kobe Bryant (I miss him) in season 2006-2007, or perhaps half time in his career&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Lebron Jmaes in the 2015 final&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The player section on Overwatch Stats Lab even established a special tab for this stat, showing records with Fleta deadlift &amp;gt; 50% (see the last tab below):&lt;br /&gt;
&lt;iframe src=&#34;https://overwatchleague.com/en-us/statslab-players&#34; width=&#34;768&#34; height=&#34;400px&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;It should be obvious though, that Fleta deadlifts are more suitable for describe performaces of DPS players, and less applicable to the other two positions, tanks and supports. Since final blows may not be a good measure for a player’s performance if his main responsibility indicates otherwise.&lt;/p&gt;
&lt;p&gt;I used &lt;code&gt;load_data.R&lt;/code&gt; to laod in data and do soem preprocessing. You can browse it at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(vroom)

df &amp;lt;- vroom(&amp;quot;D:/RProjects/data/overwatch/fleta_deadlifts.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;extend-the-stat-to-tanks-and-supports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extend the stat to tanks and supports&lt;/h2&gt;
&lt;p&gt;However, since OWL mandates the 2-2-2 composition only after the third stage in season 2019. A player can belong to different positions even in the same map or match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dpss &amp;lt;- c(&amp;quot;Hanzo&amp;quot;, &amp;quot;Junkrat&amp;quot;, &amp;quot;Tracer&amp;quot;, &amp;quot;Soldier: 76&amp;quot;, &amp;quot;Widowmaker&amp;quot;, &amp;quot;McCree&amp;quot;, &amp;quot;Pharah&amp;quot;, &amp;quot;Genji&amp;quot;, &amp;quot;Sombra&amp;quot;, &amp;quot;Reaper&amp;quot;, &amp;quot;Doomfist&amp;quot;, &amp;quot;Bastion&amp;quot;, &amp;quot;Mei&amp;quot;, &amp;quot;Torbjörn&amp;quot;, &amp;quot;Symmetra&amp;quot;, &amp;quot;Ashe&amp;quot;)
tanks &amp;lt;- c(&amp;quot;Orisa&amp;quot;, &amp;quot;Winston&amp;quot;, &amp;quot;Roadhog&amp;quot;, &amp;quot;D.Va&amp;quot;, &amp;quot;Zarya&amp;quot;, &amp;quot;Reinhardt&amp;quot;, &amp;quot;Wrecking Ball&amp;quot;)
supports &amp;lt;- c(&amp;quot;Lúcio&amp;quot;, &amp;quot;Mercy&amp;quot;, &amp;quot;Zenyatta&amp;quot;, &amp;quot;Brigitte&amp;quot;, &amp;quot;Moira&amp;quot;, &amp;quot;Ana&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;Baptiste&amp;quot;)

dps_records &amp;lt;- df %&amp;gt;% filter(hero %in% dpss)
tank_records &amp;lt;- df %&amp;gt;% filter(hero %in% tanks)
support_records &amp;lt;- df %&amp;gt;% filter(hero %in% supports)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For dps players, I will stick to the official measure of a Fleta deadlift: involved in more than 50% percent of team total final blows in a match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dps_deadlifts &amp;lt;- dps_records %&amp;gt;% 
  filter(stat_name == &amp;quot;Final Blows&amp;quot;) %&amp;gt;% 
  mutate(final_blows = stat_amount) %&amp;gt;% 
  select(-stat_name, -stat_amount) %&amp;gt;%
  add_count(date, team, wt = final_blows, name = &amp;quot;team_final_blows&amp;quot;) %&amp;gt;%
  filter(team_final_blows &amp;gt; 10) %&amp;gt;% 
  group_by(date, player, team_final_blows) %&amp;gt;%
  summarize(player_final_blows = sum(final_blows)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(rate = player_final_blows / team_final_blows) %&amp;gt;%
  filter(rate &amp;gt; 0.5) 

dps_deadlifts
#&amp;gt; # A tibble: 827 x 5
#&amp;gt;    date       player   team_final_blows player_final_blows  rate
#&amp;gt;    &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt;  1 2018-01-11 babybay                95                 48 0.505
#&amp;gt;  2 2018-01-11 carpe                  44                 25 0.568
#&amp;gt;  3 2018-01-11 EFFECT                139                 70 0.504
#&amp;gt;  4 2018-01-11 Fleta                 131                 68 0.519
#&amp;gt;  5 2018-01-11 Jake                   35                 20 0.571
#&amp;gt;  6 2018-01-11 Profit                 90                 58 0.644
#&amp;gt;  7 2018-01-11 TviQ                   80                 42 0.525
#&amp;gt;  8 2018-01-11 Undead                 35                 18 0.514
#&amp;gt;  9 2018-01-12 Jake                   95                 49 0.516
#&amp;gt; 10 2018-01-13 Birdring              100                 52 0.52 
#&amp;gt; # ... with 817 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the London Stage Database</title>
      <link>/post/exploring-the-london-stage-database/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-london-stage-database/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# setup
library(tidyverse)
library(jsonlite)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the data
london_stage &amp;lt;- fromJSON(&amp;quot;D:/RProjects/data/LondonStageFull.json&amp;quot;) %&amp;gt;% 
  as_tibble()

london_stage
#&amp;gt; # A tibble: 52,617 x 13
#&amp;gt;    EventId EventDate TheatreCode Season Volume Hathi CommentC TheatreId Phase2
#&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; 
#&amp;gt;  1 0       16591029  city        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;The &amp;lt;i~ 63        *p165~
#&amp;gt;  2 1       16591100  mt          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;On 23 ~ 206       *p165~
#&amp;gt;  3 2       16591218  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;Repres~ 1         *p165~
#&amp;gt;  4 3       16600200  mt          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;6 Feb.~ 206       *p166~
#&amp;gt;  5 4       16600204  cockpit     1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;$Thoma~ 73        *p166~
#&amp;gt;  6 5       16600328  dh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;At &amp;lt;i&amp;gt;~ 90        *p166~
#&amp;gt;  7 6       16600406  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;quot;       1         *p166~
#&amp;gt;  8 7       16600412  vh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;Editio~ 319       *p166~
#&amp;gt;  9 8       16600413  fh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;lt;i&amp;gt;The~ 116       *p166~
#&amp;gt; 10 9       16600416  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;quot;       1         *p166~
#&amp;gt; # ... with 52,607 more rows, and 4 more variables: Phase1 &amp;lt;chr&amp;gt;,
#&amp;gt; #   CommentCClean &amp;lt;chr&amp;gt;, BookPDF &amp;lt;chr&amp;gt;, Performances &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;json&lt;/code&gt; file can be downloaded at &lt;a href=&#34;https://londonstagedatabase.usu.edu/downloads/LondonStageJSON.zip&#34; class=&#34;uri&#34;&gt;https://londonstagedatabase.usu.edu/downloads/LondonStageJSON.zip&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing COVID-19 Publications</title>
      <link>/post/analyzing-covid-19-publications/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/analyzing-covid-19-publications/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-words-and-keywords-extraction&#34;&gt;Common words and keywords extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-a-lda-topic-model&#34;&gt;Fit a LDA topic model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-network-of-paired-words&#34;&gt;A network of paired words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post, I will be performing a simple text analysis on the abstract of publications on the coronavirus disease (COVID-19), courtesy of &lt;a href=&#34;https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov&#34;&gt;WHO&lt;/a&gt;. We begin by steps of data preprocessing.&lt;/p&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data preprocessing&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw &amp;lt;- read_csv(&amp;quot;D:/RProjects/data/covid-research.csv&amp;quot;) %&amp;gt;% 
  janitor::clean_names() 

glimpse(raw)
#&amp;gt; Rows: 4,190
#&amp;gt; Columns: 16
#&amp;gt; $ title            &amp;lt;chr&amp;gt; &amp;quot;SARS-CoV-2 is not detectable in the vaginal fluid...
#&amp;gt; $ authors          &amp;lt;chr&amp;gt; &amp;quot;Qiu, Lin; Liu, Xia; Xiao, Meng; Xie, Jing; Cao, W...
#&amp;gt; $ abstract         &amp;lt;chr&amp;gt; &amp;quot;Background Severe acute respiratory syndrome coro...
#&amp;gt; $ published_year   &amp;lt;dbl&amp;gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20...
#&amp;gt; $ published_month  &amp;lt;lgl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&amp;gt; $ journal          &amp;lt;chr&amp;gt; &amp;quot;Clinical Infectious Diseases&amp;quot;, &amp;quot;International Jou...
#&amp;gt; $ volume           &amp;lt;chr&amp;gt; NA, &amp;quot;17&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
#&amp;gt; $ issue            &amp;lt;chr&amp;gt; NA, &amp;quot;7&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
#&amp;gt; $ pages            &amp;lt;chr&amp;gt; NA, &amp;quot;2430-2430&amp;quot;, &amp;quot;112275-112275&amp;quot;, NA, &amp;quot;1-4&amp;quot;, NA, N...
#&amp;gt; $ accession_number &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, &amp;quot;32229574&amp;quot;, NA, NA, &amp;quot;3...
#&amp;gt; $ doi              &amp;lt;chr&amp;gt; &amp;quot;10.1093/cid/ciaa375&amp;quot;, &amp;quot;10.3390/IJERPH17072430&amp;quot;, &amp;quot;...
#&amp;gt; $ ref              &amp;lt;dbl&amp;gt; 26513, 26499, 26744, 26447, 27114, 26388, 26696, 2...
#&amp;gt; $ covidence_number &amp;lt;chr&amp;gt; &amp;quot;#27487&amp;quot;, &amp;quot;#27413&amp;quot;, &amp;quot;#27869&amp;quot;, &amp;quot;#27815&amp;quot;, &amp;quot;#27905&amp;quot;, ...
#&amp;gt; $ study            &amp;lt;chr&amp;gt; &amp;quot;Qiu 2020&amp;quot;, &amp;quot;Pulido 2020&amp;quot;, &amp;quot;Pillaiyar 2020&amp;quot;, &amp;quot;Piau...
#&amp;gt; $ notes            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&amp;gt; $ tags             &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For simplicity I ignore many of the vairables (mostly for identification) and rows with missing values on &lt;code&gt;abstract&lt;/code&gt;. I was a little disappointied to find out that &lt;code&gt;published_month&lt;/code&gt; are all missing, otherwise we may see a trend of some sort on research topics there. One remaining problem is that some of the papers are not written in English.
The &lt;code&gt;detect_language()&lt;/code&gt; from the &lt;a href=&#34;https://github.com/ropensci/cld3&#34;&gt;&lt;code&gt;cdl3&lt;/code&gt;&lt;/a&gt; can detect language forms at a fairly high success rate. It’s a R wrapper for Google’s Compact Language Detector 3, which is a neural network model for language identification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cld3)
raw %&amp;gt;% 
  count(language = detect_language(abstract), sort = TRUE)
#&amp;gt; # A tibble: 13 x 2
#&amp;gt;    language     n
#&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
#&amp;gt;  1 en        2482
#&amp;gt;  2 ig        1618
#&amp;gt;  3 es          26
#&amp;gt;  4 pt          18
#&amp;gt;  5 de          11
#&amp;gt;  6 ru          10
#&amp;gt;  7 fr           7
#&amp;gt;  8 zh           7
#&amp;gt;  9 &amp;lt;NA&amp;gt;         5
#&amp;gt; 10 fi           2
#&amp;gt; 11 ja           2
#&amp;gt; 12 cs           1
#&amp;gt; 13 it           1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this post I keep only the rows that were classified as “en”. Also, as illuatrated in &lt;a href=&#34;https://www.tidytextmining.com&#34;&gt;Text Mining with R&lt;/a&gt;, text analysis commonly requires preprocessing steps like tokenizing, eliminating stop words and word stemming. I added custom keywords and did some maunual transformation to make up for misclassifications by &lt;code&gt;detect_language&lt;/code&gt;, but there will still be non-English words, though. My transformations (say, both “covid” and “19” now become “covid19”) will certainly induce errors, but there is no better workaround I could think of now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(SnowballC)

words &amp;lt;- raw %&amp;gt;% 
  filter(detect_language(abstract) == &amp;quot;en&amp;quot;) %&amp;gt;% 
  unnest_tokens(word, abstract) %&amp;gt;% 
  mutate(word = wordStem(word)) %&amp;gt;%
  mutate(word = case_when(
    word == &amp;quot;19&amp;quot; ~ &amp;quot;covid19&amp;quot;,
    word == &amp;quot;covid&amp;quot; ~ &amp;quot;covid19&amp;quot;,
    word == &amp;quot;coronaviru&amp;quot; ~ &amp;quot;coronavirus&amp;quot;,
    word == &amp;quot;viru&amp;quot; ~ &amp;quot;virus&amp;quot;,
    word == &amp;quot;epidem&amp;quot; ~ &amp;quot;epidemic&amp;quot;,
    word == &amp;quot;studi&amp;quot; ~ &amp;quot;study&amp;quot;,
    word == &amp;quot;respiratori&amp;quot; ~ &amp;quot;respiratory&amp;quot;,
    word == &amp;quot;emetin&amp;quot; ~ &amp;quot;emetine&amp;quot;,
    word == &amp;quot;acut&amp;quot; ~ &amp;quot;acute&amp;quot;,
    word == &amp;quot;sever&amp;quot; ~ &amp;quot;severe&amp;quot;,
    word == &amp;quot;manag&amp;quot; ~ &amp;quot;manage&amp;quot;,
    word == &amp;quot;hospit&amp;quot; ~ &amp;quot;hospital&amp;quot;,
    word == &amp;quot;diseas&amp;quot; ~ &amp;quot;disease&amp;quot;,
    word == &amp;quot;deceas&amp;quot; ~ &amp;quot;dicease&amp;quot;,
    word == &amp;quot;caus&amp;quot; ~ &amp;quot;cause&amp;quot;,
    word == &amp;quot;emerg&amp;quot; ~ &amp;quot;emerge&amp;quot;,
    word == &amp;quot;includ&amp;quot; ~ &amp;quot;include&amp;quot;, 
    word == &amp;quot;dai&amp;quot; ~ &amp;quot;wet nurse&amp;quot;,
    word == &amp;quot;ncovid&amp;quot; ~ &amp;quot;ncov&amp;quot;,
    word == &amp;quot;countri&amp;quot; ~ &amp;quot;country&amp;quot;,
    word == &amp;quot;provid&amp;quot; ~ &amp;quot;provide&amp;quot;,
    word == &amp;quot;peopl&amp;quot; ~ &amp;quot;people&amp;quot;,
    TRUE ~ word
  )) %&amp;gt;% 
  anti_join(stop_words %&amp;gt;% 
            add_row(word = c( &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;,  &amp;quot;dub&amp;quot;, &amp;quot;thi&amp;quot;, &amp;quot;ha&amp;quot;, &amp;quot;wa&amp;quot;, &amp;quot;检查&amp;quot;, &amp;quot;cd&amp;quot;, &amp;quot;gt&amp;quot;,
                              &amp;quot;lt&amp;quot;, &amp;quot;tnt&amp;quot;, &amp;quot;thei&amp;quot;), 
                    lexicon = &amp;quot;custom&amp;quot;)) %&amp;gt;% 
  filter(!(str_detect(word, &amp;quot;^\\d+$&amp;quot;) | str_detect(word, &amp;quot;^\\d+\\w$&amp;quot;)))


words
#&amp;gt; # A tibble: 245,052 x 16
#&amp;gt;    title authors published_year published_month journal volume issue pages
#&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;           &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
#&amp;gt;  1 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  2 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  3 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  4 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  5 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  6 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  7 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  8 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt;  9 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt; 10 SARS~ Qiu, L~           2020 NA              Clinic~ &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; 
#&amp;gt; # ... with 245,042 more rows, and 8 more variables: accession_number &amp;lt;chr&amp;gt;,
#&amp;gt; #   doi &amp;lt;chr&amp;gt;, ref &amp;lt;dbl&amp;gt;, covidence_number &amp;lt;chr&amp;gt;, study &amp;lt;chr&amp;gt;, notes &amp;lt;chr&amp;gt;,
#&amp;gt; #   tags &amp;lt;chr&amp;gt;, word &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;common-words-and-keywords-extraction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common words and keywords extraction&lt;/h1&gt;
&lt;p&gt;An immediate question is, what are the most common words among all these publications?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words %&amp;gt;% 
  count(word, sort = TRUE) %&amp;gt;%
  top_n(50) %&amp;gt;%
  ggplot(aes(y = fct_reorder(word, n),
             x = n)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0.01, 0)) + 
  labs(y = NULL,
       x = &amp;quot;# of words&amp;quot;,
       title = &amp;quot;Top 50 common words in COVID-19 publications&amp;quot;) +
  theme(text = element_text(size = 18),
        plot.title.position = &amp;quot;plot&amp;quot;,
        plot.title = element_text(size = 35, face = &amp;quot;bold&amp;quot;),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’m also interested in paper-specific properties, namely their keywords, what topics distinguish them from others? In comparison to the commonly used algorithm tf-idf, I prefer using weighted log odds proposed by &lt;span class=&#34;citation&#34;&gt;Monroe, Colaresi, and Quinn (&lt;a href=&#34;#ref-monroe_colaresi_quinn&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, which a standardized metric from a complete statistical model. It is also implemented in the R package &lt;a href=&#34;https://github.com/juliasilge/tidylo&#34;&gt;&lt;code&gt;tidylo&lt;/code&gt;&lt;/a&gt;&lt;span class=&#34;citation&#34;&gt;(Schnoebelen and Silge &lt;a href=&#34;#ref-tidylo&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. The reason is that tf-idf cannot extract the varying use trend of common words, if a word appears in every research paper, then its inverse document frequency will be zero. For weighted log odds this is not the case, even if all researched mentioned this word it can still differentiate those who used it a lot more often from those who used less. This could be essential when we are trying to find an emphasis on which researchers place as our understanding of the virus advances. Sadly I have no access to the exact date of the publication, so I will just display words with topest score and their corresponding publications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidylo)

words %&amp;gt;%
  count(title, word) %&amp;gt;% 
  bind_log_odds(set = title, feature = word, n = n) %&amp;gt;%
  top_n(20) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log_odds&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A high ATP concentration enhances the cooperative translocation of the SARS coronavirus helicase nsP13 in the unwinding of duplex RNA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;duplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.525439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A survey on awareness of digestive system injury caused by corona virus disease 2019 in gastroenterologists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;gastroenterologist&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.230132&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;hljby&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.917622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pedv&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.760966&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Clinical characteristics of 113 deceased patients with coronavirus disease 2019: retrospective study&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;dicease&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.426271&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Effect of TLR agonist on infections bronchitis virus replication and cytokine expression in embryonated chicken eggs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;tlr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.608637&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Emetine, Ipecac, Ipecac Alkaloids and Analogues as Potential Antiviral Agents for Coronaviruses&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;emetine&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.577769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Experimental Treatment with Favipiravir for COVID-19: An Open-Label Control Study&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;fpv&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.576505&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Expert consensus on Pulmonary Function Testing during the epidemic of Corona Virus Disease 2019&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;检查&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.749430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Facemask shortage and the novel coronavirus disease (COVID-19) outbreak: Reflections on public health measures&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;facemask&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.637175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Frequency and Distribution of Chest Radiographic Findings in COVID-19 Positive Patients&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cxr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.607049&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Genetic, antigenic and pathogenic characterization of avian coronaviruses isolated from pheasants (Phasianus colchicus) in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;phcov&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.577311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Influence of trust on two different risk perceptions as an affective and cognitive dimension during Middle East respiratory syndrome coronavirus (MERS-CoV) outbreak in South Korea: serial cross-sectional surveys&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;percept&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.250063&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Microneedle array delivered recombinant coronavirus vaccines: Immunogenicity and rapid translational development&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mna&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.871558&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Pregnant women with new coronavirus infection: a clinical characteristics and placental pathological analysis of three cases&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;placenta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.762505&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Responding to the COVID-19 pandemic in complex humanitarian crises&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;humanitarian&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.681217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Serial interval in determining the estimation of reproduction number of the novel coronavirus disease (COVID-19) during the early outbreak | Journal of Travel Medicine | Oxford Academic&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;si&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.108847&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Short-term effects of ambient PM1 and PM2.5 air pollution on hospital admission for respiratory diseases: Case-crossover evidence from Shenzhen, China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pm1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.918315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The experience of high-flow nasal cannula in hospitalized patients with 2019 novel coronavirus-infected pneumonia in two hospitals of Chongqing, China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;hfnc&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.706804&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Thoughts and suggestions on modern construction of disease prevention and control system&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;modern&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.138237&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Many words listed here are acronyms and terms in biology, chemistry and medicine. For example, “hljby” is a type of &lt;strong&gt;p&lt;/strong&gt;orcine &lt;strong&gt;e&lt;/strong&gt;pidemic &lt;strong&gt;d&lt;/strong&gt;iarrhoea &lt;strong&gt;v&lt;/strong&gt;irus which “pedv” stands for, “fpv” means &lt;strong&gt;F&lt;/strong&gt;avi&lt;strong&gt;p&lt;/strong&gt;ira&lt;strong&gt;v&lt;/strong&gt;ir (a type of drug), and “tlr” represents &lt;strong&gt;T&lt;/strong&gt;oll-&lt;strong&gt;l&lt;/strong&gt;ike &lt;strong&gt;r&lt;/strong&gt;eceptors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-lda-topic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fit a LDA topic model&lt;/h1&gt;
&lt;p&gt;Let’s then fit a 5-topic LDA topic model, before that we should convert the data frame to a docuemnt term matrix using &lt;code&gt;cast_dtm&lt;/code&gt;. There are various implementations of this kind of model, here I use &lt;code&gt;stm::stm&lt;/code&gt;. The choice of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; (number of topics) here is somewhat arbitrary here, but &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; had a great &lt;a href=&#34;https://juliasilge.com/blog/evaluating-stm/&#34;&gt;post&lt;/a&gt; about it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfm &amp;lt;- cast_dfm(words %&amp;gt;% count(title, word),
                term = word,
                document = title,
                value = n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;dfm&lt;/code&gt; is a document-term matrix with 2415 documents and 13383 features.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stm)
topic_model &amp;lt;- stm(dfm, K = 5, init.type = &amp;quot;LDA&amp;quot;, verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Topic-term probability distributions are accessed by &lt;code&gt;tidy()&lt;/code&gt;, this gives a glance of the underlying meaning of these topics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# topic-term distribution
tidy(topic_model) %&amp;gt;% 
  group_by(topic) %&amp;gt;% 
  top_n(10) %&amp;gt;% 
  ungroup() %&amp;gt;%
  mutate(topic = factor(topic) %&amp;gt;% str_c(&amp;quot;topic&amp;quot;, .)) %&amp;gt;% 
  ggplot(aes(y = reorder_within(term, beta, topic),
         x = beta,
         fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  scale_y_reordered() + 
  facet_wrap(~ topic, scales = &amp;quot;free_y&amp;quot;, nrow = 3) + 
  labs(y = NULL,
       x = &amp;quot;Docuemtn-term probabilities&amp;quot;,
       title = &amp;quot;A 6-topic LDA model&amp;quot;) + 
  theme(text = element_text(size = 18),
        plot.title = element_text(size = 30, face = &amp;quot;bold&amp;quot;),
        strip.text = element_text(size = 25, hjust = 0.05),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to interpret these topics without domain knwoledge. But it seems to me that topic3 is related to clinical findings, topic4 to china and wuhan, the epicenter of covid19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-network-of-paired-words&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A network of paired words&lt;/h1&gt;
&lt;p&gt;Another question of interest is the relationship between words: what group of words tend to appear together? I look at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Phi_coefficient&#34;&gt;phi coefficient&lt;/a&gt;, which is essentailly &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistc in a contingency table applied to categorical variables.&lt;/p&gt;
&lt;p&gt;As each abstract is a natual unit of measure, a pair of words that both appear in the same abstract are seen as “appearing together”. We could compute &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; based on pairwise counts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(widyr)

word_cors &amp;lt;- words %&amp;gt;% 
  add_count(word) %&amp;gt;% 
  filter(n &amp;gt; 20) %&amp;gt;%
  select(-n) %&amp;gt;%
  pairwise_cor(item = word, feature = title, sort = TRUE)

word_cors
#&amp;gt; # A tibble: 2,385,480 x 3
#&amp;gt;    item1        item2        correlation
#&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
#&amp;gt;  1 arabia       saudi              1    
#&amp;gt;  2 saudi        arabia             1    
#&amp;gt;  3 pave         crazi              0.970
#&amp;gt;  4 crazi        pave               0.970
#&amp;gt;  5 kong         hong               0.935
#&amp;gt;  6 hong         kong               0.935
#&amp;gt;  7 dehydrogenas lactat             0.931
#&amp;gt;  8 lactat       dehydrogenas       0.931
#&amp;gt;  9 reserv       copyright          0.928
#&amp;gt; 10 copyright    reserv             0.928
#&amp;gt; # ... with 2,385,470 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A network visualization of word correlation is a good idea:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
library(tidygraph)

word_cors %&amp;gt;% 
  filter(correlation &amp;gt; 0.4) %&amp;gt;% 
  as_tbl_graph() %&amp;gt;% 
  ggraph(layout = &amp;quot;fr&amp;quot;) + 
  geom_edge_link(aes(alpha = correlation), show.legend = FALSE) + 
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 6.5) + 
  geom_node_text(aes(label = name), repel = TRUE, size = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, there are still some non-English words that stemming and adding stopwrods cannot handle… Nonetheless, we are be able to identify some of the clusters revovling around infant infection (infant, pregnant, newborn, mother), pathology (angiotensin, protein, receptor), symptoms (lung, thicken, lesion), etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-monroe_colaresi_quinn&#34;&gt;
&lt;p&gt;Monroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008. “Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.” &lt;em&gt;Political Analysis&lt;/em&gt; 16 (4): 372–403. &lt;a href=&#34;https://doi.org/10.1093/pan/mpn018&#34;&gt;https://doi.org/10.1093/pan/mpn018&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tidylo&#34;&gt;
&lt;p&gt;Schnoebelen, Tyler, and Julia Silge. 2020. &lt;em&gt;Tidylo: Tidy Log Odds Ratio Weighted by Uninformative Prior&lt;/em&gt;. &lt;a href=&#34;http://github.com/juliasilge/tidylo&#34;&gt;http://github.com/juliasilge/tidylo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
