<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis | Qiushi Yan</title>
    <link>/categories/data-analysis/</link>
      <atom:link href="/categories/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Qiushi Yan ¬© 2020</copyright><lastBuildDate>Sat, 02 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Data Analysis</title>
      <link>/categories/data-analysis/</link>
    </image>
    
    <item>
      <title>Text Classfication with Penalized Logistic Regression</title>
      <link>/post/text-classification-logistic/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/text-classification-logistic/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-word-frequency&#34;&gt;Comparing word frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling&#34;&gt;Modeling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#train-a-penalized-logistic-regression-model&#34;&gt;Train a penalized logistic regression model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tuning-lambda&#34;&gt;Tuning lambda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post I aim to train a text classification model with penalized logistic regression using the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt; framework. Data are from 5 books and downloaded via the &lt;a href=&#34;https://docs.ropensci.org/gutenbergr/&#34;&gt;&lt;code&gt;gutenbergr&lt;/code&gt;&lt;/a&gt; package, written by either Emily Bront√´ or Charlotte Bront√´. And the goal is to predict the author of a line, in other words the probability of line being written by one sister instead of another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books &amp;lt;- gutenberg_works() %&amp;gt;% 
  filter(str_detect(author, &amp;quot;Bront√´, Emily|Bront√´, Charlotte&amp;quot;)) %&amp;gt;% 
  gutenberg_download(meta_fields = c(&amp;quot;title&amp;quot;, &amp;quot;author&amp;quot;)) %&amp;gt;% 
  transmute(title,
            author = if_else(author == &amp;quot;Bront√´, Emily&amp;quot;, 
                             &amp;quot;Emily Bront√´&amp;quot;, 
                             &amp;quot;Charlotte Bront√´&amp;quot;) %&amp;gt;% factor(),
            line_index = row_number(),
            text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;books&lt;/code&gt; is at line level&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books
#&amp;gt; # A tibble: 88,989 x 4
#&amp;gt;    title        author     line_index text                                      
#&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                     
#&amp;gt;  1 Wuthering H~ Emily Bro~          1 &amp;quot;WUTHERING HEIGHTS&amp;quot;                       
#&amp;gt;  2 Wuthering H~ Emily Bro~          2 &amp;quot;&amp;quot;                                        
#&amp;gt;  3 Wuthering H~ Emily Bro~          3 &amp;quot;&amp;quot;                                        
#&amp;gt;  4 Wuthering H~ Emily Bro~          4 &amp;quot;CHAPTER I&amp;quot;                               
#&amp;gt;  5 Wuthering H~ Emily Bro~          5 &amp;quot;&amp;quot;                                        
#&amp;gt;  6 Wuthering H~ Emily Bro~          6 &amp;quot;&amp;quot;                                        
#&amp;gt;  7 Wuthering H~ Emily Bro~          7 &amp;quot;1801.--I have just returned from a visit~
#&amp;gt;  8 Wuthering H~ Emily Bro~          8 &amp;quot;neighbour that I shall be troubled with.~
#&amp;gt;  9 Wuthering H~ Emily Bro~          9 &amp;quot;country!  In all England, I do not belie~
#&amp;gt; 10 Wuthering H~ Emily Bro~         10 &amp;quot;situation so completely removed from the~
#&amp;gt; # ... with 88,979 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain tidy text structure illustrated in &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R&lt;/a&gt;, I use &lt;code&gt;unnest_tokens()&lt;/code&gt; to perform tokenization and remove all the stop words. I also removed characters like &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&#39;s&lt;/code&gt;, &lt;code&gt;&#39;&lt;/code&gt; and whitespaces to return valid column names after widening. But it turns out this served as some sort of stemming too! (heathcliff‚Äôs becomes heathcliff). Then low frequency words (whose frequency is less than 0.05% of an author‚Äôs total word counts) are removed. The cutoff may be a little too high if you plot that histogram, but I really need this to save computation efforts on my laptop üòÖ.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_books &amp;lt;- books %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;% 
  filter(!str_detect(word, &amp;quot;^\\d+$&amp;quot;)) %&amp;gt;% 
  mutate(word = str_remove_all(word, &amp;quot;_|&amp;#39;s|&amp;#39;|\\s&amp;quot;))
  
total_words &amp;lt;- clean_books %&amp;gt;%
  count(author, name = &amp;quot;total&amp;quot;)

tidy_books &amp;lt;- clean_books %&amp;gt;%
  left_join(total_words) %&amp;gt;% 
  group_by(author, total, word) %&amp;gt;%
  filter((n() / total) &amp;gt; 0.0005) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-word-frequency&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing word frequency&lt;/h1&gt;
&lt;p&gt;Before building an actual predictive model, let‚Äôs do some EDA to see different tendency to use a particular word! This will also shed light on what we would expect from the text classification. Now, we will compare word frequency (proportion) between the two sisters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  group_by(author, total) %&amp;gt;%
  count(word) %&amp;gt;% 
  mutate(prop = n / total) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(-total, -n) %&amp;gt;%
  pivot_wider(names_from  = author, values_from = prop,
              values_fill = list(prop = 0)) %&amp;gt;% 
  ggplot(aes(x = `Charlotte Bront√´`, y = `Emily Bront√´`, 
             color = abs(`Emily Bront√´` -  `Charlotte Bront√´`))) + 
  geom_jitter(width = 0.001, height = 0.001, alpha = 0.2, size = 2.5) + 
  geom_abline(color = &amp;quot;gray40&amp;quot;, lty = 2) + 
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, size = 7.5) + 
  scale_color_gradient(low = &amp;quot;darkslategray4&amp;quot;, high = &amp;quot;gray75&amp;quot;) +
  scale_x_continuous(labels = scales::label_percent()) + 
  scale_y_continuous(labels = scales::label_percent()) +  
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  coord_cartesian(xlim = c(0, NA)) + 
  labs(title = &amp;quot;Word frequency between two sisters&amp;quot;) + 
  theme(text = element_text(size = 18),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Words lie near the line such as ‚Äúhome‚Äù, ‚Äúhead‚Äù and ‚Äúhalf‚Äù indicate similar tendency to use that word, while those that are far from the line are words that are found more in one set of texts than another, for example ‚Äúheadthcliff‚Äù, ‚Äúlinton‚Äù, ‚Äúcatherine‚Äù, etc.&lt;/p&gt;
&lt;p&gt;What does this plot tell us? Judged only by word frequency, it looks that there are a number of words that are quite characteristic of Emily Bront√´ (upper left corner). Charlotte, on the other hand, has few representative words (bottom right corner). We will investigate this further in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data preprocessing&lt;/h2&gt;
&lt;p&gt;There are 423 and features (words) and 47119 observations in total. Approximately 18% of the response are 1 (Emily Bront√´).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  count(author) %&amp;gt;% 
  mutate(prop = n / sum(n))
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   author               n  prop
#&amp;gt;   &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt; 1 Charlotte Bront√´ 64858 0.817
#&amp;gt; 2 Emily Bront√´     14489 0.183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it‚Äôs time to widen our data to reach an appropriate model structure, this similar to a document-term matrix, with rows being a line and column word count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
set.seed(2020)
doParallel::registerDoParallel()

model_df &amp;lt;- tidy_books %&amp;gt;% 
  count(line_index, word) %&amp;gt;% 
  pivot_wider(names_from = word, values_from = n,
              values_fill = list(n = 0)) %&amp;gt;% 
  left_join(books, by = c(&amp;quot;line_index&amp;quot; = &amp;quot;line_index&amp;quot;)) %&amp;gt;% 
  select(-title, -text)

model_df
#&amp;gt; # A tibble: 47,119 x 425
#&amp;gt;    line_index heights wuthering chapter returned visit heathcliff heaven black
#&amp;gt;         &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt;  1          1       1         1       0        0     0          0      0     0
#&amp;gt;  2          4       0         0       1        0     0          0      0     0
#&amp;gt;  3          7       0         0       0        1     1          0      0     0
#&amp;gt;  4         11       0         0       0        0     0          1      1     0
#&amp;gt;  5         13       0         0       0        0     0          0      0     1
#&amp;gt;  6         15       0         0       0        0     0          0      0     0
#&amp;gt;  7         18       0         0       0        0     0          1      0     0
#&amp;gt;  8         20       0         0       0        0     0          0      0     0
#&amp;gt;  9         22       0         0       0        0     0          0      0     0
#&amp;gt; 10         23       0         0       0        0     0          0      0     0
#&amp;gt; # ... with 47,109 more rows, and 416 more variables: eyes &amp;lt;int&amp;gt;, heart &amp;lt;int&amp;gt;,
#&amp;gt; #   fingers &amp;lt;int&amp;gt;, answer &amp;lt;int&amp;gt;, sir &amp;lt;int&amp;gt;, hope &amp;lt;int&amp;gt;, grange &amp;lt;int&amp;gt;,
#&amp;gt; #   heard &amp;lt;int&amp;gt;, thrushcross &amp;lt;int&amp;gt;, interrupted &amp;lt;int&amp;gt;, walk &amp;lt;int&amp;gt;,
#&amp;gt; #   closed &amp;lt;int&amp;gt;, uttered &amp;lt;int&amp;gt;, gate &amp;lt;int&amp;gt;, words &amp;lt;int&amp;gt;, horse &amp;lt;int&amp;gt;,
#&amp;gt; #   hand &amp;lt;int&amp;gt;, entered &amp;lt;int&amp;gt;, joseph &amp;lt;int&amp;gt;, bring &amp;lt;int&amp;gt;, suppose &amp;lt;int&amp;gt;,
#&amp;gt; #   nay &amp;lt;int&amp;gt;, dinner &amp;lt;int&amp;gt;, guess &amp;lt;int&amp;gt;, times &amp;lt;int&amp;gt;, wind &amp;lt;int&amp;gt;, house &amp;lt;int&amp;gt;,
#&amp;gt; #   strong &amp;lt;int&amp;gt;, set &amp;lt;int&amp;gt;, wall &amp;lt;int&amp;gt;, door &amp;lt;int&amp;gt;, earnshaw &amp;lt;int&amp;gt;,
#&amp;gt; #   hareton &amp;lt;int&amp;gt;, short &amp;lt;int&amp;gt;, appeared &amp;lt;int&amp;gt;, desire &amp;lt;int&amp;gt;, entrance &amp;lt;int&amp;gt;,
#&amp;gt; #   brought &amp;lt;int&amp;gt;, family &amp;lt;int&amp;gt;, sitting &amp;lt;int&amp;gt;, call &amp;lt;int&amp;gt;, kitchen &amp;lt;int&amp;gt;,
#&amp;gt; #   parlour &amp;lt;int&amp;gt;, deep &amp;lt;int&amp;gt;, observed &amp;lt;int&amp;gt;, light &amp;lt;int&amp;gt;, eye &amp;lt;int&amp;gt;,
#&amp;gt; #   lay &amp;lt;int&amp;gt;, floor &amp;lt;int&amp;gt;, white &amp;lt;int&amp;gt;, countenance &amp;lt;int&amp;gt;, arm &amp;lt;int&amp;gt;,
#&amp;gt; #   chair &amp;lt;int&amp;gt;, seated &amp;lt;int&amp;gt;, round &amp;lt;int&amp;gt;, table &amp;lt;int&amp;gt;, time &amp;lt;int&amp;gt;,
#&amp;gt; #   living &amp;lt;int&amp;gt;, dark &amp;lt;int&amp;gt;, people &amp;lt;int&amp;gt;, hate &amp;lt;int&amp;gt;, love &amp;lt;int&amp;gt;,
#&amp;gt; #   loved &amp;lt;int&amp;gt;, fast &amp;lt;int&amp;gt;, dear &amp;lt;int&amp;gt;, mother &amp;lt;int&amp;gt;, home &amp;lt;int&amp;gt;,
#&amp;gt; #   summer &amp;lt;int&amp;gt;, fine &amp;lt;int&amp;gt;, company &amp;lt;int&amp;gt;, creature &amp;lt;int&amp;gt;, notice &amp;lt;int&amp;gt;,
#&amp;gt; #   told &amp;lt;int&amp;gt;, head &amp;lt;int&amp;gt;, looked &amp;lt;int&amp;gt;, return &amp;lt;int&amp;gt;, poor &amp;lt;int&amp;gt;, till &amp;lt;int&amp;gt;,
#&amp;gt; #   doubt &amp;lt;int&amp;gt;, seat &amp;lt;int&amp;gt;, silence &amp;lt;int&amp;gt;, left &amp;lt;int&amp;gt;, dog &amp;lt;int&amp;gt;,
#&amp;gt; #   master &amp;lt;int&amp;gt;, sat &amp;lt;int&amp;gt;, scarcely &amp;lt;int&amp;gt;, half &amp;lt;int&amp;gt;, hearth &amp;lt;int&amp;gt;,
#&amp;gt; #   arms &amp;lt;int&amp;gt;, fire &amp;lt;int&amp;gt;, purpose &amp;lt;int&amp;gt;, tongue &amp;lt;int&amp;gt;, remained &amp;lt;int&amp;gt;,
#&amp;gt; #   devil &amp;lt;int&amp;gt;, manner &amp;lt;int&amp;gt;, matter &amp;lt;int&amp;gt;, ill &amp;lt;int&amp;gt;, muttered &amp;lt;int&amp;gt;,
#&amp;gt; #   worse &amp;lt;int&amp;gt;, leave &amp;lt;int&amp;gt;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train-a-penalized-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train a penalized logistic regression model&lt;/h2&gt;
&lt;p&gt;Split the data into training set and testing set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_split &amp;lt;- initial_split(model_df)
book_train &amp;lt;- training(book_split)
book_test &amp;lt;- testing(book_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify a L1 penalized logistic model, center and scale all predictors and combine them in to a &lt;code&gt;workflow&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_spec &amp;lt;- logistic_reg(penalty = 0.05, mixture = 1) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

book_rec &amp;lt;- recipe(author ~ ., data = book_train) %&amp;gt;% 
  update_role(line_index, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_zv(all_predictors()) %&amp;gt;% 
  step_normalize(all_predictors())

book_wf &amp;lt;- workflow() %&amp;gt;% 
  add_model(logistic_spec) %&amp;gt;% 
  add_recipe(book_rec)

initial_fit &amp;lt;- book_wf %&amp;gt;% 
  fit(data = book_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;initial_fit&lt;/code&gt; is a simple fitted regression model without any hyperparameters. By default &lt;code&gt;glmnet&lt;/code&gt; calls for 100 values of lambda even if I specify &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0.05\)&lt;/span&gt;. So the extracted result aren‚Äôt that helpful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;% 
  tidy()
#&amp;gt; # A tibble: 26,019 x 5
#&amp;gt;    term         step estimate lambda dev.ratio
#&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
#&amp;gt;  1 (Intercept)     1 -1.64    0.0838  5.24e-14
#&amp;gt;  2 (Intercept)     2 -1.64    0.0764  8.68e- 3
#&amp;gt;  3 heathcliff      2  0.0469  0.0764  8.68e- 3
#&amp;gt;  4 linton          2  0.00120 0.0764  8.68e- 3
#&amp;gt;  5 (Intercept)     3 -1.64    0.0696  2.51e- 2
#&amp;gt;  6 heathcliff      3  0.0801  0.0696  2.51e- 2
#&amp;gt;  7 catherine       3  0.0269  0.0696  2.51e- 2
#&amp;gt;  8 linton          3  0.0413  0.0696  2.51e- 2
#&amp;gt;  9 (Intercept)     4 -1.64    0.0634  3.80e- 2
#&amp;gt; 10 heathcliff      4  0.107   0.0634  3.80e- 2
#&amp;gt; # ... with 26,009 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make predictions with &lt;code&gt;initial_fit&lt;/code&gt; anyway, and examine metrics like overall accuracy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict &amp;lt;- predict(initial_fit, book_test) %&amp;gt;% 
    bind_cols(predict(initial_fit, book_test, type = &amp;quot;prob&amp;quot;)) %&amp;gt;%
    bind_cols(book_test %&amp;gt;% select(author, line_index))

initial_predict
#&amp;gt; # A tibble: 11,779 x 5
#&amp;gt;    .pred_class     `.pred_Charlotte Bro~ `.pred_Emily Bron~ author    line_index
#&amp;gt;    &amp;lt;fct&amp;gt;                           &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;int&amp;gt;
#&amp;gt;  1 Charlotte Bron~                 0.844              0.156 Emily Br~          1
#&amp;gt;  2 Charlotte Bron~                 0.844              0.156 Emily Br~         13
#&amp;gt;  3 Charlotte Bron~                 0.844              0.156 Emily Br~         30
#&amp;gt;  4 Charlotte Bron~                 0.844              0.156 Emily Br~         31
#&amp;gt;  5 Charlotte Bron~                 0.844              0.156 Emily Br~         36
#&amp;gt;  6 Charlotte Bron~                 0.844              0.156 Emily Br~         56
#&amp;gt;  7 Charlotte Bron~                 0.844              0.156 Emily Br~         68
#&amp;gt;  8 Charlotte Bron~                 0.844              0.156 Emily Br~         75
#&amp;gt;  9 Charlotte Bron~                 0.844              0.156 Emily Br~         89
#&amp;gt; 10 Charlotte Bron~                 0.844              0.156 Emily Br~         96
#&amp;gt; # ... with 11,769 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How good is our initial model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict %&amp;gt;% 
  accuracy(truth = author, estimate = .pred_class)
#&amp;gt; # A tibble: 1 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.844&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nearly 84% of all predictions are right. This isn‚Äôt a very statisfactory result since ‚ÄúCharlotte Bront√´‚Äù accounts for 81% of &lt;code&gt;author&lt;/code&gt;, making our model only slightly better than a classifier that would assngin all &lt;code&gt;author&lt;/code&gt; with ‚ÄúCharlotte Bront√´‚Äù anyway.&lt;/p&gt;
&lt;div id=&#34;tuning-lambda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tuning lambda&lt;/h3&gt;
&lt;p&gt;We can figure out an appropriate penalty using resampling and tune the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_wf_tune &amp;lt;- book_wf %&amp;gt;%
  update_model(logistic_spec %&amp;gt;% set_args(penalty = tune()))

lambda_grid &amp;lt;- grid_regular(penalty(), levels = 100)
book_folds &amp;lt;- vfold_cv(book_train, v = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I build a set of 10 cross validations resamples, and set &lt;code&gt;levels = 100&lt;/code&gt; to try 100 choices of lambda ranging from 0 to 1.&lt;/p&gt;
&lt;p&gt;Then I tune the grid:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results &amp;lt;- logistic_wf_tune %&amp;gt;%    
  tune_grid(
    resamples = book_folds,
    grid = lambda_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is an &lt;code&gt;autoplot()&lt;/code&gt; method for the tuned results, but I instead plot two metrics versus lambda respectivcely by myself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  collect_metrics() %&amp;gt;% 
  mutate(lower_bound = mean - std_err,
         upper_bound = mean + std_err) %&amp;gt;%
  ggplot(aes(penalty, mean)) + 
  geom_line(aes(color = .metric), size = 1.5, show.legend = FALSE) + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) + 
  facet_wrap(~ .metric, nrow = 2) + 
  labs(y = NULL,
       x = expression(lambda),
       title = &amp;quot;Performance metric of logistic regession versus differenct choices of L1 regularization&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, the two metrics both display a monotone decrease as lambda increases, but does not exhibit much change once lambda is greater than 0.1, which is essentailly random guess according to the author‚Äôs respective proportion of appearance in the data. This plot shows that the model is generally better at small penalty, suggesting that the majority of the predictors are fairly important to the model. We may lean towards larger penalty with slightly worse performance, bacause they lead to simpler models. It follows that we may want to choose lambda in top rows in the following data frame&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_models &amp;lt;- logistic_results %&amp;gt;% 
    show_best(&amp;quot;roc_auc&amp;quot;, n = 100) %&amp;gt;%
    arrange(desc(penalty)) %&amp;gt;% 
    filter(mean &amp;gt; 0.9)

top_models
#&amp;gt; # A tibble: 76 x 6
#&amp;gt;     penalty .metric .estimator  mean     n std_err
#&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
#&amp;gt;  1 0.00376  roc_auc binary     0.906    10 0.00215
#&amp;gt;  2 0.00298  roc_auc binary     0.908    10 0.00219
#&amp;gt;  3 0.00236  roc_auc binary     0.908    10 0.00222
#&amp;gt;  4 0.00187  roc_auc binary     0.908    10 0.00227
#&amp;gt;  5 0.00148  roc_auc binary     0.909    10 0.00209
#&amp;gt;  6 0.00118  roc_auc binary     0.910    10 0.00207
#&amp;gt;  7 0.000933 roc_auc binary     0.910    10 0.00202
#&amp;gt;  8 0.000739 roc_auc binary     0.910    10 0.00205
#&amp;gt;  9 0.000586 roc_auc binary     0.910    10 0.00209
#&amp;gt; 10 0.000464 roc_auc binary     0.910    10 0.00210
#&amp;gt; # ... with 66 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;select_best()&lt;/code&gt; with return the 9th row with &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.000586\)&lt;/span&gt; for its highest performance on &lt;code&gt;roc_auc&lt;/code&gt;. But I‚Äôll stick to the parsimonious principle and pick &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.00376\)&lt;/span&gt; at the cost of a fall in roc_auc by 0.005 and in accuracy by 0.001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  select_best(metric = &amp;quot;roc_auc&amp;quot;)
#&amp;gt; # A tibble: 1 x 1
#&amp;gt;    penalty
#&amp;gt;      &amp;lt;dbl&amp;gt;
#&amp;gt; 1 0.000586

book_wf_final &amp;lt;- finalize_workflow(logistic_wf_tune,
                                  parameters = top_models %&amp;gt;% slice(1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the model specification in the workflow is filled with the picked lambda:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_wf_final %&amp;gt;% pull_workflow_spec()
#&amp;gt; Logistic Regression Model Specification (classification)
#&amp;gt; 
#&amp;gt; Main Arguments:
#&amp;gt;   penalty = 0.00376493580679246
#&amp;gt;   mixture = 1
#&amp;gt; 
#&amp;gt; Computational engine: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next thing is to fit the best model with the training set, and evaluate against the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final &amp;lt;- last_fit(book_wf_final, split = book_split)

logistic_final %&amp;gt;% 
  collect_metrics()
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.938
#&amp;gt; 2 roc_auc  binary         0.904&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;% 
  collect_predictions() %&amp;gt;%
  roc_curve(truth = author, `.pred_Emily Bront√´`) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;roc_curve.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The accuracy of our logisitc model rises by a rough 7% to 93.8%, with &lt;code&gt;roc_auc&lt;/code&gt; being nearly 0.904. This is pretty good!&lt;/p&gt;
&lt;p&gt;There is also the confusion matrix to check. The model does well in identifying Charlotte Bront√´ (low false positive rate, high sensitivity), yet suffers relatively high false negative rate (mistakenly identify 39% of Emily Bront√´ as Charlotte Bront√´, low specificity).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;%
  collect_predictions() %&amp;gt;%
  conf_mat(truth = author, estimate = .pred_class) 
#&amp;gt;                   Truth
#&amp;gt; Prediction         Charlotte Bront√´ Emily Bront√´
#&amp;gt;   Charlotte Bront√´             9921          726
#&amp;gt;   Emily Bront√´                    1         1131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To examine the effect of predictors, I agian use &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;pull_workflow&lt;/code&gt; to extract model fit. Variable importance plots implemented in the &lt;a href=&#34;https://koalaverse.github.io/vip/index.html&#34;&gt;vip&lt;/a&gt; package provides an intuitive way to visualize importance of predictors in this scenario, using th absolute value of the t-statistic as a measure of VI.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vip)

logistic_vi &amp;lt;- book_wf_final %&amp;gt;% 
  fit(book_train) %&amp;gt;% 
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = top_models[1, ]$penalty) %&amp;gt;% 
  group_by(Sign) %&amp;gt;% 
  top_n(30, wt = abs(Importance)) %&amp;gt;%
  ungroup() %&amp;gt;% 
  mutate(Sign = if_else(Sign == &amp;quot;POS&amp;quot;, 
                        &amp;quot;More Emily Bront√´&amp;quot;, &amp;quot;More Charlotte Bront√´&amp;quot;))

logistic_vi %&amp;gt;% 
  ggplot(aes(y = reorder_within(Variable, abs(Importance), Sign),
             x = Importance)) + 
  geom_col(aes(fill = Sign), show.legend = FALSE) +
  scale_y_reordered() + 
  facet_wrap(~ Sign, nrow = 1, scales = &amp;quot;free&amp;quot;) + 
  labs(title = &amp;quot;How word usage classifies Bront√´ sisters&amp;quot;,
       x = NULL,
       y = NULL) + 
  theme(axis.text = element_text(size = 18),
        plot.title = element_text(size = 24),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-25&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-25-1.png&#34; alt=&#34;Variable importance plot for penalized logistic regression&#34; width=&#34;1056&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Variable importance plot for penalized logistic regression
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Is it cheating to use names of a character to classify authors? Perhaps I should consider include more books and remove names for text classification next time.&lt;/p&gt;
&lt;p&gt;Note that variale importance in the left panel is generally smaller than the right, this corresponds to what we find in the &lt;a href=&#34;#comparing-word-frequency&#34;&gt;word frequency&lt;/a&gt; plot that Emily Bront√´ has more and stronger characteristic words.&lt;/p&gt;
&lt;p&gt;In conclusion, the model with the best lambda seems quite powerful in distinguishing these two authors. I look forward to build a multinomial classification model in &lt;code&gt;tidymodels&lt;/code&gt; to include Anne Bront√´ some other time!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Who is Carrying There Team in OWL?</title>
      <link>/post/2020-04-25-who-is-carrying-there-team-in-owl/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-04-25-who-is-carrying-there-team-in-owl/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-fleta-deadlift&#34;&gt;The Fleta deadlift&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#extend-the-stat-to-tanks-and-supports&#34;&gt;Extend the stat to tanks and supports&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;the-fleta-deadlift&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Fleta deadlift&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://overwatchleague.com/en-us/statslab&#34;&gt;Overwatch Stats Lab&lt;/a&gt; is a treasure trove holding vast digital riches for people who want to observe the Overwatch League in a more detailed manner. One of my favourite statistic is the &lt;strong&gt;Fleta deadlift&lt;/strong&gt;, calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Fleta deadlifts} = \frac{\text{final blows of a player}}{\text{final blows of the whole team}}
\]&lt;/span&gt;
It comes as no surprise that the index was named after Fleta, the current Shanghai dragons DPS player, who is known for his former amazing performance in a less talented team before he joined Seoul Dyanasty and entered OWL. If you are not familiar with esports or overwatch, I have some equivelent ‚ÄúFleta deadlifted his team‚Äù examples in the NBA context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Michael Jordan in the 1998 final&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Kobe Bryant (I miss him) in season 2006-2007, or perhaps half time in his career&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Lebron Jmaes in the 2015 final&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The player section on Overwatch Stats Lab even established a special tab for this stat, showing records with Fleta deadlift &amp;gt; 50% (see the last tab below):&lt;br /&gt;
&lt;iframe src=&#34;https://overwatchleague.com/en-us/statslab-players&#34; width=&#34;768&#34; height=&#34;400px&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;It should be obvious though, that Fleta deadlifts are more suitable for describe performaces of DPS players, and less applicable to the other two positions, tanks and supports. Since final blows may not be a good measure for a player‚Äôs performance if his main responsibility indicates otherwise.&lt;/p&gt;
&lt;p&gt;I used &lt;code&gt;load_data.R&lt;/code&gt; to laod in data and do soem preprocessing. You can browse it at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(vroom)

df &amp;lt;- vroom(&amp;quot;D:/RProjects/data/overwatch/fleta_deadlifts.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;extend-the-stat-to-tanks-and-supports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extend the stat to tanks and supports&lt;/h2&gt;
&lt;p&gt;However, since OWL mandates the 2-2-2 composition only after the third stage in season 2019. A player can belong to different positions even in the same map or match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dpss &amp;lt;- c(&amp;quot;Hanzo&amp;quot;, &amp;quot;Junkrat&amp;quot;, &amp;quot;Tracer&amp;quot;, &amp;quot;Soldier: 76&amp;quot;, &amp;quot;Widowmaker&amp;quot;, &amp;quot;McCree&amp;quot;, &amp;quot;Pharah&amp;quot;, &amp;quot;Genji&amp;quot;, &amp;quot;Sombra&amp;quot;, &amp;quot;Reaper&amp;quot;, &amp;quot;Doomfist&amp;quot;, &amp;quot;Bastion&amp;quot;, &amp;quot;Mei&amp;quot;, &amp;quot;Torbj√∂rn&amp;quot;, &amp;quot;Symmetra&amp;quot;, &amp;quot;Ashe&amp;quot;)
tanks &amp;lt;- c(&amp;quot;Orisa&amp;quot;, &amp;quot;Winston&amp;quot;, &amp;quot;Roadhog&amp;quot;, &amp;quot;D.Va&amp;quot;, &amp;quot;Zarya&amp;quot;, &amp;quot;Reinhardt&amp;quot;, &amp;quot;Wrecking Ball&amp;quot;)
supports &amp;lt;- c(&amp;quot;L√∫cio&amp;quot;, &amp;quot;Mercy&amp;quot;, &amp;quot;Zenyatta&amp;quot;, &amp;quot;Brigitte&amp;quot;, &amp;quot;Moira&amp;quot;, &amp;quot;Ana&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;Baptiste&amp;quot;)

dps_records &amp;lt;- df %&amp;gt;% filter(hero %in% dpss)
tank_records &amp;lt;- df %&amp;gt;% filter(hero %in% tanks)
support_records &amp;lt;- df %&amp;gt;% filter(hero %in% supports)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For dps players, I will stick to the official measure of a Fleta deadlift: involved in more than 50% percent of team total final blows in a match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dps_deadlifts &amp;lt;- dps_records %&amp;gt;% 
  filter(stat_name == &amp;quot;Final Blows&amp;quot;) %&amp;gt;% 
  mutate(final_blows = stat_amount) %&amp;gt;% 
  select(-stat_name, -stat_amount) %&amp;gt;%
  add_count(date, team, wt = final_blows, name = &amp;quot;team_final_blows&amp;quot;) %&amp;gt;%
  filter(team_final_blows &amp;gt; 10) %&amp;gt;% 
  group_by(date, player, team_final_blows) %&amp;gt;%
  summarize(player_final_blows = sum(final_blows)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(rate = player_final_blows / team_final_blows) %&amp;gt;%
  filter(rate &amp;gt; 0.5) 

dps_deadlifts
#&amp;gt; # A tibble: 827 x 5
#&amp;gt;    date       player   team_final_blows player_final_blows  rate
#&amp;gt;    &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt;  1 2018-01-11 babybay                95                 48 0.505
#&amp;gt;  2 2018-01-11 carpe                  44                 25 0.568
#&amp;gt;  3 2018-01-11 EFFECT                139                 70 0.504
#&amp;gt;  4 2018-01-11 Fleta                 131                 68 0.519
#&amp;gt;  5 2018-01-11 Jake                   35                 20 0.571
#&amp;gt;  6 2018-01-11 Profit                 90                 58 0.644
#&amp;gt;  7 2018-01-11 TviQ                   80                 42 0.525
#&amp;gt;  8 2018-01-11 Undead                 35                 18 0.514
#&amp;gt;  9 2018-01-12 Jake                   95                 49 0.516
#&amp;gt; 10 2018-01-13 Birdring              100                 52 0.52 
#&amp;gt; # ... with 817 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the London Stage Database</title>
      <link>/post/exploring-the-london-stage-database/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-london-stage-database/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# setup
library(tidyverse)
library(jsonlite)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the data
london_stage &amp;lt;- fromJSON(&amp;quot;D:/RProjects/data/LondonStageFull.json&amp;quot;) %&amp;gt;% 
  as_tibble()

london_stage
#&amp;gt; # A tibble: 52,617 x 13
#&amp;gt;    EventId EventDate TheatreCode Season Volume Hathi CommentC TheatreId Phase2
#&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; 
#&amp;gt;  1 0       16591029  city        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;The &amp;lt;i~ 63        *p165~
#&amp;gt;  2 1       16591100  mt          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;On 23 ~ 206       *p165~
#&amp;gt;  3 2       16591218  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;Repres~ 1         *p165~
#&amp;gt;  4 3       16600200  mt          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;6 Feb.~ 206       *p166~
#&amp;gt;  5 4       16600204  cockpit     1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;$Thoma~ 73        *p166~
#&amp;gt;  6 5       16600328  dh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;At &amp;lt;i&amp;gt;~ 90        *p166~
#&amp;gt;  7 6       16600406  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;quot;       1         *p166~
#&amp;gt;  8 7       16600412  vh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;Editio~ 319       *p166~
#&amp;gt;  9 8       16600413  fh          1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;lt;i&amp;gt;The~ 116       *p166~
#&amp;gt; 10 9       16600416  none        1659-~ 1      &amp;quot;&amp;quot;    &amp;quot;&amp;quot;       1         *p166~
#&amp;gt; # ... with 52,607 more rows, and 4 more variables: Phase1 &amp;lt;chr&amp;gt;,
#&amp;gt; #   CommentCClean &amp;lt;chr&amp;gt;, BookPDF &amp;lt;chr&amp;gt;, Performances &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;json&lt;/code&gt; file can be downloaded at &lt;a href=&#34;https://londonstagedatabase.usu.edu/downloads/LondonStageJSON.zip&#34; class=&#34;uri&#34;&gt;https://londonstagedatabase.usu.edu/downloads/LondonStageJSON.zip&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing COVID-19 Publications</title>
      <link>/post/analyzing-covid-19-publications/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/analyzing-covid-19-publications/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-words-and-keywords-extraction&#34;&gt;Common words and keywords extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-a-lda-model&#34;&gt;Fit a LDA model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-network-of-paired-words&#34;&gt;A network of paired words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post, I will be performing a simple text analysis on the abstract of publications on the coronavirus disease (COVID-19), courtesy of &lt;a href=&#34;https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov&#34;&gt;WHO&lt;/a&gt;. We begin by steps of data preprocessing.&lt;/p&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data preprocessing&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw &amp;lt;- read_csv(&amp;quot;D:/RProjects/data/covid-research.csv&amp;quot;) %&amp;gt;% 
  janitor::clean_names() 

glimpse(raw)
#&amp;gt; Rows: 4,190
#&amp;gt; Columns: 16
#&amp;gt; $ title            &amp;lt;chr&amp;gt; &amp;quot;SARS-CoV-2 is not detectable in the vaginal fluid...
#&amp;gt; $ authors          &amp;lt;chr&amp;gt; &amp;quot;Qiu, Lin; Liu, Xia; Xiao, Meng; Xie, Jing; Cao, W...
#&amp;gt; $ abstract         &amp;lt;chr&amp;gt; &amp;quot;Background Severe acute respiratory syndrome coro...
#&amp;gt; $ published_year   &amp;lt;dbl&amp;gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20...
#&amp;gt; $ published_month  &amp;lt;lgl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&amp;gt; $ journal          &amp;lt;chr&amp;gt; &amp;quot;Clinical Infectious Diseases&amp;quot;, &amp;quot;International Jou...
#&amp;gt; $ volume           &amp;lt;chr&amp;gt; NA, &amp;quot;17&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
#&amp;gt; $ issue            &amp;lt;chr&amp;gt; NA, &amp;quot;7&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
#&amp;gt; $ pages            &amp;lt;chr&amp;gt; NA, &amp;quot;2430-2430&amp;quot;, &amp;quot;112275-112275&amp;quot;, NA, &amp;quot;1-4&amp;quot;, NA, N...
#&amp;gt; $ accession_number &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, &amp;quot;32229574&amp;quot;, NA, NA, &amp;quot;3...
#&amp;gt; $ doi              &amp;lt;chr&amp;gt; &amp;quot;10.1093/cid/ciaa375&amp;quot;, &amp;quot;10.3390/IJERPH17072430&amp;quot;, &amp;quot;...
#&amp;gt; $ ref              &amp;lt;dbl&amp;gt; 26513, 26499, 26744, 26447, 27114, 26388, 26696, 2...
#&amp;gt; $ covidence_number &amp;lt;chr&amp;gt; &amp;quot;#27487&amp;quot;, &amp;quot;#27413&amp;quot;, &amp;quot;#27869&amp;quot;, &amp;quot;#27815&amp;quot;, &amp;quot;#27905&amp;quot;, ...
#&amp;gt; $ study            &amp;lt;chr&amp;gt; &amp;quot;Qiu 2020&amp;quot;, &amp;quot;Pulido 2020&amp;quot;, &amp;quot;Pillaiyar 2020&amp;quot;, &amp;quot;Piau...
#&amp;gt; $ notes            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
#&amp;gt; $ tags             &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For simplicity I ignore many of the vairables (mostly for identification) and rows with missing values on &lt;code&gt;abstract&lt;/code&gt;. I was a little disappointied to find out that &lt;code&gt;published_month&lt;/code&gt; are all missing, otherwise we may see a trend of some sort on research topics there. One remaining problem is that some of the papers are not written in English, I find this function &lt;code&gt;stringi::stri_enc_isascii&lt;/code&gt; in an attempt to filter out non-English text, although this will not rid the text of German, French, Italian and other similar languages. I deleted some of them manually in the next step by expanding stop words. But this still left much room for improvement. Anyway, let‚Äôs move on for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- raw %&amp;gt;%
  filter(!is.na(abstract),
         stringi::stri_enc_isascii(abstract)) %&amp;gt;% 
  select(title, abstract)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As illuatrated in &lt;a href=&#34;https://www.tidytextmining.com&#34;&gt;Text Mining with R&lt;/a&gt;, text analysis commonly requires preprocessing steps like tokenizing, eliminating stop words and word stemming. A little problem is that &lt;code&gt;unnest_tokens()&lt;/code&gt; recognize ‚Äúcovid‚Äù and ‚Äú19‚Äù as two separated words, so I just add ‚Äú19‚Äù to my custom stop words list. As a result, we have to bear in mind that most ‚Äúcovid‚Äù seen below is actually ‚Äúcovid19‚Äù.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(SnowballC)

words &amp;lt;- df %&amp;gt;% 
  unnest_tokens(word, abstract) %&amp;gt;% 
  mutate(word = wordStem(word)) %&amp;gt;% 
  anti_join(stop_words %&amp;gt;% 
              add_row(word = c(&amp;quot;19&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;manag&amp;quot;, &amp;quot;dai&amp;quot;, &amp;quot;studi&amp;quot;, &amp;quot;epidem&amp;quot;, &amp;quot;includ&amp;quot;, &amp;quot;coronaviru&amp;quot;, &amp;quot;diseas&amp;quot;, &amp;quot;emetin&amp;quot;, &amp;quot;dai&amp;quot;, &amp;quot;acut&amp;quot;, &amp;quot;dub&amp;quot;, &amp;quot;hospit&amp;quot;, &amp;quot;hfnc&amp;quot;, &amp;quot;caus&amp;quot;, &amp;quot;develop&amp;quot;, &amp;quot;thi&amp;quot;), 
                      lexicon = &amp;quot;custom&amp;quot;)) 

words
#&amp;gt; # A tibble: 159,434 x 2
#&amp;gt;    title                                                              word      
#&amp;gt;    &amp;lt;chr&amp;gt;                                                              &amp;lt;chr&amp;gt;     
#&amp;gt;  1 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ background
#&amp;gt;  2 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ sever     
#&amp;gt;  3 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ respirato~
#&amp;gt;  4 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ syndrom   
#&amp;gt;  5 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ sar       
#&amp;gt;  6 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ cov       
#&amp;gt;  7 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ mainli    
#&amp;gt;  8 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ spread    
#&amp;gt;  9 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ respirato~
#&amp;gt; 10 SARS-CoV-2 is not detectable in the vaginal fluid of women with s~ droplet   
#&amp;gt; # ... with 159,424 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;common-words-and-keywords-extraction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common words and keywords extraction&lt;/h1&gt;
&lt;p&gt;An immediate question is, what are the most common words among all these publications?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words %&amp;gt;% 
  count(word, sort = TRUE) %&amp;gt;%
  top_n(50) %&amp;gt;%
  ggplot(aes(y = fct_reorder(word, n),
             x = n)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0.01, 0)) + 
  labs(y = NULL,
       x = &amp;quot;# of words&amp;quot;,
       title = &amp;quot;Top 50 common words in COVID-19 publications&amp;quot;) +
  theme(text = element_text(size = 18),
        plot.title = element_text(size = 35, face = &amp;quot;bold&amp;quot;),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I‚Äôm also interested in paper-specific properties, namely their keywords, what topics distinguish them from others? In comparison to the commonly used algorithm tf-idf, I prefer using weighted log odds proposed by &lt;span class=&#34;citation&#34;&gt;Monroe, Colaresi, and Quinn (&lt;a href=&#34;#ref-monroe_colaresi_quinn&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, which a standardized metric from a complete statistical model. It is also implemented in the R package &lt;a href=&#34;https://github.com/juliasilge/tidylo&#34;&gt;&lt;code&gt;tidylo&lt;/code&gt;&lt;/a&gt;&lt;span class=&#34;citation&#34;&gt;(Schnoebelen and Silge &lt;a href=&#34;#ref-tidylo&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. The reason is that tf-idf cannot extract the varying use trend of common words, if a word appears in every research paper, then its inverse document frequency will be zero. For weighted log odds this is not the case, even if all researched mentioned this word it can still differentiate those who used it a lot more often from those who used less. This could be essential when we are trying to find an emphasis on which researchers place as our understanding of the virus advances. Sadly I have no access to the exact date of the publication, so I will just display words with topest score and their corresponding publications:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidylo)

words %&amp;gt;%
  count(title, word) %&amp;gt;% 
  bind_log_odds(set = title, feature = word, n = n) %&amp;gt;%
  top_n(20) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;title&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;log_odds&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A comparative study on the clinical features of COVID-19 pneumonia to other pneumonias&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ncovid&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.057257&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A high ATP concentration enhances the cooperative translocation of the SARS coronavirus helicase nsP13 in the unwinding of duplex RNA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;duplex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.524146&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A serological survey of canine respiratory coronavirus in New Zealand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;dog&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.528876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A survey on awareness of digestive system injury caused by corona virus disease 2019 in gastroenterologists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.423606&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A survey on awareness of digestive system injury caused by corona virus disease 2019 in gastroenterologists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;gastroenterologist&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.225689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Analysis of clinical features of 153 patients with novel coronavirus pneumonia in Chongqing&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.800243&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Analysis of clinical features of 153 patients with novel coronavirus pneumonia in Chongqing&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;gt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.917806&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Analysis of clinical features of 153 patients with novel coronavirus pneumonia in Chongqing&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ital&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.585456&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Analysis of clinical features of 153 patients with novel coronavirus pneumonia in Chongqing&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.032569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;hljby&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.915206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Characterization and evolution of the coronavirus porcine epidemic diarrhoea virus HLJBY isolated in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pedv&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.922771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Clinical characteristics of 113 deceased patients with coronavirus disease 2019: retrospective study&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;deceas&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.419020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Construction of 5G intelligent medical service system in novel coronavirus pneumonia prevention and control&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5g&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.231003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Frequency and Distribution of Chest Radiographic Findings in COVID-19 Positive Patients&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cxr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.915366&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Genetic, antigenic and pathogenic characterization of avian coronaviruses isolated from pheasants (Phasianus colchicus) in China&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;phcov&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.576166&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Medicinal chemistry strategies toward host targeting antiviral agents&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;hta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.573884&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Pregnant women with new coronavirus infection: a clinical characteristics and placental pathological analysis of three cases&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;placenta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.757207&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Retrospective study of low-to-moderate dose glucocorticoids on viral clearance in patients with novel coronavirus pneumonia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;iqr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.238798&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The clinical study on the relationship between serum albumin concentration and lymphocyte levels in patients with 2019-novel coronavirus pneumonia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lym&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.536764&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Thoughts and suggestions on modern construction of disease prevention and control system&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;modern&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.443162&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-lda-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fit a LDA model&lt;/h1&gt;
&lt;p&gt;Let‚Äôs then fit a 6-topic LDA topic model, before that we should convert the data frame to a docuemnt term matrix using &lt;code&gt;cast_dtm&lt;/code&gt;. There are various implementations of this kind of model, here I use &lt;code&gt;stm::stm&lt;/code&gt;. The choice of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; (number of topics) here is somewhat arbitrary here, but &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; had a great &lt;a href=&#34;https://juliasilge.com/blog/evaluating-stm/&#34;&gt;post&lt;/a&gt; about it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfm &amp;lt;- cast_dfm(words %&amp;gt;% count(title, word),
                term = word,
                document = title,
                value = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stm)
topic_model &amp;lt;- stm(dfm, K = 5, init.type = &amp;quot;LDA&amp;quot;, verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;topic_model &amp;lt;- pins::pin_get(&amp;quot;covid-topic-model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Topic-term probability distributions are accessed by &lt;code&gt;tidy()&lt;/code&gt;, this gives a glance of the underlying meaning of these topics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# topic-term distribution
tidy(topic_model) %&amp;gt;% 
  group_by(topic) %&amp;gt;% 
  top_n(10) %&amp;gt;% 
  ungroup() %&amp;gt;%
  mutate(topic = factor(topic) %&amp;gt;% str_c(&amp;quot;topic&amp;quot;, .)) %&amp;gt;% 
  ggplot(aes(y = reorder_within(term, beta, topic),
         x = beta,
         fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  scale_y_reordered() + 
  facet_wrap(~ topic, scales = &amp;quot;free_y&amp;quot;, nrow = 3) + 
  labs(y = NULL,
       x = &amp;quot;Docuemtn-term probabilities&amp;quot;,
       title = &amp;quot;A 6-topic LDA model&amp;quot;) + 
  theme(text = element_text(size = 18),
        plot.title = element_text(size = 30, face = &amp;quot;bold&amp;quot;),
        strip.text = element_text(size = 25, hjust = 0.05),
        axis.ticks.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-network-of-paired-words&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A network of paired words&lt;/h1&gt;
&lt;p&gt;Another question of interest is the relationship between words: what group of words tend to appear together? I look at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Phi_coefficient&#34;&gt;phi coefficient&lt;/a&gt;, which is essentailly &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistc in a contingency table applied to categorical variables.&lt;/p&gt;
&lt;p&gt;As each abstract is a natual unit of measure, a pair of words that both appear in the same abstract are seen as ‚Äúappearing together‚Äù. We could compute &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; based on pairwise counts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(widyr)

word_cors &amp;lt;- words %&amp;gt;% 
  add_count(word) %&amp;gt;% 
  filter(n &amp;gt; 20) %&amp;gt;%
  select(-n) %&amp;gt;%
  pairwise_cor(item = word, feature = title, sort = TRUE)

word_cors
#&amp;gt; # A tibble: 1,386,506 x 3
#&amp;gt;    item1     item2     correlation
#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
#&amp;gt;  1 65288     65289           0.960
#&amp;gt;  2 65289     65288           0.960
#&amp;gt;  3 lopinavir ritonavir       0.955
#&amp;gt;  4 ritonavir lopinavir       0.955
#&amp;gt;  5 reserv    copyright       0.947
#&amp;gt;  6 copyright reserv          0.947
#&amp;gt;  7 glass     ground          0.946
#&amp;gt;  8 ground    glass           0.946
#&amp;gt;  9 effus     pleural         0.940
#&amp;gt; 10 pleural   effus           0.940
#&amp;gt; # ... with 1,386,496 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A network visualization of word correlation is a good idea:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
library(tidygraph)

word_cors %&amp;gt;% 
  filter(correlation &amp;gt; 0.4) %&amp;gt;% 
  as_tbl_graph() %&amp;gt;% 
  ggraph(layout = &amp;quot;fr&amp;quot;) + 
  geom_edge_link(aes(alpha = correlation), show.legend = FALSE) + 
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 6.5) + 
  geom_node_text(aes(label = name), repel = TRUE, size = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/analyzing-covid-19-publications/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, there are still many non-English words that stemming and adding stopwrods cannot handle‚Ä¶ Nonetheless, we are be able to identify some of the clusters revovling around infant infection (infant, pregnant, newborn, mother), pathology (angiotensin, protein, receptor), symptoms (lung, thicken, lesion), etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-monroe_colaresi_quinn&#34;&gt;
&lt;p&gt;Monroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008. ‚ÄúFightin‚Äô Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.‚Äù &lt;em&gt;Political Analysis&lt;/em&gt; 16 (4): 372‚Äì403. &lt;a href=&#34;https://doi.org/10.1093/pan/mpn018&#34;&gt;https://doi.org/10.1093/pan/mpn018&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tidylo&#34;&gt;
&lt;p&gt;Schnoebelen, Tyler, and Julia Silge. 2020. &lt;em&gt;Tidylo: Tidy Log Odds Ratio Weighted by Uninformative Prior&lt;/em&gt;. &lt;a href=&#34;http://github.com/juliasilge/tidylo&#34;&gt;http://github.com/juliasilge/tidylo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
