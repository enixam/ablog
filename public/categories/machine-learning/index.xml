<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Qiushi Yan</title>
    <link>/categories/machine-learning/</link>
      <atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Qiushi Yan © 2020</copyright><lastBuildDate>Fri, 08 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Machine Learning</title>
      <link>/categories/machine-learning/</link>
    </image>
    
    <item>
      <title>Machine Learning with Titanic Data</title>
      <link>/post/2020-05-08-machine-learning-with-titanic-data/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-08-machine-learning-with-titanic-data/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://github.com/ritvikmath/Machine-Learning-With-Titanic-Data/blob/master/UMSA%20May%202018%20-%20Machine%20Learning%20in%20Python.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/ritvikmath/Machine-Learning-With-Titanic-Data/blob/master/UMSA%20May%202018%20-%20Machine%20Learning%20in%20Python.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/&#34; class=&#34;uri&#34;&gt;https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import pearsonr
import math
import random 

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

from datetime import datetime
import itertools&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Random sample of all titanic passengers.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;titanic = pd.read_excel(&amp;#39;D:/RProjects/data/blog/titanic.xls&amp;#39;)
titanic.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    pclass  survived  ...   body                        home.dest
## 0       1         1  ...    NaN                     St Louis, MO
## 1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON
## 2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON
## 3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON
## 4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON
## 
## [5 rows x 14 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;titanic.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 1309 entries, 0 to 1308
## Data columns (total 14 columns):
## pclass       1309 non-null int64
## survived     1309 non-null int64
## name         1309 non-null object
## sex          1309 non-null object
## age          1046 non-null float64
## sibsp        1309 non-null int64
## parch        1309 non-null int64
## ticket       1309 non-null object
## fare         1308 non-null float64
## cabin        295 non-null object
## embarked     1307 non-null object
## boat         486 non-null object
## body         121 non-null float64
## home.dest    745 non-null object
## dtypes: float64(3), int64(4), object(7)
## memory usage: 143.3+ KB&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Animal Crossing Reviews</title>
      <link>/post/2020-05-07-analyzing-animal-crossing-reviews/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-07-analyzing-animal-crossing-reviews/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eda-and-data-cleaning&#34;&gt;EDA and data cleaning&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#repeated-reviews&#34;&gt;Repeated reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#incomplete-reviews-because-they-are-too-long&#34;&gt;Incomplete reviews because they are too long&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-english-reviews&#34;&gt;Non-English reviews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#section&#34;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sentiment-analysis-of-user-reviews&#34;&gt;Sentiment analysis of user reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictive-modeling-for-grades-with-user-reviews&#34;&gt;Predictive modeling for grades with user reviews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Our modeling goal is to predict the rating for the life simulation video game, Animal Crossing, from its user reviews. The data comes from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-05/readme.md&#34;&gt;this weeks’s &lt;code&gt;#TidyTuesday&lt;/code&gt;&lt;/a&gt;, scraped from &lt;a href=&#34;https://github.com/jefflomacy/villagerdb&#34;&gt;VillagerDB&lt;/a&gt; and &lt;a href=&#34;https://www.metacritic.com/game/switch/animal-crossing-new-horizons/critic-reviews&#34;&gt;Metacritic&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
set.seed(2020)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;eda-and-data-cleaning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;EDA and data cleaning&lt;/h1&gt;
&lt;p&gt;The reviews data contains four columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;grade&lt;/code&gt;: 0-100 score given by the critic (missing for some) where higher score = better.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_name&lt;/code&gt;: user name of the reviewer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;text&lt;/code&gt;: review of the reviewer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;date&lt;/code&gt;: when the review is published&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews &amp;lt;- readr::read_tsv(&amp;quot;D:/RProjects/data/blog/animal-crossing-reviews.tsv&amp;quot;)
glimpse(reviews)
#&amp;gt; Rows: 1,473
#&amp;gt; Columns: 4
#&amp;gt; $ grade     &amp;lt;dbl&amp;gt; 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...
#&amp;gt; $ user_name &amp;lt;chr&amp;gt; &amp;quot;mds27272&amp;quot;, &amp;quot;lolo2178&amp;quot;, &amp;quot;Roachant&amp;quot;, &amp;quot;Houndf&amp;quot;, &amp;quot;ProfessorF...
#&amp;gt; $ text      &amp;lt;chr&amp;gt; &amp;quot;My gf started playing before me. No option to create my ...
#&amp;gt; $ date      &amp;lt;date&amp;gt; 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An bar plot of all grades show a bimodal distribution. This is perhaps not that astonishing when it comes to reviewing, since people tend to go to extremes and give polarized opinions. This may suggest that we cut &lt;code&gt;grades&lt;/code&gt; into discrete levels and build a classification model afterwards, rather than modeling bare grades itself with regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews %&amp;gt;% 
  count(grade) %&amp;gt;% 
  ggplot() + 
  geom_col(aes(x = factor(grade), 
               y = n),
           fill = &amp;quot;midnightblue&amp;quot;, alpha = 0.6) + 
  labs(x = &amp;quot;grade&amp;quot;, y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data is messy in several ways (I have chosen 3 lines for example):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Review contains repetition. the following review where the first 4.5 lines are repeated in the following lines&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reviews that exceed certian length are incomplete and end with “Expand”. The following review also contains repetition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;non-English reviews&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2 tabset&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;repeated-reviews&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Repeated reviews&lt;/h3&gt;
&lt;p&gt;“While the game itself is great, really relaxing and gorgeous, i can’t ignore one thing that ruins the whole experience for me and a lot of other people as seen by the different user reviews.That thing is that you only have 1 island per console. This decision limits to one person being able to enjoy the full experience. It also nukes any creative control of the island, since you haveWhile the game itself is great, really relaxing and gorgeous, i can’t ignore one thing that ruins the whole experience for me and a lot of other people as seen by the different user reviews.That thing is that you only have 1 island per console. This decision limits to one person being able to enjoy the full experience. It also nukes any creative control of the island, since you have the other usershouse and furniture. I hope nintendo can soon fix this big issue, because for now, this killed any intentions i had to play the game.… Expand”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;incomplete-reviews-because-they-are-too-long&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Incomplete reviews because they are too long&lt;/h3&gt;
&lt;p&gt;“One island per console is a design decision that is, at best, in poor taste, and at worst, straight-up predatory behavior.Per console, only one player gets to experience the game at its fullest. The other players see less dialogue, experience less events, and are locked out entirely from certain parts of the game.No matter how good a game is, I cannot stand behind a company thatOne island per console is a design decision that is, at best, in poor taste, and at worst, straight-up predatory behavior.Per console, only one player gets to experience the game at its fullest. The other players see less dialogue, experience less events, and are locked out entirely from certain parts of the game.No matter how good a game is, I cannot stand behind a company that sees fit to make such decisions.… Expand”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-english-reviews&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-English reviews&lt;/h3&gt;
&lt;p&gt;“Una sola isla , es un asco . No puedes seguir avanzando, solo te queda recoger madera”&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;I use regular expressions to remove reptitions and “Expand” at the end. Repetitions happen when the review is long, and the repetition part often takes up 4 to 5 lines (here I use 350 or more characters to indicate the repetition part).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;clr::detect_language&lt;/code&gt; is used to exclude non-English text. This a R wrapper around Google’s Compact Language Detector 3, a neural network model for language identification . There will be misclassifications, though. As the proportion of exclusion is fairly low, we’re OK.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cld3)
reviews %&amp;gt;% 
  mutate(language = detect_language(text)) %&amp;gt;% 
  count(language, sort = TRUE)
#&amp;gt; # A tibble: 11 x 2
#&amp;gt;    language     n
#&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
#&amp;gt;  1 en        1394
#&amp;gt;  2 es          48
#&amp;gt;  3 ru           7
#&amp;gt;  4 it           6
#&amp;gt;  5 fr           5
#&amp;gt;  6 pt           5
#&amp;gt;  7 de           3
#&amp;gt;  8 &amp;lt;NA&amp;gt;         2
#&amp;gt;  9 ja           1
#&amp;gt; 10 pl           1
#&amp;gt; 11 th           1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_en &amp;lt;- reviews %&amp;gt;% 
  filter(detect_language(text) == &amp;quot;en&amp;quot;)


repetition_clean &amp;lt;- reviews %&amp;gt;% 
  filter(str_detect(text, &amp;quot;(.{350,})\\1.+&amp;quot;)) %&amp;gt;% 
  mutate(text = str_replace(text, &amp;quot;(.{350,})\\1(.+)Expand$&amp;quot;, &amp;quot;\\1\\2&amp;quot;))

reviews_clean &amp;lt;- anti_join(reviews_en, repetition_clean, 
                           by = c(&amp;quot;user_name&amp;quot; = &amp;quot;user_name&amp;quot;)) %&amp;gt;% 
  bind_rows(repetition_clean)

# draw some sample reviews
reviews_clean %&amp;gt;% 
  sample_n(3) %&amp;gt;% 
  pull(text)
#&amp;gt; [1] &amp;quot;I am currently playing this game single player however I feel compelled to review this game based on the multiplayer experience. The decision to lock the game to one island per console is nothing short of a cash grab. I am a long time Nintendo fan since I received a SNES for Christmas as a child but this behavior disgusts me and I am reluctant to spend any more money with this company if they do not address this problem. Nintendo consoles have always been family focused machines that encourage multiplayer and this seems to go against everything they have ever done to date.Sadly they have likely already sold enough units to cover costs and will simply ignore the outrage from their customers.… &amp;quot;
#&amp;gt; [2] &amp;quot;This game is excellent, I can&amp;#39;t stop playing it, it&amp;#39;s fun and has so many thing to do that I can spend hours and hours playing it and still don&amp;#39;t get tiredBest game that I have played in a while&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
#&amp;gt; [3] &amp;quot;Cast: 6/10Story: 6/10Fun: 9/10Gameplay: 8/10Recommended----------------------&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can examine how this cleaning process works by comparing the distribution of review length, before and after. The cleaning should reduce the amount of medium long and long reviews, in exchange for shorter ones.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews %&amp;gt;%
  transmute(length = str_length(text),
            type = &amp;quot;before&amp;quot;) %&amp;gt;% 
  bind_rows(reviews_clean %&amp;gt;%
              transmute(length = str_length(text), type = &amp;quot;after&amp;quot;)) %&amp;gt;%
  ggplot() + 
  geom_density(aes(length, fill = type), alpha = 0.2) + 
  scale_fill_discrete(name = NULL) + 
  labs(title = &amp;quot;Distribution of review length before and after cleaning&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-07-analyzing-animal-crossing-reviews/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis-of-user-reviews&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sentiment analysis of user reviews&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-modeling-for-grades-with-user-reviews&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predictive modeling for grades with user reviews&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
reviews %&amp;gt;% 
  transmute(rating = case_when(
    grade &amp;lt;= 4 ~ &amp;quot;low&amp;quot;,
    grade &amp;gt; 4 &amp;amp; grade &amp;lt;= 8 ~ &amp;quot;medium&amp;quot;,
    grade &amp;gt; 8 ~ &amp;quot;high&amp;quot;,
  ),
  text) %&amp;gt;% 
  unnest_tokens(line, text, token = &amp;quot;lines&amp;quot;) 
#&amp;gt; # A tibble: 1,473 x 2
#&amp;gt;    rating line                                                                  
#&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;                                                                 
#&amp;gt;  1 high   cant stop playing!                                                    
#&amp;gt;  2 high   as a first timer to the animal crossing series, it was a bit hard to ~
#&amp;gt;  3 high   this is my first animal crossing game i’ve ever played. i am seriousl~
#&amp;gt;  4 high   i like it so far. unfair to rate the game a 0 because of a poor desig~
#&amp;gt;  5 high   this game is absolutely amazing as a constant side game for one&amp;#39;s day~
#&amp;gt;  6 high   probably the best animal crossing has ever been, and to fans of the s~
#&amp;gt;  7 high   an amazing game and the best ac yet! if you don’t understand there is~
#&amp;gt;  8 high   an absolutely awesome addition to the franchise. everything about thi~
#&amp;gt;  9 high   this review contains spoilers, click expand to view.                 ~
#&amp;gt; 10 high   there are no words to describe the flawlessly, gorgeous graphics and ~
#&amp;gt; # ... with 1,463 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating Topic Models</title>
      <link>/post/2020-05-03-evaluating-topic-models/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-05-03-evaluating-topic-models/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Text Classfication with Penalized Logistic Regression</title>
      <link>/post/text-classification-logistic/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/text-classification-logistic/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-word-frequency&#34;&gt;Comparing word frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling&#34;&gt;Modeling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preprocessing&#34;&gt;Data preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#train-a-penalized-logistic-regression-model&#34;&gt;Train a penalized logistic regression model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tuning-lambda&#34;&gt;Tuning lambda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post I aim to train a text classification model with penalized logistic regression using the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt; framework. Data are from 5 books and downloaded via the &lt;a href=&#34;https://docs.ropensci.org/gutenbergr/&#34;&gt;&lt;code&gt;gutenbergr&lt;/code&gt;&lt;/a&gt; package, written by either Emily Brontë or Charlotte Brontë. And the goal is to predict the author of a line, in other words the probability of line being written by one sister instead of another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books &amp;lt;- gutenberg_works() %&amp;gt;% 
  filter(str_detect(author, &amp;quot;Brontë, Emily|Brontë, Charlotte&amp;quot;)) %&amp;gt;% 
  gutenberg_download(meta_fields = c(&amp;quot;title&amp;quot;, &amp;quot;author&amp;quot;)) %&amp;gt;% 
  transmute(title,
            author = if_else(author == &amp;quot;Brontë, Emily&amp;quot;, 
                             &amp;quot;Emily Brontë&amp;quot;, 
                             &amp;quot;Charlotte Brontë&amp;quot;) %&amp;gt;% factor(),
            line_index = row_number(),
            text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;books&lt;/code&gt; is at line level&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books
#&amp;gt; # A tibble: 88,989 x 4
#&amp;gt;    title        author     line_index text                                      
#&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                     
#&amp;gt;  1 Wuthering H~ Emily Bro~          1 &amp;quot;WUTHERING HEIGHTS&amp;quot;                       
#&amp;gt;  2 Wuthering H~ Emily Bro~          2 &amp;quot;&amp;quot;                                        
#&amp;gt;  3 Wuthering H~ Emily Bro~          3 &amp;quot;&amp;quot;                                        
#&amp;gt;  4 Wuthering H~ Emily Bro~          4 &amp;quot;CHAPTER I&amp;quot;                               
#&amp;gt;  5 Wuthering H~ Emily Bro~          5 &amp;quot;&amp;quot;                                        
#&amp;gt;  6 Wuthering H~ Emily Bro~          6 &amp;quot;&amp;quot;                                        
#&amp;gt;  7 Wuthering H~ Emily Bro~          7 &amp;quot;1801.--I have just returned from a visit~
#&amp;gt;  8 Wuthering H~ Emily Bro~          8 &amp;quot;neighbour that I shall be troubled with.~
#&amp;gt;  9 Wuthering H~ Emily Bro~          9 &amp;quot;country!  In all England, I do not belie~
#&amp;gt; 10 Wuthering H~ Emily Bro~         10 &amp;quot;situation so completely removed from the~
#&amp;gt; # ... with 88,979 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain tidy text structure illustrated in &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R&lt;/a&gt;, I use &lt;code&gt;unnest_tokens()&lt;/code&gt; to perform tokenization and remove all the stop words. I also removed characters like &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&#39;s&lt;/code&gt;, &lt;code&gt;&#39;&lt;/code&gt; and whitespaces to return valid column names after widening. But it turns out this served as some sort of stemming too! (heathcliff’s becomes heathcliff). Then low frequency words (whose frequency is less than 0.05% of an author’s total word counts) are removed. The cutoff may be a little too high if you plot that histogram, but I really need this to save computation efforts on my laptop 😅.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_books &amp;lt;- books %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;% 
  filter(!str_detect(word, &amp;quot;^\\d+$&amp;quot;)) %&amp;gt;% 
  mutate(word = str_remove_all(word, &amp;quot;_|&amp;#39;s|&amp;#39;|\\s&amp;quot;))
  
total_words &amp;lt;- clean_books %&amp;gt;%
  count(author, name = &amp;quot;total&amp;quot;)

tidy_books &amp;lt;- clean_books %&amp;gt;%
  left_join(total_words) %&amp;gt;% 
  group_by(author, total, word) %&amp;gt;%
  filter((n() / total) &amp;gt; 0.0005) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-word-frequency&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing word frequency&lt;/h1&gt;
&lt;p&gt;Before building an actual predictive model, let’s do some EDA to see different tendency to use a particular word! This will also shed light on what we would expect from the text classification. Now, we will compare word frequency (proportion) between the two sisters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  group_by(author, total) %&amp;gt;%
  count(word) %&amp;gt;% 
  mutate(prop = n / total) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(-total, -n) %&amp;gt;%
  pivot_wider(names_from  = author, values_from = prop,
              values_fill = list(prop = 0)) %&amp;gt;% 
  ggplot(aes(x = `Charlotte Brontë`, y = `Emily Brontë`, 
             color = abs(`Emily Brontë` -  `Charlotte Brontë`))) + 
  geom_jitter(width = 0.001, height = 0.001, alpha = 0.2, size = 2.5) + 
  geom_abline(color = &amp;quot;gray40&amp;quot;, lty = 2) + 
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, size = 7.5) + 
  scale_color_gradient(low = &amp;quot;darkslategray4&amp;quot;, high = &amp;quot;gray75&amp;quot;) +
  scale_x_continuous(labels = scales::label_percent()) + 
  scale_y_continuous(labels = scales::label_percent()) +  
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  coord_cartesian(xlim = c(0, NA)) + 
  labs(title = &amp;quot;Word frequency between two sisters&amp;quot;) + 
  theme(text = element_text(size = 18),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Words lie near the line such as “home”, “head” and “half” indicate similar tendency to use that word, while those that are far from the line are words that are found more in one set of texts than another, for example “headthcliff”, “linton”, “catherine”, etc.&lt;/p&gt;
&lt;p&gt;What does this plot tell us? Judged only by word frequency, it looks that there are a number of words that are quite characteristic of Emily Brontë (upper left corner). Charlotte, on the other hand, has few representative words (bottom right corner). We will investigate this further in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data preprocessing&lt;/h2&gt;
&lt;p&gt;There are 423 and features (words) and 47119 observations in total. Approximately 18% of the response are 1 (Emily Brontë).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_books %&amp;gt;% 
  count(author) %&amp;gt;% 
  mutate(prop = n / sum(n))
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   author               n  prop
#&amp;gt;   &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt; 1 Charlotte Brontë 64858 0.817
#&amp;gt; 2 Emily Brontë     14489 0.183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’s time to widen our data to reach an appropriate model structure, this similar to a document-term matrix, with rows being a line and column word count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
set.seed(2020)
doParallel::registerDoParallel()

model_df &amp;lt;- tidy_books %&amp;gt;% 
  count(line_index, word) %&amp;gt;% 
  pivot_wider(names_from = word, values_from = n,
              values_fill = list(n = 0)) %&amp;gt;% 
  left_join(books, by = c(&amp;quot;line_index&amp;quot; = &amp;quot;line_index&amp;quot;)) %&amp;gt;% 
  select(-title, -text)

model_df
#&amp;gt; # A tibble: 47,119 x 425
#&amp;gt;    line_index heights wuthering chapter returned visit heathcliff heaven black
#&amp;gt;         &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt;  1          1       1         1       0        0     0          0      0     0
#&amp;gt;  2          4       0         0       1        0     0          0      0     0
#&amp;gt;  3          7       0         0       0        1     1          0      0     0
#&amp;gt;  4         11       0         0       0        0     0          1      1     0
#&amp;gt;  5         13       0         0       0        0     0          0      0     1
#&amp;gt;  6         15       0         0       0        0     0          0      0     0
#&amp;gt;  7         18       0         0       0        0     0          1      0     0
#&amp;gt;  8         20       0         0       0        0     0          0      0     0
#&amp;gt;  9         22       0         0       0        0     0          0      0     0
#&amp;gt; 10         23       0         0       0        0     0          0      0     0
#&amp;gt; # ... with 47,109 more rows, and 416 more variables: eyes &amp;lt;int&amp;gt;, heart &amp;lt;int&amp;gt;,
#&amp;gt; #   fingers &amp;lt;int&amp;gt;, answer &amp;lt;int&amp;gt;, sir &amp;lt;int&amp;gt;, hope &amp;lt;int&amp;gt;, grange &amp;lt;int&amp;gt;,
#&amp;gt; #   heard &amp;lt;int&amp;gt;, thrushcross &amp;lt;int&amp;gt;, interrupted &amp;lt;int&amp;gt;, walk &amp;lt;int&amp;gt;,
#&amp;gt; #   closed &amp;lt;int&amp;gt;, uttered &amp;lt;int&amp;gt;, gate &amp;lt;int&amp;gt;, words &amp;lt;int&amp;gt;, horse &amp;lt;int&amp;gt;,
#&amp;gt; #   hand &amp;lt;int&amp;gt;, entered &amp;lt;int&amp;gt;, joseph &amp;lt;int&amp;gt;, bring &amp;lt;int&amp;gt;, suppose &amp;lt;int&amp;gt;,
#&amp;gt; #   nay &amp;lt;int&amp;gt;, dinner &amp;lt;int&amp;gt;, guess &amp;lt;int&amp;gt;, times &amp;lt;int&amp;gt;, wind &amp;lt;int&amp;gt;, house &amp;lt;int&amp;gt;,
#&amp;gt; #   strong &amp;lt;int&amp;gt;, set &amp;lt;int&amp;gt;, wall &amp;lt;int&amp;gt;, door &amp;lt;int&amp;gt;, earnshaw &amp;lt;int&amp;gt;,
#&amp;gt; #   hareton &amp;lt;int&amp;gt;, short &amp;lt;int&amp;gt;, appeared &amp;lt;int&amp;gt;, desire &amp;lt;int&amp;gt;, entrance &amp;lt;int&amp;gt;,
#&amp;gt; #   brought &amp;lt;int&amp;gt;, family &amp;lt;int&amp;gt;, sitting &amp;lt;int&amp;gt;, call &amp;lt;int&amp;gt;, kitchen &amp;lt;int&amp;gt;,
#&amp;gt; #   parlour &amp;lt;int&amp;gt;, deep &amp;lt;int&amp;gt;, observed &amp;lt;int&amp;gt;, light &amp;lt;int&amp;gt;, eye &amp;lt;int&amp;gt;,
#&amp;gt; #   lay &amp;lt;int&amp;gt;, floor &amp;lt;int&amp;gt;, white &amp;lt;int&amp;gt;, countenance &amp;lt;int&amp;gt;, arm &amp;lt;int&amp;gt;,
#&amp;gt; #   chair &amp;lt;int&amp;gt;, seated &amp;lt;int&amp;gt;, round &amp;lt;int&amp;gt;, table &amp;lt;int&amp;gt;, time &amp;lt;int&amp;gt;,
#&amp;gt; #   living &amp;lt;int&amp;gt;, dark &amp;lt;int&amp;gt;, people &amp;lt;int&amp;gt;, hate &amp;lt;int&amp;gt;, love &amp;lt;int&amp;gt;,
#&amp;gt; #   loved &amp;lt;int&amp;gt;, fast &amp;lt;int&amp;gt;, dear &amp;lt;int&amp;gt;, mother &amp;lt;int&amp;gt;, home &amp;lt;int&amp;gt;,
#&amp;gt; #   summer &amp;lt;int&amp;gt;, fine &amp;lt;int&amp;gt;, company &amp;lt;int&amp;gt;, creature &amp;lt;int&amp;gt;, notice &amp;lt;int&amp;gt;,
#&amp;gt; #   told &amp;lt;int&amp;gt;, head &amp;lt;int&amp;gt;, looked &amp;lt;int&amp;gt;, return &amp;lt;int&amp;gt;, poor &amp;lt;int&amp;gt;, till &amp;lt;int&amp;gt;,
#&amp;gt; #   doubt &amp;lt;int&amp;gt;, seat &amp;lt;int&amp;gt;, silence &amp;lt;int&amp;gt;, left &amp;lt;int&amp;gt;, dog &amp;lt;int&amp;gt;,
#&amp;gt; #   master &amp;lt;int&amp;gt;, sat &amp;lt;int&amp;gt;, scarcely &amp;lt;int&amp;gt;, half &amp;lt;int&amp;gt;, hearth &amp;lt;int&amp;gt;,
#&amp;gt; #   arms &amp;lt;int&amp;gt;, fire &amp;lt;int&amp;gt;, purpose &amp;lt;int&amp;gt;, tongue &amp;lt;int&amp;gt;, remained &amp;lt;int&amp;gt;,
#&amp;gt; #   devil &amp;lt;int&amp;gt;, manner &amp;lt;int&amp;gt;, matter &amp;lt;int&amp;gt;, ill &amp;lt;int&amp;gt;, muttered &amp;lt;int&amp;gt;,
#&amp;gt; #   worse &amp;lt;int&amp;gt;, leave &amp;lt;int&amp;gt;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train-a-penalized-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train a penalized logistic regression model&lt;/h2&gt;
&lt;p&gt;Split the data into training set and testing set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_split &amp;lt;- initial_split(model_df)
book_train &amp;lt;- training(book_split)
book_test &amp;lt;- testing(book_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify a L1 penalized logistic model, center and scale all predictors and combine them in to a &lt;code&gt;workflow&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_spec &amp;lt;- logistic_reg(penalty = 0.05, mixture = 1) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

book_rec &amp;lt;- recipe(author ~ ., data = book_train) %&amp;gt;% 
  update_role(line_index, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_zv(all_predictors()) %&amp;gt;% 
  step_normalize(all_predictors())

book_wf &amp;lt;- workflow() %&amp;gt;% 
  add_model(logistic_spec) %&amp;gt;% 
  add_recipe(book_rec)

initial_fit &amp;lt;- book_wf %&amp;gt;% 
  fit(data = book_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;initial_fit&lt;/code&gt; is a simple fitted regression model without any hyperparameters. By default &lt;code&gt;glmnet&lt;/code&gt; calls for 100 values of lambda even if I specify &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0.05\)&lt;/span&gt;. So the extracted result aren’t that helpful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;% 
  tidy()
#&amp;gt; # A tibble: 26,019 x 5
#&amp;gt;    term         step estimate lambda dev.ratio
#&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
#&amp;gt;  1 (Intercept)     1 -1.64    0.0838  5.24e-14
#&amp;gt;  2 (Intercept)     2 -1.64    0.0764  8.68e- 3
#&amp;gt;  3 heathcliff      2  0.0469  0.0764  8.68e- 3
#&amp;gt;  4 linton          2  0.00120 0.0764  8.68e- 3
#&amp;gt;  5 (Intercept)     3 -1.64    0.0696  2.51e- 2
#&amp;gt;  6 heathcliff      3  0.0801  0.0696  2.51e- 2
#&amp;gt;  7 catherine       3  0.0269  0.0696  2.51e- 2
#&amp;gt;  8 linton          3  0.0413  0.0696  2.51e- 2
#&amp;gt;  9 (Intercept)     4 -1.64    0.0634  3.80e- 2
#&amp;gt; 10 heathcliff      4  0.107   0.0634  3.80e- 2
#&amp;gt; # ... with 26,009 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make predictions with &lt;code&gt;initial_fit&lt;/code&gt; anyway, and examine metrics like overall accuracy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict &amp;lt;- predict(initial_fit, book_test) %&amp;gt;% 
    bind_cols(predict(initial_fit, book_test, type = &amp;quot;prob&amp;quot;)) %&amp;gt;%
    bind_cols(book_test %&amp;gt;% select(author, line_index))

initial_predict
#&amp;gt; # A tibble: 11,779 x 5
#&amp;gt;    .pred_class     `.pred_Charlotte Bro~ `.pred_Emily Bron~ author    line_index
#&amp;gt;    &amp;lt;fct&amp;gt;                           &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;int&amp;gt;
#&amp;gt;  1 Charlotte Bron~                 0.844              0.156 Emily Br~          1
#&amp;gt;  2 Charlotte Bron~                 0.844              0.156 Emily Br~         13
#&amp;gt;  3 Charlotte Bron~                 0.844              0.156 Emily Br~         30
#&amp;gt;  4 Charlotte Bron~                 0.844              0.156 Emily Br~         31
#&amp;gt;  5 Charlotte Bron~                 0.844              0.156 Emily Br~         36
#&amp;gt;  6 Charlotte Bron~                 0.844              0.156 Emily Br~         56
#&amp;gt;  7 Charlotte Bron~                 0.844              0.156 Emily Br~         68
#&amp;gt;  8 Charlotte Bron~                 0.844              0.156 Emily Br~         75
#&amp;gt;  9 Charlotte Bron~                 0.844              0.156 Emily Br~         89
#&amp;gt; 10 Charlotte Bron~                 0.844              0.156 Emily Br~         96
#&amp;gt; # ... with 11,769 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How good is our initial model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_predict %&amp;gt;% 
  accuracy(truth = author, estimate = .pred_class)
#&amp;gt; # A tibble: 1 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.844&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nearly 84% of all predictions are right. This isn’t a very statisfactory result since “Charlotte Brontë” accounts for 81% of &lt;code&gt;author&lt;/code&gt;, making our model only slightly better than a classifier that would assngin all &lt;code&gt;author&lt;/code&gt; with “Charlotte Brontë” anyway.&lt;/p&gt;
&lt;div id=&#34;tuning-lambda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tuning lambda&lt;/h3&gt;
&lt;p&gt;We can figure out an appropriate penalty using resampling and tune the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_wf_tune &amp;lt;- book_wf %&amp;gt;%
  update_model(logistic_spec %&amp;gt;% set_args(penalty = tune()))

lambda_grid &amp;lt;- grid_regular(penalty(), levels = 100)
book_folds &amp;lt;- vfold_cv(book_train, v = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I build a set of 10 cross validations resamples, and set &lt;code&gt;levels = 100&lt;/code&gt; to try 100 choices of lambda ranging from 0 to 1.&lt;/p&gt;
&lt;p&gt;Then I tune the grid:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results &amp;lt;- logistic_wf_tune %&amp;gt;%    
  tune_grid(
    resamples = book_folds,
    grid = lambda_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is an &lt;code&gt;autoplot()&lt;/code&gt; method for the tuned results, but I instead plot two metrics versus lambda respectivcely by myself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  collect_metrics() %&amp;gt;% 
  mutate(lower_bound = mean - std_err,
         upper_bound = mean + std_err) %&amp;gt;%
  ggplot(aes(penalty, mean)) + 
  geom_line(aes(color = .metric), size = 1.5, show.legend = FALSE) + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) + 
  facet_wrap(~ .metric, nrow = 2, scales = &amp;quot;free&amp;quot;) + 
  labs(y = NULL,
       x = expression(lambda),
       title = &amp;quot;Performance metric of logistic regession versus differenct choices of L1 regularization&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, the two metrics both display a monotone decrease as lambda increases, but does not exhibit much change once lambda is greater than 0.1, which is essentailly random guess according to the author’s respective proportion of appearance in the data. This plot shows that the model is generally better at small penalty, suggesting that the majority of the predictors are fairly important to the model. We may lean towards larger penalty with slightly worse performance, bacause they lead to simpler models. It follows that we may want to choose lambda in top rows in the following data frame&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_models &amp;lt;- logistic_results %&amp;gt;% 
    show_best(&amp;quot;roc_auc&amp;quot;, n = 100) %&amp;gt;%
    arrange(desc(penalty)) %&amp;gt;% 
    filter(mean &amp;gt; 0.9)

top_models
#&amp;gt; # A tibble: 76 x 6
#&amp;gt;     penalty .metric .estimator  mean     n std_err
#&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
#&amp;gt;  1 0.00376  roc_auc binary     0.906    10 0.00215
#&amp;gt;  2 0.00298  roc_auc binary     0.908    10 0.00219
#&amp;gt;  3 0.00236  roc_auc binary     0.908    10 0.00222
#&amp;gt;  4 0.00187  roc_auc binary     0.908    10 0.00227
#&amp;gt;  5 0.00148  roc_auc binary     0.909    10 0.00209
#&amp;gt;  6 0.00118  roc_auc binary     0.910    10 0.00207
#&amp;gt;  7 0.000933 roc_auc binary     0.910    10 0.00202
#&amp;gt;  8 0.000739 roc_auc binary     0.910    10 0.00205
#&amp;gt;  9 0.000586 roc_auc binary     0.910    10 0.00209
#&amp;gt; 10 0.000464 roc_auc binary     0.910    10 0.00210
#&amp;gt; # ... with 66 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;select_best()&lt;/code&gt; with return the 9th row with &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.000586\)&lt;/span&gt; for its highest performance on &lt;code&gt;roc_auc&lt;/code&gt;. But I’ll stick to the parsimonious principle and pick &lt;span class=&#34;math inline&#34;&gt;\(\lambda \approx 0.00376\)&lt;/span&gt; at the cost of a fall in &lt;code&gt;roc_auc&lt;/code&gt; by 0.005 and in &lt;code&gt;accuracy&lt;/code&gt; by 0.001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_results %&amp;gt;% 
  select_best(metric = &amp;quot;roc_auc&amp;quot;)
#&amp;gt; # A tibble: 1 x 1
#&amp;gt;    penalty
#&amp;gt;      &amp;lt;dbl&amp;gt;
#&amp;gt; 1 0.000586

book_wf_final &amp;lt;- finalize_workflow(logistic_wf_tune,
                                  parameters = top_models %&amp;gt;% slice(1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the model specification in the workflow is filled with the picked lambda:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;book_wf_final %&amp;gt;% pull_workflow_spec()
#&amp;gt; Logistic Regression Model Specification (classification)
#&amp;gt; 
#&amp;gt; Main Arguments:
#&amp;gt;   penalty = 0.00376493580679246
#&amp;gt;   mixture = 1
#&amp;gt; 
#&amp;gt; Computational engine: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next thing is to fit the best model with the training set, and evaluate against the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final &amp;lt;- last_fit(book_wf_final, split = book_split)

logistic_final %&amp;gt;% 
  collect_metrics()
#&amp;gt; # A tibble: 2 x 3
#&amp;gt;   .metric  .estimator .estimate
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
#&amp;gt; 1 accuracy binary         0.938
#&amp;gt; 2 roc_auc  binary         0.904&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;% 
  collect_predictions() %&amp;gt;%
  roc_curve(truth = author, `.pred_Emily Brontë`) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;roc_curve.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The accuracy of our logisitc model rises by a rough 9% to 93.8%, with &lt;code&gt;roc_auc&lt;/code&gt; being nearly 0.904. This is pretty good!&lt;/p&gt;
&lt;p&gt;There is also the confusion matrix to check. The model does well in identifying Charlotte Brontë (low false positive rate, high sensitivity), yet suffers relatively high false negative rate (mistakenly identify 39% of Emily Brontë as Charlotte Brontë, aka low specificity). In part, this is due to class imbalance (four out of five books were written by Charlotte).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_final %&amp;gt;%
  collect_predictions() %&amp;gt;%
  conf_mat(truth = author, estimate = .pred_class) 
#&amp;gt;                   Truth
#&amp;gt; Prediction         Charlotte Brontë Emily Brontë
#&amp;gt;   Charlotte Brontë             9921          726
#&amp;gt;   Emily Brontë                    1         1131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To examine the effect of predictors, I agian use &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;pull_workflow&lt;/code&gt; to extract model fit. Variable importance plots implemented in the &lt;a href=&#34;https://koalaverse.github.io/vip/index.html&#34;&gt;vip&lt;/a&gt; package provides an intuitive way to visualize importance of predictors in this scenario, using the absolute value of the t-statistic as a measure of VI.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vip)

logistic_vi &amp;lt;- book_wf_final %&amp;gt;% 
  fit(book_train) %&amp;gt;% 
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = top_models[1, ]$penalty) %&amp;gt;% 
  group_by(Sign) %&amp;gt;% 
  top_n(30, wt = abs(Importance)) %&amp;gt;%
  ungroup() %&amp;gt;% 
  mutate(Sign = if_else(Sign == &amp;quot;POS&amp;quot;, 
                        &amp;quot;More Emily Brontë&amp;quot;, 
                        &amp;quot;More Charlotte Brontë&amp;quot;))

logistic_vi %&amp;gt;% 
  ggplot(aes(y = reorder_within(Variable, abs(Importance), Sign),
             x = Importance)) + 
  geom_col(aes(fill = Sign), 
           show.legend = FALSE, alpha = 0.6) +
  scale_y_reordered() + 
  facet_wrap(~ Sign, nrow = 1, scales = &amp;quot;free&amp;quot;) + 
  labs(title = &amp;quot;How word usage classifies Brontë sisters&amp;quot;,
       x = NULL,
       y = NULL) + 
  theme(axis.text = element_text(size = 18),
        plot.title = element_text(size = 24),
        plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-25&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2020-05-02-text-classification-with-logistic-rm/index_files/figure-html/unnamed-chunk-25-1.png&#34; alt=&#34;Variable importance plot for penalized logistic regression&#34; width=&#34;1056&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Variable importance plot for penalized logistic regression
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Is it cheating to use names of a character to classify authors? Perhaps I should consider include more books and remove names for text classification next time.&lt;/p&gt;
&lt;p&gt;Note that variale importance in the left panel is generally smaller than the right, this corresponds to what we find in the &lt;a href=&#34;#comparing-word-frequency&#34;&gt;word frequency&lt;/a&gt; plot that Emily Brontë has more and stronger characteristic words.&lt;/p&gt;
&lt;p&gt;In conclusion, the model with the best lambda seems quite powerful in distinguishing these two authors. I look forward to build a multinomial classification model in &lt;code&gt;tidymodels&lt;/code&gt; to include Anne Brontë some other time!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Comparison of Classification Methods</title>
      <link>/post/2020-04-15-tidy-comparison-of-classification-methods/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-04-15-tidy-comparison-of-classification-methods/</guid>
      <description>


&lt;pre&gt;&lt;code&gt;## -- Attaching packages ----------------------------- tidyverse 1.3.0 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.0.9000     v purrr   0.3.4     
## v tibble  3.0.1          v dplyr   0.8.5     
## v tidyr   1.0.2          v stringr 1.4.0     
## v readr   1.3.1          v forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts -------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   135   365&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
