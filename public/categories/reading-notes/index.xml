<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reading Notes on Qiushi Yan</title>
    <link>/categories/reading-notes/</link>
    <description>Recent content in Reading Notes on Qiushi Yan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Nov 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/reading-notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Better science in less time</title>
      <link>/post/better-science-in-less-time/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/better-science-in-less-time/</guid>
      <description>


&lt;p&gt;The &lt;a href=&#34;http://www.oceanhealthindex.org&#34;&gt;Ocean Health Index project&lt;/a&gt;(OHI) provides a means to advance comprehensive ocean policy and compare future progress, with an aim of informing decisions about how to use or protect marine ecosystems, and promoting ongoing assessment of ocean health.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“Our path to better science in less time using open data science tools”&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(Lowndes et al. &lt;a href=&#34;#ref-lowndes2017our&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; describes how several free software tools have fundamentally upgraded OHI team’s approach to collaborative research, and assist better science in less time.&lt;/p&gt;
&lt;div id=&#34;reproducibility-challenges-in-environmental-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility challenges in environmental science&lt;/h2&gt;
&lt;p&gt;Reproducibility challenges are haunting all major branches of science, and of all its victims, environmental scientists face arguably unique challenges in achieving goals of transparency and reproducibility because “they rely on vast amounts of data spanning natural, economic and social sciences that create semantic and synthesis issues exceeding those for most other disciplines”.&lt;/p&gt;
&lt;p&gt;For another, environmental scientists, perhaps more than ever, are eager to build trust in public. So that in recognition of their work, some people would be roused to action. However, proposed environmental solutions can be opaque, controversial and prohibitively costly either in financial or resource terms, increasing the need for scientists to work transparently, reproducibly and collaboratively with data.&lt;/p&gt;
&lt;p&gt;Without appropriate training, scientists tend to develop their own bespoke workarounds to keep
pace, with which comes wasted time struggling to create their own conventions for managing, wrangling and versioning data. The OHI project experienced this sort of struggle first, and by borrowing philosophies, tools, and workflows primarily created for software development, they have dramatically improved work reproducibily, transparency and effectiveness in less time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-happy-transition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;the happy transition&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:transition&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/ohi_transition.png&#34; alt=&#34;Better science in less time, illustrated by the Ocean Health Index project&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Better science in less time, illustrated by the Ocean Health Index project
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:transition&#34;&gt;1&lt;/a&gt; shows an interesting transition of the OHI project from traditional methods to modern data science tools, and most importantly, with greater reproducibility and collaboration effectiveness. As they described:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The original assessment in 2012 focused solely on scientific methods (for example, obtaining and analysing data, developing models, calculating, and presenting results; dark shading). In 2013, by necessity we gave more focus to data science (for example, data organization and wrangling, coding, versioning, and documentation; light shading), using open data science tools. We established R as the main language for all data preparation and modelling (using RStudio), which drastically decreased the time involved to complete the assessment. In 2014, we adopted Git and GitHub for version control, project management, and collaboration. This further decreased the time required to repeat the assessment. We also created the OHI Toolbox, which includes our R package ohicore for core analytical operations used in all OHI assessments. In subsequent years we have continued (and plan to continue) this trajectory towards better science in less time by improving code with principles of tidy data ; standardizing file and data structure; and focusing more on communication, in part by creating websites with the same open data science tools and workflow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In OHI’s 2012 annual report, global assessments of 220 coastal nations and territories are presented. Data from nearly one hundred sources were &lt;strong&gt;prepared manually&lt;/strong&gt; — that is, without coding, typically in Microsoft Excel—which included organizing, transforming, rescaling, gap filling and formatting data. Records of data processing, if any, are scattered. Members communicated and shared files frequently, with long, often forwarded and vaguely titled email chains.&lt;/p&gt;
&lt;p&gt;Not surprisingly, team members struggled to efficienty repeat our own work during the second assessment in 2013. And a 130-pages documentation just seems not sufficient.&lt;/p&gt;
&lt;p&gt;However, in 2013 OHI team started to tackle with reproducibility, collaboration and communication, “with R and Rstudio for coding and visualization, Git for version control, GitHub for collaboration, and a combination of GitHub and Rstudio for organization, documentation, project management, online publishing, distribution and communication”.&lt;/p&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reproducibility&lt;/h3&gt;
&lt;p&gt;Primary data preparation (coding and documenting) is now conducted in R, and a single language makes it more practical to learn and collaborate. The principles of tidy data&lt;span class=&#34;citation&#34;&gt;(Wickham and others &lt;a href=&#34;#ref-wickham2014tidy&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;, and grammar of data manipulation provided by dplyr&lt;span class=&#34;citation&#34;&gt;(Wickham et al. &lt;a href=&#34;#ref-R-dplyr&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; have dramatically facilitate learning and analysis. And metadata, data processing decisions are well documented with R Markdown&lt;span class=&#34;citation&#34;&gt;(Allaire et al. &lt;a href=&#34;#ref-R-rmarkdown&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, instead of plain text files.&lt;/p&gt;
&lt;p&gt;Modeling practices are also unified and standardized in R, and the OHI framwork are incorporated in an R package &lt;strong&gt;ohicore&lt;/strong&gt;, which can be downloaded in R via &lt;code&gt;devtools::install_github(&#39;ohi-science/ohicore&#39;)&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/ohi.png&#34; alt=&#34;ohicore: an toolbox in R presented by the project&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: ohicore: an toolbox in R presented by the project
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Of all things that &lt;strong&gt;ohicore&lt;/strong&gt; package has to offer, most seminal is that it has made self-made OHI assessment possible without recreating the wheel. There are currently two dozen OHI assessments underway, most of which are led by independent groups.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Scientific advancement comes from building off the past work of others. Only in an growing, accomodating community could modern science thrive.&lt;br /&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;For version control, Git has consigned to history local folder with weird naming conventions. Rstudio plays a key role here by offering an simple interface with Git.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collaboration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collaboration&lt;/h3&gt;
&lt;p&gt;The OHI team transitioned from a team of distinct roles (scientists and programmer) to become a team with overlapping skill sets(data scientists). This transition has enable easier collaboration(i.e., to vet code as a team).&lt;/p&gt;
&lt;p&gt;GitHub is now central to OHI’s collaboration workflow, an interesting report is that the team uses the &lt;em&gt;“Issues”&lt;/em&gt; feature to record discussions, decisions, collect feedbacks. Compared to email, team members can communicate directly by linking to specific lins of code in any branch of a repo. Conversations are open to all, and &lt;code&gt;@&lt;/code&gt; can be readily used when assigning tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;communication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Communication&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Open data science tools have made us reimagine what communication can mean for science and management. They enable us to not only share our code online, but to create reports, ebooks, interactive web applications, and entire websites, which we can share for free to communicate our work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Code from the OHI project can be browsed at &lt;a href=&#34;https://github.com/ohi-science&#34; class=&#34;uri&#34;&gt;https://github.com/ohi-science&lt;/a&gt; and official website at &lt;a href=&#34;https://ohi-science.org/&#34; class=&#34;uri&#34;&gt;https://ohi-science.org/&lt;/a&gt;. A training book (also authored by OHI tema) about open data science with R can be found at &lt;a href=&#34;http://ohi-science.org/data-science-training/&#34; class=&#34;uri&#34;&gt;http://ohi-science.org/data-science-training/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Communication outside the project (result publishing) are also part of the Rstudio-github workflow, saving scientists from copying and pasting code, figures and text. Thus, the website provides scientific methods, publications, data, and code, as well as instruction, news, and blog posts.&lt;/p&gt;
&lt;p&gt;Ecologists and environmental scientists arguably have a heightened responsibility for transparency and openness, as data products provide important snapshots of systems that may be forever altered due to climate change and other human pressures 16,18 . There is particular urgency for efficiency and transparency, as well as opportunity to democratize science in fields that operate at the interface of science and policy.&lt;/p&gt;
&lt;p&gt;Two critical barrier to embracing open science using efficient tools are brieved in the last section of this paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;knowing which tools exist that can be directly useful to one’s research&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;having the confidence to develop the skills to use them&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I really enjoy reading the last section, in which the author explained why open, transparent and reproducible science should be promoted and how to kickstart the rewarding learning path. The original line can be found at &lt;a href=&#34;https://www.nature.com/articles/s41559-017-0160#Sec7&#34; class=&#34;uri&#34;&gt;https://www.nature.com/articles/s41559-017-0160#Sec7&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rmarkdown&#34;&gt;
&lt;p&gt;Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2019. &lt;em&gt;Rmarkdown: Dynamic Documents for R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rmarkdown&#34;&gt;https://CRAN.R-project.org/package=rmarkdown&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lowndes2017our&#34;&gt;
&lt;p&gt;Lowndes, Julia S Stewart, Benjamin D Best, Courtney Scarborough, Jamie C Afflerbach, Melanie R Frazier, Casey C O’Hara, Ning Jiang, and Benjamin S Halpern. 2017. “Our Path to Better Science in Less Time Using Open Data Science Tools.” &lt;em&gt;Nature Ecology &amp;amp; Evolution&lt;/em&gt; 1 (6): 0160.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2019. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wickham2014tidy&#34;&gt;
&lt;p&gt;Wickham, Hadley, and others. 2014. “Tidy Data.” &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 59 (10): 1–23.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>50 Years of Data Science</title>
      <link>/post/50-years-of-data-science/</link>
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/50-years-of-data-science/</guid>
      <description>


&lt;p&gt;More than 50 years ago, John Tukey in his renowned “The Future of Dat Analysis” envisioned a would-be field, where the subject of interest is to learn from data. Other pioneers, like John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again exhorted academic statistics to embrace this so-called “data sicence / analysis” trend, to soar above theoretical statisticals which was characterized by theorems, proofs, etc. In this article, David Donoho&lt;span class=&#34;citation&#34;&gt;(Donoho &lt;a href=&#34;#ref-doi:10.1080/10618600.2017.1384734&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; presents a vision of data science based on the activities of people who are “learning from data,” and he describes an academic field dedicated to improving that activity in an evidence-based manner.&lt;/p&gt;
&lt;div id=&#34;data-science-vs.-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Science vs. Statistics&lt;/h1&gt;
&lt;p&gt;The author first starts to distinguish data science from traditional statistics. Yet 3 commonly used indicators are discredited:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Big data” is not a credible criterion for meaningful distinction between statistics and data science, for both historical and scientific reasons, viz:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The very term “statistics” was coined at the beginning of modern efforts to compile census data, roughly the size of today’s big data&lt;/li&gt;
&lt;li&gt;Statisticians have long since delved into large databases, and come up with sufficient measures&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Although some would tout “new skills” on the part of data scientists, that is another “big data” meme cloaked in another term.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data science programs doesn’t necessarily guide one to a satisfying job, comapared with statistics. Even it does, a data science master degree cannot exempt you from various constraints in real workplace&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, is there any solid case for an new entity called “data science” ? David proposed that data science would be a true science, an enlargement of traditional academic statistics. And the underlying motivation is intellectual rather than commerical. In the next 50 years, scientific publication in various disciplines will become a body of data that we can analyze and study, and the opportunity to improve the accuracy and validity of all science relies upon this would-be field: data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;review-on-the-concept&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Review on the concept&lt;/h1&gt;
&lt;p&gt;Based on the timeline, the author then organizes insights that have been published over the years about data science, as shown below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All in all I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tukey identified four driving forces in the new science &lt;span class=&#34;citation&#34;&gt;(Tukey &lt;a href=&#34;#ref-tukey1962future&#34; role=&#34;doc-biblioref&#34;&gt;1962&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The formal theories of statistics&lt;/li&gt;
&lt;li&gt;Accelerating developments in computers and display devices&lt;/li&gt;
&lt;li&gt;The challenge, in many fields, of more and ever larger bodies of data&lt;/li&gt;
&lt;li&gt;The emphasis on quantification in an ever wider variety of disciplines&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s spend a minute admiring Tukey’s prophecy…&lt;/p&gt;
&lt;p&gt;Among other things, Tukey also pointed out that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;…data analysis is a very difficult field. It must adapt itself to what people can and need to do with data. In the sense that biology is more complex than physics, and the behavioral sciences are more complex than either, it is likely that the general problems of data analysis are more complex than those of all three. It is too much to ask for close and effective guidance for data analysis from any highly formalized structure, either now or in the near future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Data analysis can gain much from formal statistics, but only if the connection is kept adequately loose.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;John Chambers presented a choice between “greater” or “lesser” statistics. He argued
&lt;span class=&#34;citation&#34;&gt;(Chambers &lt;a href=&#34;#ref-chambers1993greater&#34; role=&#34;doc-biblioref&#34;&gt;1993&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The statistics profession faces a choice in its future research between continuing concentration on traditional topics—based largely on data analysis supported by mathematical statistics—and a broader viewpoint—based on an inclusive concept of learning from data. The latter course presents severe challenges as well as exciting opportunities. The former risks seeing statistics become increasingly marginal…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;C. F. Jeff Wu characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making, and called for courses outside out the statistics department.&lt;/p&gt;
&lt;p&gt;William S. Cleveland put forward 6 foci of data science, which offered a great conceptual framework to study the filed even from today’s perspective &lt;span class=&#34;citation&#34;&gt;(Cleveland &lt;a href=&#34;#ref-cleveland2001data&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Multidisciplinary investigations (25%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Models and Methods for Data (20%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Computing with Data (15%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pedagogy (15%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tool Evaluation (5%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Theory (20%)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UC Berkeley statistician Leo Breiman brought his “two modeling cultures theory” into the exhortation. He asserted &lt;span class=&#34;citation&#34;&gt;(Breiman and others &lt;a href=&#34;#ref-breiman2001statistical&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt; :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are two goals in analyzing the data:&lt;br /&gt;
Prediction: To be able to predict what the responses are going to be to future input variables;&lt;br /&gt;
[Inference]: To [infer] how nature is associating the response variables to the input variables.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proceeding from this basis, Brieman thought that the prdiction emphasis can be described as “predictive modeling” culture&#34;, which lay stress on accuracy of prediction made by different algorithm on various datasets. But this culture, according to his estimation, is only practiced by 2% of academic statisticians.&lt;/p&gt;
&lt;p&gt;“Generative modeling culture” corresponds to the latter inference emphasis, and accounted for 98% statistical practice. Brienman said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on [generative] models&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From today’s standpoint, Breiman’s opinions is fairly justified. But why? &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-doi:10.1080/10618600.2017.1384734&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; thinks the Common Task Framework played a key role here.&lt;/p&gt;
&lt;p&gt;An instance of the CTF has these ingredients( I actually have never heard the term before. To my mind, it’s just like a usual kaggle contest or a data visualization challenge, yet may not subjected ot choosing the “best” machine learning model or plot):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A publicly available training dataset involving, for each observation, a list of (possibly many) feature measurements, and a class label for that observation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set of enrolled competitors whose common task is to infer a class prediction rule from the training data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A scoring referee, to which competitors can submit their prediction rule. The referee runs the prediction rule against a testing dataset, which is sequestered behind a Chinese wall. The referee objectively and automatically reports the score (prediction accuracy) achieved by the submitted rule.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The synergy of minimizing prediction error with CTF is worth noting. This combination leads directly to a total focus on optimization of empirical performance, allows large numbers of researchers to compete at any given common task challenge, and allows for efficient and unemotional judging of challenge winners.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-six-divisions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Six Divisions&lt;/h1&gt;
&lt;p&gt;Further, to dipict an even larger professional on a quest to extract information from data, the author comes up with a classificcation concerning data science practices:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data Gathering, Preparation, and Exploration&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Representation and Transformation:&lt;/strong&gt; To tranfrom data collected from a wealth of formats in an attempt to represent them in one format susceptible for analysis.(i.e., get features with acoustic data, one often transforms to the cepstrum or the Fourier transform)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computing with Data&lt;/strong&gt;: Beyond basic knowledge of programming languages, data scientists need to keep current on new idioms for efficiently using those languages and need to understand the deeper issues associated with computational efficiency. Cluster and cloud computing and the ability to run massive numbers of jobs on such clusters has become an overwhelmingly powerful ingredient of the modern computational landscape.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Modeling&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Visualization and Presentation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Science about Data Science&lt;/strong&gt;: Data scientists are doing science about data science when they identify commonly occurring analysis/processing workflows, for example, using data about their frequency of occurrence in some scholarly or business domain; when they measure the effectiveness of standard workflows in terms of the human time, the computing resource, the analysis validity, or other performance metric, and when they uncover emergent phenomena in data analysis, for example, new patterns arising in data analysis workflows, or disturbing artifacts in published analysis results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;science-about-data-science&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Science About Data Science&lt;/h1&gt;
&lt;p&gt;David then elaborates on the 6th part: science about data Science, which has made data science a true branch of science, rather than a broad collection of technical activities. Data science offered a continually evolving, evidence-based approach for analysts, covering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Science-Wide Meta Analysis: the idea of “analyzing what people have analyzed” across disciplines&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Cross-Study Analysis: To study the validation of different studies on the same dataset. * Cross-Workflow Analysis: To study the impact of analysis workflow on results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these 3 approaches, comtribute to enhancing the validity of the scientific literature.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-50-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Next 50 years&lt;/h1&gt;
&lt;p&gt;At the end, what about the next 50 years of data science?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Research will be more open and reproducible. Code sharing, data sharing will allow large numbers of datasets and analysis workflows to be derived from studies science-wide&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;All scientific publifications will be transformed to data thta can be mined&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Ample data will be available to measure the performance of algorithms and models across a whole ensemble of situations&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each proposed notion of data science involves some &lt;strong&gt;enlargement&lt;/strong&gt; of academic statistics and machine learning. The “GDS” variant specifically discussed in this article derives from insights about data analysis and modeling stretching back decades. In this variant, the core motivation for the expansion to data science is intellectual. In the future, there may be great industrial demand for the skills inculcated by GDS; however, the core questions which drive the field are scientific, not industrial.&lt;/p&gt;
&lt;p&gt;GDS(initialtive for greater data science) proposes that data science is the science of learning from data; it studies the methods involved in the analysis and processing of data and proposes technology to improve methods in an evidence-based manner. The scope and impact of this science will expand enormously in coming decades as scientific data and data about science itself become ubiquitously available.&lt;/p&gt;
&lt;p&gt;Society already spends tens of billions of dollars yearly on scientific research, and much of that research takes place at universities. GDS inherently works to understand and improve the validity of the conclusions produced by university research, and can play a key role in all campuses where data analysis and modeling are major activities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-breiman2001statistical&#34;&gt;
&lt;p&gt;Breiman, Leo, and others. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” &lt;em&gt;Statistical Science&lt;/em&gt; 16 (3): 199–231.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-chambers1993greater&#34;&gt;
&lt;p&gt;Chambers, John M. 1993. “Greater or Lesser Statistics: A Choice for Future Research.” &lt;em&gt;Statistics and Computing&lt;/em&gt; 3 (4): 182–84.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cleveland2001data&#34;&gt;
&lt;p&gt;Cleveland, William S. 2001. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” &lt;em&gt;International Statistical Review&lt;/em&gt; 69 (1): 21–26.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doi:10.1080/10618600.2017.1384734&#34;&gt;
&lt;p&gt;Donoho, David. 2017. “50 Years of Data Science.” &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 26 (4): 745–66. &lt;a href=&#34;https://doi.org/10.1080/10618600.2017.1384734&#34;&gt;https://doi.org/10.1080/10618600.2017.1384734&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tukey1962future&#34;&gt;
&lt;p&gt;Tukey, John W. 1962. “The Future of Data Analysis.” &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 33 (1): 1–67.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-knitr&#34;&gt;
&lt;p&gt;Xie, Yihui. 2019. &lt;em&gt;Knitr: A General-Purpose Package for Dynamic Report Generation in R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=knitr&#34;&gt;https://CRAN.R-project.org/package=knitr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The original line was slightly modified by David, the original has “information” in place of [inference] and “extract some information about” in place of [infer]&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Here I skipped some discussion related to R as an computing environment, literate programming in &lt;code&gt;knitr&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Xie &lt;a href=&#34;#ref-R-knitr&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and the “tidy data” concept&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For now, this section is copied from the original article&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Enterprise Data Analysis and Visualization: An Interview Study</title>
      <link>/post/enterprise-data-analysis-and-visualization-an-interview-study/</link>
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/enterprise-data-analysis-and-visualization-an-interview-study/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;“Enterprise Data Analysis and Visualization: An Interview Study”&lt;/strong&gt; is an effort to penetrate into enterprises and glean real-world data science experience from analysts directly. By conducting semi-structured interviews with 35 data analysts, &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-2012-enterprise-analysis-interviews&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; provides insight on 3 archetypes describing different workflows and tasks given, common pain points analysts may encounter, future trends, etc.&lt;/p&gt;
&lt;div id=&#34;archetypes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3 Archetypes&lt;/h1&gt;
&lt;p&gt;According to the responses, we can see analysts generally fall into 3 archetypes: &lt;em&gt;hacker&lt;/em&gt;, &lt;em&gt;scripter&lt;/em&gt; and &lt;em&gt;application user&lt;/em&gt;.&lt;br /&gt;
&lt;img src=&#34;/img/archetypes.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hackers&lt;/strong&gt; are faced with the most diverse and complex tasks. They are most literate in terms of programming and thus rarely ask IT staffs fro help. More oftern than not, hackers master more than three languages, R / Matlab for analysis, Python, Perl as an scripting language, and SQL for queries.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I guess for now Python has gained ground also as a data analysis language, and Matlab is less popular among analysts doing “practical” analysis, but more frquently used in labs of nature science.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Also note that with their ability to collect, manipulate data, some hackers could be in charge of managing the data warehouse of the company, and dealing with large datasets. In constrast, they perform less statistical models, and spend more time in early-stage analytic activities prior to modeling, if any.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scripters&lt;/strong&gt; take care of advanced modeling and use a software package such as R or Matlab extensively. They are less proficient when parsing log files or scraping data off the web, and the data susceptible for modeling are often prepared by IT staff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application user&lt;/strong&gt; prefer operations done in a spreadsheet( Mostly in Excel ), or other analysis application (e.g., SAS / JMP, SPSS, etc.). Data are also pulled out from several relationald databases prior to their work. They typically worked on smaller datasets than other gorups, advanced application users may wrote scripts using an embedded language such as Visual Basic.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    To my mind, application users dipicted here seem to be titled “data analysts” for historical reasons, and there is little case for keeping such a position when there are already hackers and scripters. Perhaps they were traditional bussiness people in charge of analysis or accountants, so they are most comforatble with Microsoft Excel or SPSS. As the “big data” meme started to present itself, all of a sudden their enterprise felr it imperative to set up a “data scientist / analyst” position to keep up with this new trend. Yet I doubt if this archetype could still survive when graduates from data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysts-within-organization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysts within organization&lt;/h1&gt;
&lt;p&gt;Analysts interact closely with IT staff to complete aspects of theire job. For IT staff, his relationship includes data ingesting and acquiring, operationalizing recurring workflows, and serve as a source of documentation when analysts, say, con’t figure out how to wirte a complex SQL statement.&lt;/p&gt;
&lt;p&gt;This reliance on IT staff was particularly true in organizations where data was distributed across many different data sources. Hackers were most likely to use the IT team explicitly for this function, as they were more likely to access data directly from the warehouse. Scripters and application users relied on this function implicitly when receiving data from members of IT.&lt;/p&gt;
&lt;p&gt;Another thread of this topic is distributed data, which are generated from multiple departments of the enterprise. They are often stored in various databases and formats. adding to the diffculty of intergrating them.&lt;/p&gt;
&lt;p&gt;When analysis is finished, analysts typically shared static reports in the form of template documents. In some cases, reports could be interactive dashboards that enabled end users to filter or modify statistics computed. It’s not hard to imagine consumers of the report often give a blurred image of what they want, and hardly could analysts translate them into practical data problems.&lt;/p&gt;
&lt;p&gt;When it comes to collaboration, it is an exception rather than the rule for analysts. They do share some central repository data processing scriptes are kept to oneself as a rule. Ont the other hand, final reports in the form of charts or model summary are commonly shared among analytsts in planning meeting or presentations. These reports, however, are rarely parametrizable or interactive.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-2012-enterprise-analysis-interviews&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; identifies three impediemnts to collaboration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the diversity of tools and programming languages&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;finding a script or inter-
mediate data product someone else produced was often more time-
consumingthanwritingthescriptfromscratch&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;many analysis process are “ad hoc”, “experimental”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Challenges&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-2012-enterprise-analysis-interviews&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; identified five high-level tasks in the workflow and challenges within each of them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discovering&lt;/strong&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;: Since data are often distributed across multiple databases and sources, finding the exact data sheet or file needed can be time-consuming, not to mention that some analysts only have restricted access to the data warehhouse. Another problem is field definitions, these definitions were
often missing in relational databases and non-existent in other types of data stores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wranling&lt;/strong&gt;: Many analysts reported parsing, ingesting semi-structured data(i.e., log files, block data&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;). Another difficulty is integrating the data, after analysts manage to find them in databases. Sometimes identifiers are missing or encoded inconsistantly in some databases, and sometimes there is no column than could be an identifier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiling&lt;/strong&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;: Many analysts (22 / 35) reports issues dealing with missing data and heterogeneous data in a column. When detecting outliers, there is no general agreement between visualization and traditional staistical methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Modeling&lt;/strong&gt;: The biggest challenge in constructing an model according to respondents are feature selection, whether to choose a set of variables, which to transfrom and how to transform.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    There is a great book on feature engineering and selectin by Max Kuhn and Kjell Johnson: &lt;a href=&#34;http://www.feat.engineering/index.html&#34;&gt;Feature Engineering and Selection: A Practical Approach for Predictive Models&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Most repondents also pointed to the scalibility of existing analysis and visualization tools. Hackers are less limited by large amounts of data, obviously, but hackers were often limited by the types of analysis they could run because useful models or algorithms did not have available parallelized implementations. Visualizing model results is another pain point, analysts using more advanced machine learning methods (14/35) expressed a desire for visualization tools to help explore these models and visualize their output.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    R’s package &lt;code&gt;broom&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Robinson and Hayes &lt;a href=&#34;#ref-R-broom&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; are desgined to facilitate modeling diagnosis, visualization, etc. &lt;code&gt;tidymodels&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Kuhn and Wickham &lt;a href=&#34;#ref-R-tidymodels&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; contains a burgeoning list of such packages.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reporing&lt;/strong&gt;: The two most-cited challenges in reporoting were communicating assumptions and building interactive reports. Documentation that should have been provided alongside the report are often missing or poorly written. Even when assumptions were tracked, they were often treated as footnotes instead of first-class results. Moreover, analysts complained that reports were too
inflexible and did not allow interactive verification or sensitivity analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-trends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Future trends&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Heer (&lt;a href=&#34;#ref-2012-enterprise-analysis-interviews&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; then prophesy future trends in technology and the analytic workforce:&lt;/p&gt;
&lt;p&gt;Public data become more accessible and add to the diffculty of data discovery and intergration.&lt;/p&gt;
&lt;p&gt;The market of Hadoop-like software will continue to increase, allowing analysts to operate on less structured data formats.&lt;/p&gt;
&lt;p&gt;“Hacker-level” analysts will be in demand, analysts therefore need to be adept at both statistical reasoning and writing complex SQL or Map-Reduce code. Those who are comfortable in multiple data processing / analysis frameworks will be competent.&lt;/p&gt;
&lt;p&gt;The size of analytic teams should grow, efficient collaboration will become both increasingly important and difficult.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rmarkdown&#34;&gt;
&lt;p&gt;Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2019. &lt;em&gt;Rmarkdown: Dynamic Documents for R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rmarkdown&#34;&gt;https://CRAN.R-project.org/package=rmarkdown&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-shiny&#34;&gt;
&lt;p&gt;Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2019. &lt;em&gt;Shiny: Web Application Framework for R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=shiny&#34;&gt;https://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-2012-enterprise-analysis-interviews&#34;&gt;
&lt;p&gt;Heer, Sean Kandel AND Andreas Paepcke AND Joseph Hellerstein AND Jeffrey. 2012. “Enterprise Data Analysis and Visualization: An Interview Study.” In &lt;em&gt;IEEE Visual Analytics Science &amp;amp; Technology (Vast)&lt;/em&gt;. &lt;a href=&#34;http://idl.cs.washington.edu/papers/enterprise-analysis-interviews&#34;&gt;http://idl.cs.washington.edu/papers/enterprise-analysis-interviews&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidymodels&#34;&gt;
&lt;p&gt;Kuhn, Max, and Hadley Wickham. 2019. &lt;em&gt;Tidymodels: Easily Install and Load the ’Tidymodels’ Packages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidymodels&#34;&gt;https://CRAN.R-project.org/package=tidymodels&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-sparklyr&#34;&gt;
&lt;p&gt;Luraschi, Javier, Kevin Kuo, Kevin Ushey, JJ Allaire, and The Apache Software Foundation. 2019. &lt;em&gt;Sparklyr: R Interface to Apache Spark&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=sparklyr&#34;&gt;https://CRAN.R-project.org/package=sparklyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-broom&#34;&gt;
&lt;p&gt;Robinson, David, and Alex Hayes. 2019. &lt;em&gt;Broom: Convert Statistical Analysis Objects into Tidy Tibbles&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=broom&#34;&gt;https://CRAN.R-project.org/package=broom&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2017. &lt;em&gt;Tidyverse: Easily Install and Load the ’Tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-knitr&#34;&gt;
&lt;p&gt;Xie, Yihui. 2019. &lt;em&gt;Knitr: A General-Purpose Package for Dynamic Report Generation in R&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=knitr&#34;&gt;https://CRAN.R-project.org/package=knitr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Here discovering means acquire data necessary to complete analysis tasks&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In a block format, logical records of data are spread
across multiple lines of a file. Typically one line (the “header”) con-
tains metadata about the record, such as how many of the subsequent
lines (the “payload”) belong to the record&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;To verify data qualityand its suitability for the analysis tasks. Example tasks include inspecting outliers, examining distributions&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In the last section, the author move on to discuss improvements that can be made to analytic tools. I figure these suggestions are somehow outdated by today’s standard, and &lt;code&gt;tidyverse&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Wickham &lt;a href=&#34;#ref-R-tidyverse&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;、&lt;code&gt;knitr&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Xie &lt;a href=&#34;#ref-R-knitr&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;、&lt;code&gt;rmarkdown&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Allaire et al. &lt;a href=&#34;#ref-R-rmarkdown&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;、&lt;code&gt;shiny&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Chang et al. &lt;a href=&#34;#ref-R-shiny&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;、&lt;code&gt;sparklyr&lt;/code&gt;&lt;span class=&#34;citation&#34;&gt;(Luraschi et al. &lt;a href=&#34;#ref-R-sparklyr&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and some other packages in R have already given fine solutions to them, so this section is skipped.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>